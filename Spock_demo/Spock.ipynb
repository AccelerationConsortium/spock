{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "# Spock: LLM-based Tool for Bibliometric Analysis of  Scientific Literature\n",
    "\n",
    "### Tool Presentation:\n",
    "\n",
    "Researchers often need to work with peer publications within their lab, which can be both time-consuming and resource-demanding. This involves obtaining structured data, conducting thorough literature reviews, and post-processing findings to support their research objectives. \n",
    "Large Language Models (LLMs) and their capability of understanding and generating natural language text bring promising opportunities that can be integrated into a researcher's daily life to simply enhance automation and research pace.\n",
    "One such integration is the Spock, a bibliometric analysis tool integrated within Slack, designed to interact with the Acceleration Consortium members' Google scholar profile.\n",
    "The workflow follows an automatic retrieval of authors' peer-reviewed and non-peer-reviewed papers and patents from journals and preprint servers (i.e., arXiv, chemRxiv) submissions.\n",
    "This data is then post-processed via Retrieval-Augmented Generation (RAG) allowing an in-depth analysis of papers by giving, for instance, details about the scientific workflow and techniques used by the author of a scientific paper or describing AI/screening algorithms mentioned in the paper. Multi-class classification of the publication's topics in accelerated materials discovery is also an important feature of our tool. \n",
    "It can provide insights by keeping a systematic track of what researchers at the Laboratory are writing, compare research groups within the organization over several years to identify trends and opportunities, and facilitate easier interaction among scientists. Finally, it helps in preparing and submitting future research proposals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries and Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import Bot_LLM,Author,Publication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bot_LLM object:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features walkthrough\n",
    "\n",
    "We are going to use an \"example\" using my mentor's (Mehrad Ansari's) latest paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Bot_LLM(folder_path = 'db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the Author data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating Author object\n",
    "author = Author(\"Mehrad Ansari\")\n",
    "\n",
    "# Fetching latest date\n",
    "latest_publication = Publication(author.get_last_publication())\n",
    "\n",
    "# Setting up and ouputing in a json file\n",
    "author.setup_author(\"output.json\",llm, latest_publication)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/spock/Spock_demo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:27:12\u001b[0m | \u001b[1mChoose scihub url [0]: https://sci-hub.mobi\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:27:12\u001b[0m | \u001b[1m<- Request: scihub_url=https://sci-hub.mobi, source=TitleSource[type=title, id=History-agnostic battery degradation inference], proxies={}\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:13\u001b[0m | \u001b[1m-> Response: status_code=504, content_length=164\u001b[0m\n",
      "\u001b[33m\u001b[1m[WARNING]\u001b[0m | \u001b[32m2024/08/13 23:28:13\u001b[0m | \u001b[33m\u001b[1mError occurs, task status: crawling_failed, error: Error occurs when crawling source: TitleSource[type=title, id=History-agnostic battery degradation inference]\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:13\u001b[0m | \u001b[1mChoose scihub url [1]: https://sci-hub.se\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:13\u001b[0m | \u001b[1m<- Request: scihub_url=https://sci-hub.se, source=TitleSource[type=title, id=History-agnostic battery degradation inference], proxies={}\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:16\u001b[0m | \u001b[1m-> Response: status_code=200, content_length=5854\u001b[0m\n",
      "\u001b[33m\u001b[1m[WARNING]\u001b[0m | \u001b[32m2024/08/13 23:28:16\u001b[0m | \u001b[33m\u001b[1mError occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:16\u001b[0m | \u001b[1mChoose scihub url [2]: http://sci-hub.se\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:16\u001b[0m | \u001b[1m<- Request: scihub_url=http://sci-hub.se, source=TitleSource[type=title, id=History-agnostic battery degradation inference], proxies={}\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:17\u001b[0m | \u001b[1m-> Response: status_code=200, content_length=5854\u001b[0m\n",
      "\u001b[33m\u001b[1m[WARNING]\u001b[0m | \u001b[32m2024/08/13 23:28:17\u001b[0m | \u001b[33m\u001b[1mError occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:17\u001b[0m | \u001b[1mChoose scihub url [3]: https://sci-hub.ru\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:17\u001b[0m | \u001b[1m<- Request: scihub_url=https://sci-hub.ru, source=TitleSource[type=title, id=History-agnostic battery degradation inference], proxies={}\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:19\u001b[0m | \u001b[1m-> Response: status_code=200, content_length=5854\u001b[0m\n",
      "\u001b[33m\u001b[1m[WARNING]\u001b[0m | \u001b[32m2024/08/13 23:28:19\u001b[0m | \u001b[33m\u001b[1mError occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:19\u001b[0m | \u001b[1mChoose scihub url [4]: http://sci-hub.st\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:19\u001b[0m | \u001b[1m<- Request: scihub_url=http://sci-hub.st, source=TitleSource[type=title, id=History-agnostic battery degradation inference], proxies={}\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:20\u001b[0m | \u001b[1m-> Response: status_code=200, content_length=5854\u001b[0m\n",
      "\u001b[33m\u001b[1m[WARNING]\u001b[0m | \u001b[32m2024/08/13 23:28:21\u001b[0m | \u001b[33m\u001b[1mError occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:21\u001b[0m | \u001b[1mChoose scihub url [5]: https://sci-hub.st\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:21\u001b[0m | \u001b[1m<- Request: scihub_url=https://sci-hub.st, source=TitleSource[type=title, id=History-agnostic battery degradation inference], proxies={}\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:21\u001b[0m | \u001b[1m-> Response: status_code=200, content_length=5854\u001b[0m\n",
      "\u001b[33m\u001b[1m[WARNING]\u001b[0m | \u001b[32m2024/08/13 23:28:21\u001b[0m | \u001b[33m\u001b[1mError occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:21\u001b[0m | \u001b[1mChoose scihub url [6]: http://sci-hub.mobi\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:21\u001b[0m | \u001b[1m<- Request: scihub_url=http://sci-hub.mobi, source=TitleSource[type=title, id=History-agnostic battery degradation inference], proxies={}\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:21\u001b[0m | \u001b[1m-> Response: status_code=200, content_length=39030\u001b[0m\n",
      "\u001b[33m\u001b[1m[WARNING]\u001b[0m | \u001b[32m2024/08/13 23:28:22\u001b[0m | \u001b[33m\u001b[1mError occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:22\u001b[0m | \u001b[1mChoose scihub url [7]: http://sci-hub.ru\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:22\u001b[0m | \u001b[1m<- Request: scihub_url=http://sci-hub.ru, source=TitleSource[type=title, id=History-agnostic battery degradation inference], proxies={}\u001b[0m\n",
      "\u001b[1m[INFO]\u001b[0m | \u001b[32m2024/08/13 23:28:22\u001b[0m | \u001b[1m-> Response: status_code=200, content_length=5854\u001b[0m\n",
      "\u001b[33m\u001b[1m[WARNING]\u001b[0m | \u001b[32m2024/08/13 23:28:22\u001b[0m | \u001b[33m\u001b[1mError occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\u001b[0m\n",
      "\u001b[31m\u001b[1m[ERROR]\u001b[0m | \u001b[32m2024/08/13 23:28:22\u001b[0m | \u001b[31m\u001b[1mFailed to download the paper: History-agnostic battery degradation inference. Please try again.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Getting the Wd\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "\n",
    "# Donwloading the pdf\n",
    "path = current_directory + '/pdfs/'\n",
    "latest_publication.download_pdf(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scanning the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport glob\\nfiles_in_dir = glob.glob(\\'pdfs\\'+ \"/*\")\\ndico_response = {}\\n\\n\\ndef load_json_data(json_file_path):\\n    with open(json_file_path, \\'r\\') as json_file:\\n        data = json.load(json_file)\\n    return data\\n\\ndico_response = load_json_data(\\'benchmarking.json\\')\\ni = 1\\ntrue_index = 40\\nfor index,file in enumerate(files_in_dir):\\n    print(index,file)\\n    try:\\n        if file.split(\\'/\\')[1] not in dico_response:\\n            print(true_index,file)\\n            true_index += 1\\n            dico_response[file.split(\\'/\\')[1]] = benchmarking(file,true_index)\\n        else:\\n            print(\"file already done\")\\n    except Exception as e:\\n        print(e)\\n        dico_response[file.split(\\'/\\')[1]] = None\\n    print(file+\" done\")\\n        \\nwith open(\\'benchmarking.json\\', \\'w\\') as json_file:\\n    json.dump(dico_response, json_file)\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def benchmarking(file, index=0):\n",
    "\n",
    "    \"\"\"\n",
    "    llm = Bot_LLM(folder_path='db/db'+str(index))\n",
    "    llm.chunk_indexing(file)    \n",
    "    #llm.folder_path = txt\n",
    "    \"\"\"\n",
    "    dico = {}\n",
    "    ## Querying the RAG\n",
    "    dico['affiliation'] = llm.query_rag(\"What are the authors affiliation. Output a dictionary ?\")\n",
    "    print(\"-----\")\n",
    "    dico['topic']  = llm.get_topic_publication()\n",
    "    \n",
    "    print(\"-----\")\n",
    "    dico['new materials'] = llm.query_rag(\"does the article mention any new material discovery ?\")\n",
    "    print(\"-----\")\n",
    "    dico['screening algorithms'] = llm.query_rag(\"A screening algorithm is a systematic procedure or method used to identify individuals who may have or be at risk for a particular condition or trait within a large population. These algorithms are designed to quickly and efficiently screen out those who are unlikely to have the condition, while identifying those who may require further diagnostic evaluation or intervention. If there are any, What are the screening algorithms used in the paper ?\")\n",
    "    print(\"-----\")\n",
    "    dico['AI algorithms'] = llm.query_rag(\"AI algorithms are computational methods and processes used to solve specific tasks by mimicking human intelligence. These algorithms enable machines to learn from data, make decisions, and perform tasks that typically require human intelligence.\")\n",
    "    print(\"-----\")\n",
    "    \n",
    "    dico['workflow'] = llm.query_rag(\"Can you describe to me the workflow used by the author ?\")\n",
    "    print(\"-----\")\n",
    "    dico['methods'] = llm.query_rag(\"can you do a methods description ?\")\n",
    "    print(\"-----\")\n",
    "    dico['models'] = llm.query_rag(\"what are the models used in the article ?\")\n",
    "    print(\"-----\")\n",
    "    dico['funding'] = llm.query_rag(\"does the article mention who funded it\")\n",
    "    print(\"-----\")\n",
    "    print(dico)\n",
    "    return dico\n",
    "\n",
    "\"\"\"\n",
    "import glob\n",
    "files_in_dir = glob.glob('pdfs'+ \"/*\")\n",
    "dico_response = {}\n",
    "\n",
    "\n",
    "def load_json_data(json_file_path):\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "dico_response = load_json_data('benchmarking.json')\n",
    "i = 1\n",
    "true_index = 40\n",
    "for index,file in enumerate(files_in_dir):\n",
    "    print(index,file)\n",
    "    try:\n",
    "        if file.split('/')[1] not in dico_response:\n",
    "            print(true_index,file)\n",
    "            true_index += 1\n",
    "            dico_response[file.split('/')[1]] = benchmarking(file,true_index)\n",
    "        else:\n",
    "            print(\"file already done\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        dico_response[file.split('/')[1]] = None\n",
    "    print(file+\" done\")\n",
    "        \n",
    "with open('benchmarking.json', 'w') as json_file:\n",
    "    json.dump(dico_response, json_file)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
