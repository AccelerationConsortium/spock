{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfa978e",
   "metadata": {},
   "source": [
    "## The LLM bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5994758-ba58-4c48-919e-bf63e0803e37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/gpfs/fs1/home/m/mehrad/brikiyou/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mPackage                   Version\n",
      "------------------------- --------------\n",
      "aiohttp                   3.9.5\n",
      "aiosignal                 1.3.1\n",
      "annotated-types           0.7.0\n",
      "anyio                     4.4.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.2.0\n",
      "Babel                     2.15.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "certifi                   2024.7.4\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "comm                      0.2.2\n",
      "debugpy                   1.8.2\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "executing                 2.0.1\n",
      "fastjsonschema            2.20.0\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.4.1\n",
      "greenlet                  3.0.3\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.5\n",
      "httpx                     0.27.0\n",
      "idna                      3.7\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.26.0\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.4\n",
      "json5                     0.9.25\n",
      "jsonpatch                 1.33\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.22.0\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter_client            8.6.2\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.1\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.2.3\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.2\n",
      "langchain                 0.2.6\n",
      "langchain-core            0.2.11\n",
      "langchain-text-splitters  0.2.2\n",
      "langsmith                 0.1.83\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.0.2\n",
      "multidict                 6.0.5\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.4\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "notebook_shim             0.2.4\n",
      "numpy                     1.26.4\n",
      "orjson                    3.10.6\n",
      "overrides                 7.7.0\n",
      "packaging                 24.1\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pip                       23.3.1\n",
      "platformdirs              4.2.2\n",
      "prometheus_client         0.20.0\n",
      "prompt_toolkit            3.0.47\n",
      "psutil                    6.0.0\n",
      "ptyprocess                0.7.0\n",
      "pure-eval                 0.2.2\n",
      "pycparser                 2.22\n",
      "pydantic                  2.8.2\n",
      "pydantic_core             2.20.1\n",
      "Pygments                  2.18.0\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     26.0.3\n",
      "referencing               0.35.1\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.18.1\n",
      "Send2Trash                1.8.3\n",
      "setuptools                69.0.2\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.5\n",
      "SQLAlchemy                2.0.31\n",
      "stack-data                0.6.3\n",
      "tenacity                  8.4.2\n",
      "terminado                 0.18.1\n",
      "tinycss2                  1.3.0\n",
      "tornado                   6.4.1\n",
      "traitlets                 5.14.3\n",
      "types-python-dateutil     2.9.0.20240316\n",
      "typing_extensions         4.12.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.2\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.6.0\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "wheel                     0.42.0\n",
      "yarl                      1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/gpfs/fs1/home/m/mehrad/brikiyou/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (0.2.6)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain_core in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (0.2.11)\n",
      "Collecting scholarly\n",
      "  Downloading scholarly-1.7.11-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (0.1.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (8.4.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain_core) (24.1)\n",
      "Requirement already satisfied: arrow in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (4.12.3)\n",
      "Collecting bibtexparser (from scholarly)\n",
      "  Downloading bibtexparser-1.4.1.tar.gz (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting deprecated (from scholarly)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting fake-useragent (from scholarly)\n",
      "  Downloading fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting free-proxy (from scholarly)\n",
      "  Downloading free_proxy-1.1.1.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httpx in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (0.27.0)\n",
      "Collecting python-dotenv (from scholarly)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting selenium (from scholarly)\n",
      "  Downloading selenium-4.22.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting sphinx-rtd-theme (from scholarly)\n",
      "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: typing-extensions in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (4.12.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from arrow->scholarly) (2.9.0.post0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from arrow->scholarly) (2.9.0.20240316)\n",
      "Requirement already satisfied: soupsieve>1.2 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from beautifulsoup4->scholarly) (2.5)\n",
      "Collecting pyparsing>=2.0.3 (from bibtexparser->scholarly)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated->scholarly)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting lxml (from free-proxy->scholarly)\n",
      "  Downloading lxml-5.2.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: anyio in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from httpx->scholarly) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from httpx->scholarly) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from httpx->scholarly) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from httpcore==1.*->httpx->scholarly) (0.14.0)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->scholarly)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trio~=0.17 (from selenium->scholarly)\n",
      "  Downloading trio-0.25.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium->scholarly)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: websocket-client>=1.8.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from selenium->scholarly) (1.8.0)\n",
      "Collecting sphinx<8,>=5 (from sphinx-rtd-theme->scholarly)\n",
      "  Downloading sphinx-7.3.7-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting docutils<0.21 (from sphinx-rtd-theme->scholarly)\n",
      "  Downloading docutils-0.20.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->scholarly)\n",
      "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
      "Collecting sphinxcontrib-applehelp (from sphinx<8,>=5->sphinx-rtd-theme->scholarly)\n",
      "  Downloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp (from sphinx<8,>=5->sphinx-rtd-theme->scholarly)\n",
      "  Downloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath (from sphinx<8,>=5->sphinx-rtd-theme->scholarly)\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx<8,>=5->sphinx-rtd-theme->scholarly)\n",
      "  Downloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx<8,>=5->sphinx-rtd-theme->scholarly)\n",
      "  Downloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sphinxcontrib-qthelp (from sphinx<8,>=5->sphinx-rtd-theme->scholarly)\n",
      "  Downloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (3.1.4)\n",
      "Requirement already satisfied: Pygments>=2.14 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.18.0)\n",
      "Collecting snowballstemmer>=2.0 (from sphinx<8,>=5->sphinx-rtd-theme->scholarly)\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: babel>=2.9 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.15.0)\n",
      "Collecting alabaster~=0.7.14 (from sphinx<8,>=5->sphinx-rtd-theme->scholarly)\n",
      "  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx<8,>=5->sphinx-rtd-theme->scholarly)\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium->scholarly)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium->scholarly)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->scholarly)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from Jinja2>=3.0->sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.1.5)\n",
      "Downloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m152.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m468.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m525.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading sphinx-7.3.7-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m550.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.25.1-py3-none-any.whl (467 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.7/467.7 kB\u001b[0m \u001b[31m168.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m510.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lxml-5.2.2-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading alabaster-0.7.16-py3-none-any.whl (13 kB)\n",
      "Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m522.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m539.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m529.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl (120 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m549.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m496.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m540.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: bibtexparser, free-proxy\n",
      "  Building wheel for bibtexparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bibtexparser: filename=bibtexparser-1.4.1-py3-none-any.whl size=43252 sha256=721a9f70c35160046a70d800770833251e3917b637ace41aaac604b5f6116cbe\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0g68872u/wheels/d6/93/c0/58e9981a8f55db19a654503072b50ede34bbcc8e7db5908a84\n",
      "  Building wheel for free-proxy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for free-proxy: filename=free_proxy-1.1.1-py3-none-any.whl size=5637 sha256=a56c16af823281fef0d5b48bfc086658b8e935c162b19d297d431257fe5ee3f6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0g68872u/wheels/c6/7f/3f/b764995ae2502d8642977764577198043d3b6c6738534f5ffe\n",
      "Successfully built bibtexparser free-proxy\n",
      "Installing collected packages: sortedcontainers, snowballstemmer, fake-useragent, wsproto, wrapt, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, python-dotenv, PySocks, pyparsing, outcome, mypy-extensions, marshmallow, lxml, imagesize, docutils, alabaster, typing-inspect, trio, sphinx, free-proxy, deprecated, bibtexparser, trio-websocket, sphinxcontrib-jquery, dataclasses-json, sphinx-rtd-theme, selenium, scholarly, langchain_community\n",
      "Successfully installed PySocks-1.7.1 alabaster-0.7.16 bibtexparser-1.4.1 dataclasses-json-0.6.7 deprecated-1.2.14 docutils-0.20.1 fake-useragent-1.5.1 free-proxy-1.1.1 imagesize-1.4.1 langchain_community-0.2.6 lxml-5.2.2 marshmallow-3.21.3 mypy-extensions-1.0.0 outcome-1.3.0.post0 pyparsing-3.1.2 python-dotenv-1.0.1 scholarly-1.7.11 selenium-4.22.0 snowballstemmer-2.2.0 sortedcontainers-2.4.0 sphinx-7.3.7 sphinx-rtd-theme-2.0.0 sphinxcontrib-applehelp-1.0.8 sphinxcontrib-devhelp-1.0.6 sphinxcontrib-htmlhelp-2.0.5 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.7 sphinxcontrib-serializinghtml-1.1.10 trio-0.25.1 trio-websocket-0.11.1 typing-inspect-0.9.0 wrapt-1.16.0 wsproto-1.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!module load python\n",
    "!pip list\n",
    "!source ../../bin/activate\n",
    "!pip install langchain langchain_community langchain_core scholarly\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader, TextLoader\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "import json\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import requests\n",
    "from scholarly import scholarly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc478dc-d890-4b55-ac5f-fbb144d2585d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PDFPlumberLoader, TextLoader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OllamaEmbeddings\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "\n",
    "class Bot_LLM:\n",
    "    def __init__(self,model='llama3',embed_model='mxbai-embed-large', folder_path='db2'):\n",
    "        self.llm = Ollama(model=model)\n",
    "        self.oembed = OllamaEmbeddings(model=embed_model)\n",
    "        self.folder_path = folder_path\n",
    "        self.vectorestore = None\n",
    "\n",
    "    \n",
    "    def get_topic_publication_abstract(self, abstract:str, input_file:str):\n",
    "        with open(input_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        parser = JsonOutputParser()\n",
    "        \n",
    "        new_text = \"\"\"The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "\n",
    "        As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
    "        the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
    "\n",
    "        Here is the output schema:\n",
    "        ```\n",
    "        {\"topic\": {'Machine Learning: [Keyword1, keyword2, keyword3], 'Batteries: [keyword1, keyword2, keyword3]}\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"Here is a text: {abstract} Please identify the topics from the following list: {liste}. Note: A single text can belong to multiple topics, so please list all relevant topics.  \\n{format_instructions}\"\n",
    "        ,\n",
    "            input_variables=[\"abstract\",\"liste\",\"topics\"],\n",
    "            partial_variables={\"format_instructions\": new_text}\n",
    "        )\n",
    "\n",
    "\n",
    "        chain = prompt | self.llm | parser\n",
    "        topics = chain.invoke({\"abstract\": abstract, \"liste\": data.keys()})\n",
    "        print('Topics: ', topics['topic'])\n",
    "        return topics['topic']\n",
    "\n",
    "    \n",
    "    def rag(self, document:str):\n",
    "        try:\n",
    "            # The document is a pdf file\n",
    "            loader = PDFPlumberLoader(document)\n",
    "            data = loader.load()\n",
    "            chunk_size = 500\n",
    "            chunk_overlap = 20\n",
    "\n",
    "        except:\n",
    "            data = [TextLoader(text).load() for text in document]\n",
    "            data = [item for sublist in data for item in sublist]\n",
    "\n",
    "            \n",
    "        text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        all_splits = text_splitter.split_documents(data)\n",
    "        self.vectorstore = Chroma.from_documents(documents=all_splits, embedding=self.oembed, persist_directory=self.folder_path)\n",
    "        \n",
    "    def query_rag(self, question:str) -> None:\n",
    "        if self.vectorstore:\n",
    "            docs = self.vectorstore.similarity_search(question)\n",
    "            from langchain.chains import RetrievalQA\n",
    "            qachain=RetrievalQA.from_chain_type(self.llm, retriever=self.vectorstore.as_retriever(), verbose=True)\n",
    "            res = qachain.invoke({\"query\": question})\n",
    "            print(res['result'])\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"No documents loaded\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf8061-7979-48aa-8429-b6995127aceb",
   "metadata": {},
   "source": [
    "# The Publication Class: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d60eae0-6833-408f-a2e3-07964944b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Publication:\n",
    "    def __init__(self,publication_filled, llm_use:bool=True) -> None:\n",
    "        self.publication_filled = publication_filled\n",
    "        self.title = self.get_publication_title()\n",
    "        self.abstract = self.get_publication_abstract().lower()\n",
    "        self.author = self.get_author_name()\n",
    "        self.year = self.get_year()\n",
    "        self.url = self.get_publication_url()\n",
    "        self.citation = self.get_citation()\n",
    "        self.pdf = self.get_pdf()\n",
    "        self.topic = None\n",
    "    \n",
    "      \n",
    "      \n",
    "    '''  \n",
    "    def get_topic(self,output_file=\"json/ouput.json\", # à voir cette histoire avec get_topic et __get_topic\n",
    "                  input_file=\"json/response.json\") -> list[str]:\n",
    "        try:\n",
    "            with open(output_file,'r') as file:\n",
    "                data = json.load(file)\n",
    "            return data[self.author]['topic']\n",
    "        except Exception as e:\n",
    "            return self.__get_topic(input_file)\n",
    "     '''   \n",
    "    def get_publication_url(self) -> str:\n",
    "        return self.publication_filled['pub_url']\n",
    "    \n",
    "    def get_publication_title(self) -> str:\n",
    "        return self.publication_filled['bib']['title'] \n",
    "\n",
    "    def get_publication_abstract(self) -> str:\n",
    "        return self.publication_filled['bib']['abstract']\n",
    "\n",
    "    def get_author_name(self) -> str:\n",
    "        return self.publication_filled['bib']['author']\n",
    "\n",
    "    def get_year(self) -> str:\n",
    "        return self.publication_filled['bib']['pub_year']\n",
    "    \n",
    "    def get_citation(self) -> str:\n",
    "        return self.publication_filled['bib']['citation']\n",
    "    \n",
    "    def get_topic(self,llm,input_file=\"json/response.json\") -> None:\n",
    "        self.topic: dict = llm.get_topic_publication_abstract(abstract=self.abstract,input_file=input_file)\n",
    "        return self.topic\n",
    "    \n",
    "    def get_pdf(self):\n",
    "        url = f\"https://scholar.google.com/scholar?q={self.title}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            html_content = response.text\n",
    "            try:\n",
    "                return self.__parse_google_scholar(html_content)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        self.available_attributes = {'title': self.title if self.title is not None else 'N/A',\n",
    "                                     'abstract' : self.abstract if self.abstract is not None else 'N/A',\n",
    "                                        'author': self.author if self.author is not None else 'N/A',\n",
    "                                        'year': self.year if self.year is not None else 'N/A',\n",
    "                                        'url': self.url if self.url is not None else 'N/A',\n",
    "                                        'citation': self.citation if self.citation is not None else 'N/A',\n",
    "                                        'pdf': self.pdf if self.pdf is not None else 'N/A',\n",
    "                                        'topic': self.topic if self.topic is not None else 'N/A'}\n",
    "        return str(self.available_attributes)\n",
    "\n",
    "\n",
    "        \n",
    "    def __parse_google_scholar(self,html_content):\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        a_tags = soup.find_all('a')\n",
    "        try:\n",
    "            pdf_link = [a['href'] for a in a_tags if 'href' in a.attrs and '.pdf' in a['href']][0]\n",
    "            print(f\"PDF link found: {pdf_link}\")\n",
    "            return pdf_link\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "    \n",
    "    def download_pdf(self,path):\n",
    "        \n",
    "        import requests\n",
    "        path = path + self.title + \".pdf\"\n",
    "        \n",
    "        if self.pdf is not None:\n",
    "            try:\n",
    "                response = requests.get(self.pdf)\n",
    "                if response.status_code == 200:\n",
    "                    with open(path, 'wb') as file:\n",
    "                        file.write(response.content)\n",
    "                    print(f\"PDF successfully downloaded and saved to {path}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download the PDF. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"An error occurred while downloading the PDF: {e}\")\n",
    "        else:\n",
    "            from scidownl import scihub_download\n",
    "            #  scidownl by title\n",
    "            try:\n",
    "                scihub_download(self.title, paper_type=\"title\", out=path)\n",
    "            except:\n",
    "                try:\n",
    "                    # By URL\n",
    "                    scihub_download(self.pdf, out=path)\n",
    "                except:\n",
    "                    print(\"Couldn't download the PDF\")\n",
    "           \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "     \n",
    "                    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42468463-5008-440a-a710-bd4a73d66915",
   "metadata": {},
   "source": [
    "# The Author\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48476c3c-3a8c-4503-bbf3-68347e88a9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nauthor = Author('Mehrad Ansari')\\npub = Publication(author.get_last_publication())\\n\\npub.download_pdf('pdfs/')\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Author:\n",
    "    def __init__(self, author):\n",
    "        \"\"\"\n",
    "        Initialize an Author object.\n",
    "\n",
    "        Args:\n",
    "            author (str): The name of the author.\n",
    "        \"\"\"\n",
    "        self.author_name = author\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return a string representation of the Author object.\n",
    "\n",
    "        Returns:\n",
    "            str: The name of the author.\n",
    "        \"\"\"\n",
    "        return self.author_name\n",
    "\n",
    "    def get_last_publication(self):\n",
    "        \"\"\"\n",
    "        Get the last publication of the author.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dict containing information about the last publication.\n",
    "        \"\"\"\n",
    "        search_query = scholarly.search_author(self.author_name)\n",
    "        first_author_result = next(search_query)\n",
    "        author = scholarly.fill(first_author_result)\n",
    "        first_publication = sorted(author['publications'], \n",
    "                                   key=lambda x: int(x['bib']['pub_year'])\n",
    "                                   if 'pub_year' in x['bib'] else 0, \n",
    "                                   reverse=True)[0]\n",
    "        first_publication_filled = scholarly.fill(first_publication)\n",
    "        return first_publication_filled\n",
    "\n",
    "    def setup_author(self, output_file, llm):\n",
    "        \"\"\"\n",
    "        Setup the author by adding their last publication to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            output_file (str): The path to the JSON file.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        with open(output_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        author_last_publication = Publication(self.get_last_publication())\n",
    "        \n",
    "\n",
    "        \n",
    "        data[self.author_name] = {\n",
    "            \"title\": author_last_publication.title,\n",
    "            \"abstract\": author_last_publication.abstract,\n",
    "            \"topic\": author_last_publication.get_topic(llm=llm), \n",
    "            \"author\": author_last_publication.author, \n",
    "            \"year\": author_last_publication.year,\n",
    "            \"url\": author_last_publication.url,\n",
    "            \"pdf\": author_last_publication.pdf,\n",
    "        }\n",
    "        \n",
    "        \n",
    "        with open(output_file, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "\n",
    "'''\n",
    "author = Author('Mehrad Ansari')\n",
    "pub = Publication(author.get_last_publication())\n",
    "\n",
    "pub.download_pdf('pdfs/')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc875ae1-baaf-468b-99ed-bd2dd9bde049",
   "metadata": {},
   "source": [
    "## Main module to test function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa3679f7-7f70-4168-a95e-5a07f34f01dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author object created successfully\n",
      "PDF link found: https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/640a28a76642bf8c8f413f32/original/history-agnostic-battery-degradation-inference.pdf\n",
      "Publication object created successfully - Having fetched the last publication\n",
      "current available attributes: \n",
      "{'title': 'History-agnostic battery degradation inference', 'abstract': 'lithium-ion batteries (libs) have attracted widespread attention as an efficient energy storage device on electric vehicles (ev) to achieve emission-free mobility. however, the performance of libs deteriorates with time and usage, and the state of health of used batteries are difficult to quantify. having accurate estimations of a battery’s remaining life across different life stages would benefit maintenance, safety, and serve as a means of qualifying used batteries for second-life applications. since the full history of a battery may not always be available in downstream applications, in this study, we demonstrate a deep learning framework that enables dynamic degradation rate prediction, including both short-term and long-term forecasting, while requiring only the most recent battery usage information. specifically, our model takes a rolling window of current and voltage time-series inputs, and predicts the near-term and …', 'author': 'Mehrad Ansari and Steven B Torrisi and Amalie Trewartha and Shijing Sun', 'year': 2024, 'url': 'https://www.sciencedirect.com/science/article/pii/S2352152X23036782', 'citation': 'Journal of Energy Storage 81, 110279, 2024', 'pdf': 'https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/640a28a76642bf8c8f413f32/original/history-agnostic-battery-degradation-inference.pdf', 'topic': 'N/A'}\n",
      "Running LLM on the abstract of the publication\n",
      "Topics:  {'Machine Learning': ['deep learning framework', 'dynamic degradation rate prediction', 'rolling window'], 'Batteries': ['lithium-ion batteries', 'electric vehicles', 'state of health', 'remaining life', 'second-life applications']}\n",
      "current available attributes: \n",
      "{'title': 'History-agnostic battery degradation inference', 'abstract': 'lithium-ion batteries (libs) have attracted widespread attention as an efficient energy storage device on electric vehicles (ev) to achieve emission-free mobility. however, the performance of libs deteriorates with time and usage, and the state of health of used batteries are difficult to quantify. having accurate estimations of a battery’s remaining life across different life stages would benefit maintenance, safety, and serve as a means of qualifying used batteries for second-life applications. since the full history of a battery may not always be available in downstream applications, in this study, we demonstrate a deep learning framework that enables dynamic degradation rate prediction, including both short-term and long-term forecasting, while requiring only the most recent battery usage information. specifically, our model takes a rolling window of current and voltage time-series inputs, and predicts the near-term and …', 'author': 'Mehrad Ansari and Steven B Torrisi and Amalie Trewartha and Shijing Sun', 'year': 2024, 'url': 'https://www.sciencedirect.com/science/article/pii/S2352152X23036782', 'citation': 'Journal of Energy Storage 81, 110279, 2024', 'pdf': 'https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/640a28a76642bf8c8f413f32/original/history-agnostic-battery-degradation-inference.pdf', 'topic': {'Machine Learning': ['deep learning framework', 'dynamic degradation rate prediction', 'rolling window'], 'Batteries': ['lithium-ion batteries', 'electric vehicles', 'state of health', 'remaining life', 'second-life applications']}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "author = Author('Mehrad Ansari')\n",
    "\n",
    "print(\"Author object created successfully\")\n",
    "\n",
    "pub = Publication(author.get_last_publication())\n",
    "\n",
    "print(\"Publication object created successfully - Having fetched the last publication\")\n",
    "\n",
    "print('current available attributes: ')\n",
    "print(pub)\n",
    "\n",
    "print('Running LLM on the abstract of the publication')\n",
    "pub.get_topic(llm=Bot_LLM())\n",
    "print('current available attributes: ')\n",
    "print(pub)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spock_package",
   "language": "python",
   "name": "spock_package"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
