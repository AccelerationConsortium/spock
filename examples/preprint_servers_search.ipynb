{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv \n",
    "import requests\n",
    "import os \n",
    "import re \n",
    "import urllib.request \n",
    "import json \n",
    "from bs4 import BeautifulSoup \n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "def standard_url_alphanumeric(input_string):\n",
    "    allowed_characters = r'[a-zA-Z0-9\\-._~:/?#\\[\\]@!$&\\'()*+,;=%]'\n",
    "    cleaned_string = re.sub(r'[^\\w\\s-]', ' ', input_string) \n",
    "    cleaned_string = re.sub(r'[-\\s]+', '-', cleaned_string)\n",
    "    cleaned_string = cleaned_string.strip(\"-\")\n",
    "    cleaned_string = \"\".join(re.findall(allowed_characters, cleaned_string))\n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    return cleaned_string.strip('-')\n",
    "\n",
    "\n",
    "def standard_title_alphanumeric(input_string):\n",
    "    allowed_characters = r'[a-zA-Z0-9\\-._~:/?#\\[\\]@!$&\\'()*+,;=%]'\n",
    "    cleaned_string = re.sub(r'[^\\w\\s-]', ' ', input_string) \n",
    "    cleaned_string = re.sub(r'[-\\s]+', '_', cleaned_string)\n",
    "    cleaned_string = cleaned_string.strip(\"_\")\n",
    "    cleaned_string = \"\".join(re.findall(allowed_characters, cleaned_string))\n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    return cleaned_string.strip('_')\n",
    "\n",
    "\n",
    "def arxiv_downloader(article_title: str, directory: str):\n",
    "    search = arxiv.Search(\n",
    "        query = article_title, \n",
    "        max_results = 1, \n",
    "        sort_by = arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    client = arxiv.Client()\n",
    "    results = client.results(search)\n",
    "    for result in results:\n",
    "        if result.title.lower() == article_title.lower():\n",
    "            print(\"Article found on arXiv.\")\n",
    "            url = result.pdf_url \n",
    "            response = requests.get(url, stream = True)\n",
    "            os.makedirs(directory, exist_ok = True)\n",
    "            article_title = standard_title_alphanumeric(article_title).lower()\n",
    "            total_size = int(response.headers.get(\"content-length\", 0))\n",
    "            with open(f\"{article_title}.pdf\", \"wb\") as f:\n",
    "\n",
    "                with tqdm(total = total_size, unit = \"B\", unit_scale = True, desc = \"Downloading...\") as pbar:\n",
    "                    for chunk in response.iter_content(chunk_size = 1024):\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "        else:\n",
    "            print(\"Article not found on arXiv.\")\n",
    "\n",
    "\n",
    "def chemrxiv_downloader(article_title: str, directory: str, max_results: int = 1):\n",
    "    search_query = article_title.split()\n",
    "    chemrxiv_url = f'https://chemrxiv.org/engage/chemrxiv/public-api/v1/items?term={\"%20\".join(search_query)}&sort=RELEVANT_DESC&limit={max_results}'\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(\n",
    "            url = chemrxiv_url, \n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        )\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            s = response.read() \n",
    "            jsonResponse = json.loads(s.decode(\"utf-8\"))\n",
    "            ids = [item[\"item\"][\"id\"] for item in jsonResponse[\"itemHits\"]]\n",
    "            titles = [item[\"item\"][\"title\"].replace(\"\\n\", \"\") for item in jsonResponse[\"itemHits\"]]\n",
    "            titles = [standard_title_alphanumeric(title) for title in titles]\n",
    "            pdfs = [\"https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/\" + id + \"/original/\" for id in ids]\n",
    "            pdfs = [pdf + title + \".pdf\" for pdf, title in zip(pdfs, titles)]\n",
    "            article_title = standard_title_alphanumeric(article_title)\n",
    "            title = titles[0]\n",
    "            pdf = pdfs[0]\n",
    "            if title.lower() == article_title.lower():\n",
    "                print(\"Article found on ChemRxiv.\")\n",
    "                os.makedirs(directory, exist_ok = True)\n",
    "                response = requests.get(pdf, stream = True)\n",
    "                total_size = int(response.headers.get(\"content-length\", 0))\n",
    "                with open(f\"{article_title.lower()}.pdf\", \"wb\") as f:\n",
    "                    with tqdm(total = total_size, unit = \"B\", unit_scale = True, desc = \"Downloading...\") as pbar:\n",
    "                        for chunk in response.iter_content(chunk_size = 1024):\n",
    "                            f.write(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "\n",
    "            else:\n",
    "                print(\"Article not found on ChemRxiv.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return \"An error occurred.\"\n",
    "    \n",
    "\n",
    "def biorxiv_downloader(article_title: str, directory: str):\n",
    "    search_query = article_title.split()\n",
    "    search_query = \"%252B\".join(search_query)\n",
    "    biorxiv_url = f\"https://www.biorxiv.org/search/\" + search_query\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(\n",
    "            url = biorxiv_url, \n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        )\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            s = response.read() \n",
    "            soup = BeautifulSoup(s, \"html.parser\")\n",
    "            titles = soup.find_all('span', class_='highwire-cite-title')\n",
    "            titles = [title.get_text(strip = True) for title in titles]\n",
    "            articles = soup.find_all(\"a\", class_='highwire-cite-linked-title')\n",
    "            links = [f\"https://www.biorxiv.org{article['href']}\" for article in articles]\n",
    "            pdfs = [link + \".full.pdf\" for link in links]\n",
    "            titles = titles[1::int(len(titles) / len(links))]\n",
    "\n",
    "            title = titles[0]\n",
    "            pdf = pdfs[0]\n",
    "\n",
    "            title = standard_title_alphanumeric(title)\n",
    "            article_title = standard_title_alphanumeric(article_title)\n",
    "\n",
    "            if title.lower() == article_title.lower():\n",
    "                print(\"Article found on bioRxiv.\")\n",
    "                os.makedirs(directory, exist_ok = True)\n",
    "                response = requests.get(pdf, stream = True)\n",
    "                total_size = int(response.headers.get(\"content-length\", 0))\n",
    "                with open(f\"{article_title.lower()}.pdf\", \"wb\") as f:\n",
    "                    with tqdm(total = total_size, unit = \"B\", unit_scale = True, desc = \"Downloading...\") as pbar:\n",
    "                        for chunk in response.iter_content(chunk_size = 1024):\n",
    "                            f.write(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "                            \n",
    "            else:\n",
    "                print(\"Article not found on bioRxiv.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return \"An error occurred.\"\n",
    "    \n",
    "\n",
    "def medrxiv_downloader(article_title: str, directory: str):\n",
    "    search_query = article_title.split()\n",
    "    search_query = \"%252B\".join(search_query)\n",
    "    biorxiv_url = f\"https://www.medrxiv.org/search/\" + search_query\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(\n",
    "            url = biorxiv_url, \n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        )\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            s = response.read() \n",
    "            soup = BeautifulSoup(s, \"html.parser\")\n",
    "            titles = soup.find_all('span', class_='highwire-cite-title')\n",
    "            titles = [title.get_text(strip = True) for title in titles]\n",
    "            articles = soup.find_all(\"a\", class_='highwire-cite-linked-title')\n",
    "            links = [f\"https://www.medrxiv.org{article['href']}\" for article in articles]\n",
    "            pdfs = [link + \".full.pdf\" for link in links]\n",
    "            titles = titles[1::int(len(titles) / len(links))]\n",
    "\n",
    "            title = titles[0]\n",
    "            pdf = pdfs[0]\n",
    "\n",
    "            title = standard_title_alphanumeric(title)\n",
    "            article_title = standard_title_alphanumeric(article_title)\n",
    "\n",
    "            if title.lower() == article_title.lower():\n",
    "                print(\"Article found on medRxiv.\")\n",
    "                os.makedirs(directory, exist_ok = True)\n",
    "                response = requests.get(pdf, stream = True)\n",
    "                total_size = int(response.headers.get(\"content-length\", 0))\n",
    "                with open(f\"{article_title.lower()}.pdf\", \"wb\") as f:\n",
    "                    with tqdm(total = total_size, unit = \"B\", unit_scale = True, desc = \"Downloading...\") as pbar:\n",
    "                        for chunk in response.iter_content(chunk_size = 1024):\n",
    "                            f.write(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "                            \n",
    "            else:\n",
    "                print(\"Article not found on medRxiv.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return \"An error occurred.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article found on arXiv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...: 100%|██████████| 8.63M/8.63M [00:01<00:00, 4.56MB/s]\n"
     ]
    }
   ],
   "source": [
    "title = \"High-Tc superconductor candidates proposed by machine learning\"\n",
    "arxiv_downloader(\n",
    "    title, \n",
    "    \"/home/siwoo/PythonScripts/Natural_Language_Processing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article found on ChemRxiv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...: 100%|██████████| 9.96M/9.96M [00:02<00:00, 4.11MB/s]\n"
     ]
    }
   ],
   "source": [
    "title = \"Swern oxidation transition state theory is ok\"\n",
    "chemrxiv_downloader(\n",
    "    title, \n",
    "    \"/home/siwoo/PythonScripts/Natural_Language_Processing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article found on bioRxiv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...: 7.32MB [00:01, 6.29MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "title = \"Functional identification of soluble uric acid as an endogenous inhibitor of CD38\"\n",
    "biorxiv_downloader(\n",
    "    title, \n",
    "    \"/home/siwoo/PythonScripts/Natural_Language_Processing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article found on medRxiv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...: 100%|██████████| 9.45M/9.45M [00:02<00:00, 3.25MB/s]\n"
     ]
    }
   ],
   "source": [
    "title = \"Complex patterns of multimorbidity associated with severe COVID-19 and Long COVID\"\n",
    "medrxiv_downloader(\n",
    "    title, \n",
    "    \"/home/siwoo/PythonScripts/Natural_Language_Processing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_search_and_download(article_title: str, directory: str, pre_print: str):\n",
    "    allowed_options = {\"arxiv\", \"chemrxiv\", \"biorxiv\", \"medrxiv\"}\n",
    "    if pre_print in allowed_options:\n",
    "        if pre_print == \"arxiv\":\n",
    "            return arxiv_downloader(article_title, directory)\n",
    "        if pre_print == \"chemrxiv\":\n",
    "            return chemrxiv_downloader(article_title, directory)\n",
    "        if pre_print == \"biorxiv\":\n",
    "            return biorxiv_downloader(article_title, directory)\n",
    "        if pre_print == \"medrxiv\":\n",
    "            medrxiv_downloader(article_title, directory)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid option '{pre_print}'. Allowed options are: {allowed_options}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article found on arXiv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...: 100%|██████████| 8.63M/8.63M [00:01<00:00, 5.26MB/s]\n"
     ]
    }
   ],
   "source": [
    "article_search_and_download(\n",
    "    \"High-Tc superconductor candidates proposed by machine learning\", \n",
    "    \"/home/siwoo/PythonScripts/Natural_Language_Processing\", \n",
    "    \"arxiv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
