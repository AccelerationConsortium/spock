{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfa978e",
   "metadata": {},
   "source": [
    "## The LLM bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5994758-ba58-4c48-919e-bf63e0803e37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/gpfs/fs1/home/m/mehrad/brikiyou/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mPackage                                  Version\n",
      "---------------------------------------- ---------------\n",
      "aiohttp                                  3.9.5\n",
      "aiosignal                                1.3.1\n",
      "alabaster                                0.7.16\n",
      "annotated-types                          0.7.0\n",
      "anyio                                    4.4.0\n",
      "argon2-cffi                              23.1.0\n",
      "argon2-cffi-bindings                     21.2.0\n",
      "arrow                                    1.3.0\n",
      "arxiv                                    2.1.3\n",
      "asgiref                                  3.8.1\n",
      "asttokens                                2.4.1\n",
      "async-lru                                2.0.4\n",
      "attrs                                    23.2.0\n",
      "Babel                                    2.15.0\n",
      "backoff                                  2.2.1\n",
      "bcrypt                                   4.1.3\n",
      "beautifulsoup4                           4.12.3\n",
      "bibtexparser                             1.4.1\n",
      "bleach                                   6.1.0\n",
      "build                                    1.2.1\n",
      "cachetools                               5.3.3\n",
      "certifi                                  2024.7.4\n",
      "cffi                                     1.16.0\n",
      "charset-normalizer                       3.3.2\n",
      "chroma-hnswlib                           0.7.3\n",
      "chromadb                                 0.5.3\n",
      "click                                    8.1.7\n",
      "coloredlogs                              15.0.1\n",
      "comm                                     0.2.2\n",
      "cryptography                             42.0.8\n",
      "dataclasses-json                         0.6.7\n",
      "debugpy                                  1.8.2\n",
      "decorator                                5.1.1\n",
      "defusedxml                               0.7.1\n",
      "Deprecated                               1.2.14\n",
      "dill                                     0.3.8\n",
      "distro                                   1.9.0\n",
      "dnspython                                2.6.1\n",
      "docutils                                 0.20.1\n",
      "email_validator                          2.2.0\n",
      "executing                                2.0.1\n",
      "fake-useragent                           1.5.1\n",
      "fastapi                                  0.111.0\n",
      "fastapi-cli                              0.0.4\n",
      "fastjsonschema                           2.20.0\n",
      "feedparser                               6.0.11\n",
      "filelock                                 3.15.4\n",
      "flatbuffers                              24.3.25\n",
      "fqdn                                     1.5.1\n",
      "free-proxy                               1.1.1\n",
      "frozenlist                               1.4.1\n",
      "fsspec                                   2024.6.1\n",
      "google-auth                              2.31.0\n",
      "googleapis-common-protos                 1.63.2\n",
      "greenlet                                 3.0.3\n",
      "grpcio                                   1.64.1\n",
      "h11                                      0.14.0\n",
      "httpcore                                 1.0.5\n",
      "httptools                                0.6.1\n",
      "httpx                                    0.27.0\n",
      "huggingface-hub                          0.23.4\n",
      "humanfriendly                            10.0\n",
      "idna                                     3.7\n",
      "imagesize                                1.4.1\n",
      "importlib_metadata                       7.1.0\n",
      "importlib_resources                      6.4.0\n",
      "iniconfig                                2.0.0\n",
      "ipykernel                                6.29.5\n",
      "ipython                                  8.26.0\n",
      "ipywidgets                               8.1.3\n",
      "isoduration                              20.11.0\n",
      "jedi                                     0.19.1\n",
      "Jinja2                                   3.1.4\n",
      "json5                                    0.9.25\n",
      "jsonpatch                                1.33\n",
      "jsonpointer                              3.0.0\n",
      "jsonschema                               4.22.0\n",
      "jsonschema-specifications                2023.12.1\n",
      "jupyter                                  1.0.0\n",
      "jupyter_client                           8.6.2\n",
      "jupyter-console                          6.6.3\n",
      "jupyter_core                             5.7.2\n",
      "jupyter-events                           0.10.0\n",
      "jupyter-lsp                              2.2.5\n",
      "jupyter_server                           2.14.1\n",
      "jupyter_server_terminals                 0.5.3\n",
      "jupyterlab                               4.2.3\n",
      "jupyterlab_pygments                      0.3.0\n",
      "jupyterlab_server                        2.27.2\n",
      "jupyterlab_widgets                       3.0.11\n",
      "kubernetes                               30.1.0\n",
      "langchain                                0.2.6\n",
      "langchain-community                      0.2.6\n",
      "langchain-core                           0.2.21\n",
      "langchain-experimental                   0.0.63\n",
      "langchain-openai                         0.1.17\n",
      "langchain-text-splitters                 0.2.2\n",
      "langchainhub                             0.1.20\n",
      "langsmith                                0.1.83\n",
      "loguru                                   0.7.2\n",
      "lxml                                     5.2.2\n",
      "markdown-it-py                           3.0.0\n",
      "MarkupSafe                               2.1.5\n",
      "marshmallow                              3.21.3\n",
      "matplotlib-inline                        0.1.7\n",
      "mdurl                                    0.1.2\n",
      "mistune                                  3.0.2\n",
      "mmh3                                     4.1.0\n",
      "monotonic                                1.6\n",
      "mpmath                                   1.3.0\n",
      "multidict                                6.0.5\n",
      "multiprocess                             0.70.16\n",
      "mypy-extensions                          1.0.0\n",
      "nbclient                                 0.10.0\n",
      "nbconvert                                7.16.4\n",
      "nbformat                                 5.10.4\n",
      "nest-asyncio                             1.6.0\n",
      "networkx                                 3.3\n",
      "notebook                                 7.2.1\n",
      "notebook_shim                            0.2.4\n",
      "numpy                                    1.26.4\n",
      "nvidia-cublas-cu12                       12.1.3.1\n",
      "nvidia-cuda-cupti-cu12                   12.1.105\n",
      "nvidia-cuda-nvrtc-cu12                   12.1.105\n",
      "nvidia-cuda-runtime-cu12                 12.1.105\n",
      "nvidia-cudnn-cu12                        8.9.2.26\n",
      "nvidia-cufft-cu12                        11.0.2.54\n",
      "nvidia-curand-cu12                       10.3.2.106\n",
      "nvidia-cusolver-cu12                     11.4.5.107\n",
      "nvidia-cusparse-cu12                     12.1.0.106\n",
      "nvidia-nccl-cu12                         2.20.5\n",
      "nvidia-nvjitlink-cu12                    12.5.82\n",
      "nvidia-nvtx-cu12                         12.1.105\n",
      "oauthlib                                 3.2.2\n",
      "ollama                                   0.3.0\n",
      "onnxruntime                              1.18.1\n",
      "openai                                   1.35.15\n",
      "opentelemetry-api                        1.25.0\n",
      "opentelemetry-exporter-otlp-proto-common 1.25.0\n",
      "opentelemetry-exporter-otlp-proto-grpc   1.25.0\n",
      "opentelemetry-instrumentation            0.46b0\n",
      "opentelemetry-instrumentation-asgi       0.46b0\n",
      "opentelemetry-instrumentation-fastapi    0.46b0\n",
      "opentelemetry-proto                      1.25.0\n",
      "opentelemetry-sdk                        1.25.0\n",
      "opentelemetry-semantic-conventions       0.46b0\n",
      "opentelemetry-util-http                  0.46b0\n",
      "orjson                                   3.10.6\n",
      "outcome                                  1.3.0.post0\n",
      "overrides                                7.7.0\n",
      "packaging                                24.1\n",
      "pandocfilters                            1.5.1\n",
      "parso                                    0.8.4\n",
      "pdfminer.six                             20231228\n",
      "pdfplumber                               0.11.1\n",
      "pexpect                                  4.9.0\n",
      "pillow                                   10.4.0\n",
      "pip                                      23.3.1\n",
      "platformdirs                             4.2.2\n",
      "pluggy                                   1.5.0\n",
      "posthog                                  3.5.0\n",
      "prometheus_client                        0.20.0\n",
      "prompt_toolkit                           3.0.47\n",
      "protobuf                                 4.25.3\n",
      "psutil                                   6.0.0\n",
      "ptyprocess                               0.7.0\n",
      "pure-eval                                0.2.2\n",
      "pyasn1                                   0.6.0\n",
      "pyasn1_modules                           0.4.0\n",
      "pycparser                                2.22\n",
      "pydantic                                 2.8.2\n",
      "pydantic_core                            2.20.1\n",
      "Pygments                                 2.18.0\n",
      "pyparsing                                3.1.2\n",
      "pypdfium2                                4.30.0\n",
      "PyPika                                   0.48.9\n",
      "pyproject_hooks                          1.1.0\n",
      "PySocks                                  1.7.1\n",
      "pytest                                   8.3.1\n",
      "python-dateutil                          2.9.0.post0\n",
      "python-dotenv                            1.0.1\n",
      "python-json-logger                       2.0.7\n",
      "python-multipart                         0.0.9\n",
      "PyYAML                                   6.0.1\n",
      "pyzmq                                    26.0.3\n",
      "qtconsole                                5.5.2\n",
      "QtPy                                     2.4.1\n",
      "referencing                              0.35.1\n",
      "regex                                    2024.5.15\n",
      "requests                                 2.32.3\n",
      "requests-oauthlib                        2.0.0\n",
      "rfc3339-validator                        0.1.4\n",
      "rfc3986-validator                        0.1.1\n",
      "rich                                     13.7.1\n",
      "rpds-py                                  0.18.1\n",
      "rsa                                      4.9\n",
      "scholarly                                1.7.11\n",
      "scidownl                                 1.0.2\n",
      "selenium                                 4.22.0\n",
      "Send2Trash                               1.8.3\n",
      "setuptools                               69.0.2\n",
      "sgmllib3k                                1.0.0\n",
      "shellingham                              1.5.4\n",
      "six                                      1.16.0\n",
      "sniffio                                  1.3.1\n",
      "snowballstemmer                          2.2.0\n",
      "sortedcontainers                         2.4.0\n",
      "soupsieve                                2.5\n",
      "Sphinx                                   7.3.7\n",
      "sphinx-rtd-theme                         2.0.0\n",
      "sphinxcontrib-applehelp                  1.0.8\n",
      "sphinxcontrib-devhelp                    1.0.6\n",
      "sphinxcontrib-htmlhelp                   2.0.5\n",
      "sphinxcontrib-jquery                     4.1\n",
      "sphinxcontrib-jsmath                     1.0.1\n",
      "sphinxcontrib-qthelp                     1.0.7\n",
      "sphinxcontrib-serializinghtml            1.1.10\n",
      "SQLAlchemy                               2.0.31\n",
      "stack-data                               0.6.3\n",
      "starlette                                0.37.2\n",
      "sympy                                    1.12.1\n",
      "tablib                                   3.6.1\n",
      "tabulate                                 0.9.0\n",
      "tenacity                                 8.4.2\n",
      "terminado                                0.18.1\n",
      "tiktoken                                 0.7.0\n",
      "tinycss2                                 1.3.0\n",
      "tokenizers                               0.19.1\n",
      "torch                                    2.3.1\n",
      "tornado                                  6.4.1\n",
      "tqdm                                     4.66.4\n",
      "traitlets                                5.14.3\n",
      "trio                                     0.25.1\n",
      "trio-websocket                           0.11.1\n",
      "triton                                   2.3.1\n",
      "typer                                    0.12.3\n",
      "types-python-dateutil                    2.9.0.20240316\n",
      "types-requests                           2.32.0.20240712\n",
      "typing_extensions                        4.12.2\n",
      "typing-inspect                           0.9.0\n",
      "ujson                                    5.10.0\n",
      "uri-template                             1.3.0\n",
      "urllib3                                  2.2.2\n",
      "uvicorn                                  0.30.1\n",
      "uvloop                                   0.19.0\n",
      "watchfiles                               0.22.0\n",
      "wcwidth                                  0.2.13\n",
      "webcolors                                24.6.0\n",
      "webencodings                             0.5.1\n",
      "websocket-client                         1.8.0\n",
      "websockets                               12.0\n",
      "wget                                     3.2\n",
      "wheel                                    0.42.0\n",
      "widgetsnbextension                       4.0.11\n",
      "wrapt                                    1.16.0\n",
      "wsproto                                  1.2.0\n",
      "yarl                                     1.9.4\n",
      "zipp                                     3.19.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/gpfs/fs1/home/m/mehrad/brikiyou/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (0.2.6)\n",
      "Requirement already satisfied: langchain_community in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (0.2.6)\n",
      "Requirement already satisfied: langchain_core in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (0.2.21)\n",
      "Requirement already satisfied: scholarly in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (1.7.11)\n",
      "Requirement already satisfied: pdfplumber in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (0.11.1)\n",
      "Requirement already satisfied: langchain_experimental in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (0.0.63)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (0.1.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain) (8.4.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langchain_core) (24.1)\n",
      "Requirement already satisfied: arrow in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (4.12.3)\n",
      "Requirement already satisfied: bibtexparser in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (1.4.1)\n",
      "Requirement already satisfied: deprecated in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (1.2.14)\n",
      "Requirement already satisfied: fake-useragent in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (1.5.1)\n",
      "Requirement already satisfied: free-proxy in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (1.1.1)\n",
      "Requirement already satisfied: httpx in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (0.27.0)\n",
      "Requirement already satisfied: python-dotenv in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (1.0.1)\n",
      "Requirement already satisfied: selenium in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (4.22.0)\n",
      "Requirement already satisfied: sphinx-rtd-theme in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from scholarly) (4.12.2)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from pdfminer.six==20231228->pdfplumber) (42.0.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from arrow->scholarly) (2.9.0.post0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from arrow->scholarly) (2.9.0.20240316)\n",
      "Requirement already satisfied: soupsieve>1.2 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from beautifulsoup4->scholarly) (2.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from bibtexparser->scholarly) (3.1.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from deprecated->scholarly) (1.16.0)\n",
      "Requirement already satisfied: lxml in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from free-proxy->scholarly) (5.2.2)\n",
      "Requirement already satisfied: anyio in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from httpx->scholarly) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from httpx->scholarly) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from httpx->scholarly) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from httpcore==1.*->httpx->scholarly) (0.14.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from requests[socks]->scholarly) (1.7.1)\n",
      "Requirement already satisfied: trio~=0.17 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from selenium->scholarly) (0.25.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from selenium->scholarly) (0.11.1)\n",
      "Requirement already satisfied: websocket-client>=1.8.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from selenium->scholarly) (1.8.0)\n",
      "Requirement already satisfied: sphinx<8,>=5 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx-rtd-theme->scholarly) (7.3.7)\n",
      "Requirement already satisfied: docutils<0.21 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx-rtd-theme->scholarly) (0.20.1)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx-rtd-theme->scholarly) (4.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.0.8)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.0.6)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.5)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.1.10)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.0.7)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (3.1.4)\n",
      "Requirement already satisfied: Pygments>=2.14 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.18.0)\n",
      "Requirement already satisfied: snowballstemmer>=2.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.9 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.15.0)\n",
      "Requirement already satisfied: alabaster~=0.7.14 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (0.7.16)\n",
      "Requirement already satisfied: imagesize>=1.3 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.4.1)\n",
      "Requirement already satisfied: sortedcontainers in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from trio~=0.17->selenium->scholarly) (2.4.0)\n",
      "Requirement already satisfied: outcome in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from trio~=0.17->selenium->scholarly) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium->scholarly) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: pycparser in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from Jinja2>=3.0->sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!module load python\n",
    "!pip list\n",
    "!pip install langchain langchain_community langchain_core scholarly pdfplumber langchain_experimental\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader, TextLoader\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "import json\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import requests\n",
    "from scholarly import scholarly\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc478dc-d890-4b55-ac5f-fbb144d2585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "class Bot_LLM:\n",
    "    def __init__(self,model='llama3',embed_model='mxbai-embed-large', folder_path='db2'):\n",
    "        self.llm = Ollama(model=model)\n",
    "        self.oembed = OllamaEmbeddings(model=embed_model)\n",
    "        self.folder_path = folder_path\n",
    "        self.vectorestore = None\n",
    "\n",
    "    \n",
    "    def get_topic_publication_abstract(self, abstract:str, input_file:str):\n",
    "        with open(input_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        parser = JsonOutputParser()\n",
    "        \n",
    "        new_text = \"\"\"The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "\n",
    "        As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
    "        the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
    "\n",
    "        Here is the output schema:\n",
    "        ```\n",
    "        {\"topic\": {'Machine Learning: [Keyword1, keyword2, keyword3], 'Batteries: [keyword1, keyword2, keyword3]}\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"Here is a text: {abstract} Please identify the topics from the following list: {liste}. Note: A single text can belong to multiple topics, so please list all relevant topics.  \\n{format_instructions}\"\n",
    "        ,\n",
    "            input_variables=[\"abstract\",\"liste\",\"topics\"],\n",
    "            partial_variables={\"format_instructions\": new_text}\n",
    "        )\n",
    "\n",
    "\n",
    "        chain = prompt | self.llm | parser\n",
    "        topics = chain.invoke({\"abstract\": abstract, \"liste\": data.keys()})\n",
    "        print('Topics: ', topics['topic'])\n",
    "        return topics['topic']\n",
    "\n",
    "    def decomposition(self, question):\n",
    "        from langchain.prompts import ChatPromptTemplate\n",
    "        from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "        # Decomposition\n",
    "        template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "        The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "        Generate multiple search queries related to: {question} \\n\n",
    "        Output (3 queries):\"\"\"\n",
    "        prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "        generate_queries_decomposition = ( prompt_decomposition | self.llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "        \n",
    "        questions = generate_queries_decomposition.invoke({\"question\":question})\n",
    "        print(questions)\n",
    "        # Prompt\n",
    "        template = \"\"\"Here is the question you need to answer:\n",
    "        \n",
    "        \\n --- \\n {question} \\n --- \\n\n",
    "        \n",
    "        Here is any available background question + answer pairs:\n",
    "        \n",
    "        \\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "        \n",
    "        Here is additional context relevant to the question: \n",
    "        \n",
    "        \\n --- \\n {context} \\n --- \\n\n",
    "        \n",
    "        Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "        \"\"\"\n",
    "        \n",
    "        decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "        from operator import itemgetter\n",
    "        \n",
    "        \n",
    "        def format_qa_pair(question, answer):\n",
    "            \"\"\"Format Q and A pair\"\"\"\n",
    "            \n",
    "            formatted_string = \"\"\n",
    "            formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "            return formatted_string.strip()\n",
    "        \n",
    "        \n",
    "        q_a_pairs = \"\"\n",
    "        for q in questions:\n",
    "            \n",
    "            rag_chain = (\n",
    "            {\"context\": itemgetter(\"question\") | self.vectorstore.as_retriever(), \n",
    "             \"question\": itemgetter(\"question\"),\n",
    "             \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "            | decomposition_prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser())\n",
    "        \n",
    "            answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "            q_a_pair = format_qa_pair(q,answer)\n",
    "            q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "            print(q_a_pairs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def chunk_indexing(self, document:str):\n",
    "        # Check if the document is a valid file path\n",
    "\n",
    "        data = []\n",
    "        if isinstance(document, str) and os.path.isfile(document):\n",
    "            try:\n",
    "                loader = PDFPlumberLoader(document)\n",
    "                data = loader.load()\n",
    "                chunk_size = 2000\n",
    "                chunk_overlap = 20\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error loading PDF: {e}\")\n",
    "        else:\n",
    "            try:\n",
    "                # Treat the document as raw text content\n",
    "                if isinstance(document, str):\n",
    "                    data.append(Document(page_content=document))\n",
    "                elif isinstance(document, list):\n",
    "                    for text in document:\n",
    "                        data.append(Document(page_content=text))\n",
    "                chunk_size = 180\n",
    "                chunk_overlap = 5\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error processing text: {e}\")\n",
    "\n",
    "            \n",
    "        text_splitter=text_splitter = SemanticChunker(\n",
    "    self.oembed, breakpoint_threshold_type=\"standard_deviation\")\n",
    "\n",
    "\n",
    "        all_splits = text_splitter.split_documents(data)\n",
    "        self.vectorstore = Chroma.from_documents(documents=all_splits, embedding=self.oembed, persist_directory=self.folder_path)\n",
    "        \n",
    "    def query_rag(self, question:str) -> None:\n",
    "        if self.vectorstore:\n",
    "            docs = self.vectorstore.similarity_search(question)\n",
    "            from langchain.chains import RetrievalQA\n",
    "            qachain=RetrievalQA.from_chain_type(self.llm, retriever=self.vectorstore.as_retriever(), verbose=True)\n",
    "            res = qachain.invoke({\"query\": question})\n",
    "            print(res['result'])\n",
    "            return res['result']\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"No documents loaded\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf8061-7979-48aa-8429-b6995127aceb",
   "metadata": {},
   "source": [
    "# The Publication Class: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d60eae0-6833-408f-a2e3-07964944b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Publication:\n",
    "    def __init__(self,publication_filled, llm_use:bool=True) -> None:\n",
    "        self.publication_filled = publication_filled\n",
    "        self.title = self.get_publication_title()\n",
    "        self.abstract = self.get_publication_abstract().lower()\n",
    "        self.author = self.get_author_name()\n",
    "        self.year = self.get_year()\n",
    "        self.url = self.get_publication_url()\n",
    "        self.citation = self.get_citation()\n",
    "        self.pdf = None\n",
    "        self.topic = None\n",
    "    \n",
    "      \n",
    "      \n",
    "    '''  \n",
    "    def get_topic(self,output_file=\"json/ouput.json\", # à voir cette histoire avec get_topic et __get_topic\n",
    "                  input_file=\"json/response.json\") -> list[str]:\n",
    "        try:\n",
    "            with open(output_file,'r') as file:\n",
    "                data = json.load(file)\n",
    "            return data[self.author]['topic']\n",
    "        except Exception as e:\n",
    "            return self.__get_topic(input_file)\n",
    "     '''   \n",
    "    def get_publication_url(self) -> str:\n",
    "        return self.publication_filled['pub_url']\n",
    "    \n",
    "    def get_publication_title(self) -> str:\n",
    "        return self.publication_filled['bib']['title'] \n",
    "\n",
    "    def get_publication_abstract(self) -> str:\n",
    "        return self.publication_filled['bib']['abstract']\n",
    "\n",
    "    def get_author_name(self) -> str:\n",
    "        return self.publication_filled['bib']['author']\n",
    "\n",
    "    def get_year(self) -> str:\n",
    "        return self.publication_filled['bib']['pub_year']\n",
    "    \n",
    "    def get_citation(self) -> str:\n",
    "        return self.publication_filled['bib']['citation']\n",
    "    \n",
    "    def get_topic(self,llm,input_file=\"json/response.json\") -> None:\n",
    "        self.topic: dict = llm.get_topic_publication_abstract(abstract=self.abstract,input_file=input_file)\n",
    "        return self.topic\n",
    "    \n",
    "    def get_pdf(self):\n",
    "        query = self.title.replace(\" \", \"+\")\n",
    "        url = f\"https://scholar.google.com/scholar?q={query}\"\n",
    "        print(url)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            html_content = response.text\n",
    "            try:\n",
    "                self.pdf = self.__parse_google_scholar(html_content)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while fetching the PDF link: {e}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        self.available_attributes = {'title': self.title if self.title is not None else 'N/A',\n",
    "                                     'abstract' : self.abstract if self.abstract is not None else 'N/A',\n",
    "                                        'author': self.author if self.author is not None else 'N/A',\n",
    "                                        'year': self.year if self.year is not None else 'N/A',\n",
    "                                        'url': self.url if self.url is not None else 'N/A',\n",
    "                                        'citation': self.citation if self.citation is not None else 'N/A',\n",
    "                                        'pdf': self.pdf if self.pdf is not None else 'N/A',\n",
    "                                        'topic': self.topic if self.topic is not None else 'N/A'}\n",
    "        return str(self.available_attributes)\n",
    "\n",
    "\n",
    "        \n",
    "    def __parse_google_scholar(self,html_content):\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        a_tags = soup.find_all('a')\n",
    "        try:\n",
    "            pdf_link = [a['href'] for a in a_tags if 'href' in a.attrs and '.pdf' in a['href']][0]\n",
    "            print(f\"PDF link found: {pdf_link}\")\n",
    "            return pdf_link\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while parsing the PDF link: {e}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    def download_pdf(self,path):\n",
    "        \n",
    "        # Verifier si le path exist sinon le creer\n",
    "        \n",
    "        import os\n",
    "        import requests\n",
    "        \n",
    "        \n",
    "        path = path + self.title + \".pdf\"\n",
    "        if self.pdf is not None:\n",
    "            try:\n",
    "                response = requests.get(self.pdf)\n",
    "                if response.status_code == 200:\n",
    "                    with open(path, 'wb') as file:\n",
    "                        file.write(response.content)\n",
    "                    print(f\"PDF successfully downloaded and saved to {path}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download the PDF. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"An error occurred while downloading the PDF: {e}\")\n",
    "        else:\n",
    "            from scidownl import scihub_download\n",
    "            #  scidownl by title\n",
    "            try:\n",
    "                scihub_download(self.title, paper_type=\"title\", out=path)\n",
    "            except:\n",
    "                try:\n",
    "                    # By URL\n",
    "                    scihub_download(self.pdf, out=path)\n",
    "                except:\n",
    "                    print(\"Couldn't download the PDF\")\n",
    "           \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "     \n",
    "                    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42468463-5008-440a-a710-bd4a73d66915",
   "metadata": {},
   "source": [
    "# The Author\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48476c3c-3a8c-4503-bbf3-68347e88a9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nauthor = Author('Mehrad Ansari')\\npub = Publication(author.get_last_publication())\\n\\npub.download_pdf('pdfs/')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Author:\n",
    "    def __init__(self, author):\n",
    "        \"\"\"\n",
    "        Initialize an Author object.\n",
    "\n",
    "        Args:\n",
    "            author (str): The name of the author.\n",
    "        \"\"\"\n",
    "        self.author_name = author\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return a string representation of the Author object.\n",
    "\n",
    "        Returns:\n",
    "            str: The name of the author.\n",
    "        \"\"\"\n",
    "        return self.author_name\n",
    "\n",
    "    def get_last_publication(self):\n",
    "        \"\"\"\n",
    "        Get the last publication of the author.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dict containing information about the last publication.\n",
    "        \"\"\"\n",
    "        search_query = scholarly.search_author(self.author_name)\n",
    "        first_author_result = next(search_query)\n",
    "        author = scholarly.fill(first_author_result)\n",
    "        first_publication = sorted(author['publications'], \n",
    "                                   key=lambda x: int(x['bib']['pub_year'])\n",
    "                                   if 'pub_year' in x['bib'] else 0, \n",
    "                                   reverse=True)[0]\n",
    "        first_publication_filled = scholarly.fill(first_publication)\n",
    "        return first_publication_filled\n",
    "\n",
    "    def setup_author(self, output_file, llm):\n",
    "        \"\"\"\n",
    "        Setup the author by adding their last publication to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            output_file (str): The path to the JSON file.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        with open(output_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        author_last_publication = Publication(self.get_last_publication())\n",
    "        \n",
    "\n",
    "        \n",
    "        data[self.author_name] = {\n",
    "            \"title\": author_last_publication.title,\n",
    "            \"abstract\": author_last_publication.abstract,\n",
    "            \"topic\": author_last_publication.get_topic(llm=llm), \n",
    "            \"author\": author_last_publication.author, \n",
    "            \"year\": author_last_publication.year,\n",
    "            \"url\": author_last_publication.url,\n",
    "            \"pdf\": author_last_publication.pdf,\n",
    "        }\n",
    "        \n",
    "        \n",
    "        with open(output_file, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "\n",
    "'''\n",
    "author = Author('Mehrad Ansari')\n",
    "pub = Publication(author.get_last_publication())\n",
    "\n",
    "pub.download_pdf('pdfs/')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc875ae1-baaf-468b-99ed-bd2dd9bde049",
   "metadata": {},
   "source": [
    "## Main module to test function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3679f7-7f70-4168-a95e-5a07f34f01dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nllm = Bot_LLM()\\n\\n\\nauthor = Author(\\'Mehrad Ansari\\')\\n\\nprint(\"Author object created successfully \\n \\n\")\\n\\npub = Publication(author.get_last_publication())\\n\\n\\nprint(\"Publication object created successfully - Having fetched the last publication \\n \\n\")\\n\\nprint(\\'current available attributes: \\n \\n\\')\\nprint(pub)\\n\\nprint(\\'\\n \\nRunning LLM on the abstract of the publication \\n \\n\\')\\npub.get_topic(llm=llm)\\nprint(\\'current available attributes: \\n \\n\\')\\nprint(pub)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the LLM\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "llm = Bot_LLM()\n",
    "\n",
    "\n",
    "author = Author('Mehrad Ansari')\n",
    "\n",
    "print(\"Author object created successfully \\n \\n\")\n",
    "\n",
    "pub = Publication(author.get_last_publication())\n",
    "\n",
    "\n",
    "print(\"Publication object created successfully - Having fetched the last publication \\n \\n\")\n",
    "\n",
    "print('current available attributes: \\n \\n')\n",
    "print(pub)\n",
    "\n",
    "print('\\n \\nRunning LLM on the abstract of the publication \\n \\n')\n",
    "pub.get_topic(llm=llm)\n",
    "print('current available attributes: \\n \\n')\n",
    "print(pub)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208054f8",
   "metadata": {},
   "source": [
    "### Getting PDF - Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf9d213e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\n# Fetching PDF\\n\\ndef check_and_create_directory(path):\\n    \\n    if not os.path.exists(path):\\n        # If it doesn\\'t exist, create the directory\\n        os.makedirs(path)\\n        print(f\"Directory {path} created.\")\\n    else:\\n        print(f\"Directory {path} already exists.\")\\n\\n\\npub.get_pdf()\\nprint(\\'current available attributes: \\n \\n\\')\\nprint(pub)\\n\\n# Downloading PDF\\ncurrent_directory = os.getcwd()\\n\\nprint(f\"The current working directory is: {current_directory}\")\\n\\npath = current_directory + \\'/pdfs/\\'\\n\\ncheck_and_create_directory(path)\\n\\npub.download_pdf(path)\\n\\nprint(pub)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "# Fetching PDF\n",
    "\n",
    "def check_and_create_directory(path):\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        # If it doesn't exist, create the directory\n",
    "        os.makedirs(path)\n",
    "        print(f\"Directory {path} created.\")\n",
    "    else:\n",
    "        print(f\"Directory {path} already exists.\")\n",
    "\n",
    "\n",
    "pub.get_pdf()\n",
    "print('current available attributes: \\n \\n')\n",
    "print(pub)\n",
    "\n",
    "# Downloading PDF\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "path = current_directory + '/pdfs/'\n",
    "\n",
    "check_and_create_directory(path)\n",
    "\n",
    "pub.download_pdf(path)\n",
    "\n",
    "print(pub)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd88614",
   "metadata": {},
   "source": [
    "### Feeding it to the LLM\n",
    "\n",
    "We're Checking first if we got the PDF already; we're feeding it to the LLM. If we don't we give it the abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ecb669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 pdfs/DNA-encoded library-enabled discovery of proximity-inducing small molecules.pdf\n",
      "file already done\n",
      "pdfs/DNA-encoded library-enabled discovery of proximity-inducing small molecules.pdf done\n",
      "1 pdfs/investigating_the_influence_of_treatments_on_carbon_felts_for_vanadium_redox_flow_batteries.pdf\n",
      "file already done\n",
      "pdfs/investigating_the_influence_of_treatments_on_carbon_felts_for_vanadium_redox_flow_batteries.pdf done\n",
      "2 pdfs/degradable_conjugated_polymers.pdf\n",
      "file already done\n",
      "pdfs/degradable_conjugated_polymers.pdf done\n",
      "3 pdfs/efficient_multicarbon_formation_in_acidic_CO2_reduction_via_tandem_electrocatalysis.pdf\n",
      "file already done\n",
      "pdfs/efficient_multicarbon_formation_in_acidic_CO2_reduction_via_tandem_electrocatalysis.pdf done\n",
      "4 pdfs/synergistic_effects_of_thermosensitive_liposomal_doxorubicin_mild_hyperthermia_and_radiotherapy_in_breast_cancer_management_an_orthotopic_mouse_model_study.pdf\n",
      "file already done\n",
      "pdfs/synergistic_effects_of_thermosensitive_liposomal_doxorubicin_mild_hyperthermia_and_radiotherapy_in_breast_cancer_management_an_orthotopic_mouse_model_study.pdf done\n",
      "5 pdfs/Efficient_first_principles_based_modeling_via_machine_learning_from_simple_representations_to_high_entropy_materials.pdf\n",
      "file already done\n",
      "pdfs/Efficient_first_principles_based_modeling_via_machine_learning_from_simple_representations_to_high_entropy_materials.pdf done\n",
      "6 pdfs/closing_the_execution_gap_in_generative_ai_for_chemicals_and_materials_freeways_or_safeguards.pdf\n",
      "file already done\n",
      "pdfs/closing_the_execution_gap_in_generative_ai_for_chemicals_and_materials_freeways_or_safeguards.pdf done\n",
      "7 pdfs/Structure-guided_drug_discovery_back_to_the_future.pdf\n",
      "file already done\n",
      "pdfs/Structure-guided_drug_discovery_back_to_the_future.pdf done\n",
      "8 pdfs/navigating_the_materials_space_with_ml_generated_electronic_fingerprints.pdf\n",
      "file already done\n",
      "pdfs/navigating_the_materials_space_with_ml_generated_electronic_fingerprints.pdf done\n",
      "9 pdfs/Polymeric_Microcapsules_as_Robust_Mimics_of_Emulsion_2_Liquid_Membranes_for_Selective_Ion_Separations.pdf\n",
      "40 pdfs/Polymeric_Microcapsules_as_Robust_Mimics_of_Emulsion_2_Liquid_Membranes_for_Selective_Ion_Separations.pdf\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, I can output the following dictionary of authors' affiliations:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Marc A. Hillmyer\": [\"Department of Chemistry\", \"University of Minnesota\"],\n",
      "    \"Jay R. Werber\": [\"Department of Chemical Engineering & Applied Chemistry\", \"University of Toronto\"]\n",
      "}\n",
      "```\n",
      "\n",
      "Note that there are only two corresponding authors mentioned in the text, Marc A. Hillmyer and Jay R. Werber, with their affiliations listed accordingly.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, the topics of the paper appear to be:\n",
      "\n",
      "1. Climate change and challenges at the water-energy-food nexus.\n",
      "2. Innovations in sustainable, circular economies, particularly related to metals recycling and management.\n",
      "3. Separation technologies for dissolved ions, including liquid/liquid extraction (solvent extraction) and facilitated transport membranes.\n",
      "4. Design criteria for microcapsules as dispersed facilitated transport membranes.\n",
      "\n",
      "Please note that this is not an exhaustive summary of the paper's topics, but rather a concise overview based on the provided context.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "No, the article does not mention a new material discovery. The text discusses existing materials such as Lix 84-I and calixarenes, and their use in liquid-based facilitated transport membranes for ion separations.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know. The provided context does not mention any specific screening algorithms being used in this research paper. It appears to be a study on microcapsules for ion separations and metal uptake, but it does not discuss screening algorithms or population-based identification of individuals with certain conditions.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know the answer to this question because it appears to be about Artificial Intelligence (AI) algorithms, which is not related to the provided text about liquid-based facilitated transport membranes for selective ion separations. The text does not mention AI or any related concepts.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, here is a description of the workflow used by the author:\n",
      "\n",
      "1. The authors packed a column with microcapsules containing 1 wt% Lix 84-I and 0.5 M HSO internal solution.\n",
      "2. The column was fed with a feed solution containing 1 mM CuSO4, 20 mM NaMES, pH 5.6 at a flow rate of 0.1 mL/min (~2 min residence time).\n",
      "3. After the loading step, the column was washed with approximately 3 mL of 20 mM NaMES, pH 5.6.\n",
      "4. The column was then stripped and regenerated using fresh 0.5 M HSO solution, with regeneration times varying between 100 minutes to 1 day.\n",
      "5. The cycle was repeated for a total of 10 cycles.\n",
      "\n",
      "This workflow allows the authors to test the reusability of the column and assess its performance over multiple cycles.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, I can help describe the methods used in this research.\n",
      "\n",
      "The microcapsules were formed using a double-emulsion template from a commercial polystyrene-b-polybutadiene-b-polystyrene triblock polymer. The microcapsules encapsulated a dilute (less than 0.5 M) H2SO4 solution, which was used as the internal strip solution.\n",
      "\n",
      "The column packed with microcapsules was fed with a CuSO4 and NaMES solution at a flow rate of 0.1 mL/min, resulting in a residence time of approximately 2 minutes. The column was then washed with a 20 mM NaMES solution at pH 5.6, followed by stripping/regeneration using fresh 0.5 M H2SO4.\n",
      "\n",
      "The microcapsules were characterized using an Bruker Alpha Platinum ATR spectrometer and an Amscope B120C-E5 microscope equipped with a digital camera.\n",
      "\n",
      "Please let me know if you have any further questions or if there's anything else I can help with!\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "According to the article, the Pitzer model was used to calculate the osmotic pressure of a 0.5 M H2SO4 solution.\n",
      "\n",
      "Additionally, it is mentioned that the Laplace pressure was used to calculate the maximum pressure, Dp , of spherical shells (in the context of resistance to osmotic rupture).\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "No, the article does not explicitly mention funding for the research presented.\n",
      "-----\n",
      "{'affiliation': 'Based on the provided context, I can output the following dictionary of authors\\' affiliations:\\n\\n```\\n{\\n    \"Marc A. Hillmyer\": [\"Department of Chemistry\", \"University of Minnesota\"],\\n    \"Jay R. Werber\": [\"Department of Chemical Engineering & Applied Chemistry\", \"University of Toronto\"]\\n}\\n```\\n\\nNote that there are only two corresponding authors mentioned in the text, Marc A. Hillmyer and Jay R. Werber, with their affiliations listed accordingly.', 'topic': \"Based on the provided context, the topics of the paper appear to be:\\n\\n1. Climate change and challenges at the water-energy-food nexus.\\n2. Innovations in sustainable, circular economies, particularly related to metals recycling and management.\\n3. Separation technologies for dissolved ions, including liquid/liquid extraction (solvent extraction) and facilitated transport membranes.\\n4. Design criteria for microcapsules as dispersed facilitated transport membranes.\\n\\nPlease note that this is not an exhaustive summary of the paper's topics, but rather a concise overview based on the provided context.\", 'new materials': 'No, the article does not mention a new material discovery. The text discusses existing materials such as Lix 84-I and calixarenes, and their use in liquid-based facilitated transport membranes for ion separations.', 'screening algorithms': \"I don't know. The provided context does not mention any specific screening algorithms being used in this research paper. It appears to be a study on microcapsules for ion separations and metal uptake, but it does not discuss screening algorithms or population-based identification of individuals with certain conditions.\", 'AI algorithms': \"I don't know the answer to this question because it appears to be about Artificial Intelligence (AI) algorithms, which is not related to the provided text about liquid-based facilitated transport membranes for selective ion separations. The text does not mention AI or any related concepts.\", 'workflow': 'Based on the provided context, here is a description of the workflow used by the author:\\n\\n1. The authors packed a column with microcapsules containing 1 wt% Lix 84-I and 0.5 M HSO internal solution.\\n2. The column was fed with a feed solution containing 1 mM CuSO4, 20 mM NaMES, pH 5.6 at a flow rate of 0.1 mL/min (~2 min residence time).\\n3. After the loading step, the column was washed with approximately 3 mL of 20 mM NaMES, pH 5.6.\\n4. The column was then stripped and regenerated using fresh 0.5 M HSO solution, with regeneration times varying between 100 minutes to 1 day.\\n5. The cycle was repeated for a total of 10 cycles.\\n\\nThis workflow allows the authors to test the reusability of the column and assess its performance over multiple cycles.', 'methods': \"Based on the provided context, I can help describe the methods used in this research.\\n\\nThe microcapsules were formed using a double-emulsion template from a commercial polystyrene-b-polybutadiene-b-polystyrene triblock polymer. The microcapsules encapsulated a dilute (less than 0.5 M) H2SO4 solution, which was used as the internal strip solution.\\n\\nThe column packed with microcapsules was fed with a CuSO4 and NaMES solution at a flow rate of 0.1 mL/min, resulting in a residence time of approximately 2 minutes. The column was then washed with a 20 mM NaMES solution at pH 5.6, followed by stripping/regeneration using fresh 0.5 M H2SO4.\\n\\nThe microcapsules were characterized using an Bruker Alpha Platinum ATR spectrometer and an Amscope B120C-E5 microscope equipped with a digital camera.\\n\\nPlease let me know if you have any further questions or if there's anything else I can help with!\", 'models': 'According to the article, the Pitzer model was used to calculate the osmotic pressure of a 0.5 M H2SO4 solution.\\n\\nAdditionally, it is mentioned that the Laplace pressure was used to calculate the maximum pressure, Dp , of spherical shells (in the context of resistance to osmotic rupture).', 'funding': 'No, the article does not explicitly mention funding for the research presented.'}\n",
      "pdfs/Polymeric_Microcapsules_as_Robust_Mimics_of_Emulsion_2_Liquid_Membranes_for_Selective_Ion_Separations.pdf done\n",
      "10 pdfs/History-agnostic battery degradation inference.pdf\n",
      "file already done\n",
      "pdfs/History-agnostic battery degradation inference.pdf done\n",
      "11 pdfs/Enhanced_sampling_of_robust_molecular_datasets_with_uncertainty-based_collective_variables.pdf\n",
      "file already done\n",
      "pdfs/Enhanced_sampling_of_robust_molecular_datasets_with_uncertainty-based_collective_variables.pdf done\n",
      "12 pdfs/holistic_platform_for_accelerating_sorbent-based_carbon_capture.pdf\n",
      "file already done\n",
      "pdfs/holistic_platform_for_accelerating_sorbent-based_carbon_capture.pdf done\n",
      "13 pdfs/A_data-driven_framework_to_improve_the_wear_resistance_of_a_low-alloy_steel_fabricated_by_laser_powder_bed_fusion.pdf\n",
      "file already done\n",
      "pdfs/A_data-driven_framework_to_improve_the_wear_resistance_of_a_low-alloy_steel_fabricated_by_laser_powder_bed_fusion.pdf done\n",
      "14 pdfs/Data-science-based_reconstruction_of_3-D_membrane_pore_structure_using_a_single_2-D_micrograph.pdf\n",
      "41 pdfs/Data-science-based_reconstruction_of_3-D_membrane_pore_structure_using_a_single_2-D_micrograph.pdf\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know the answer to that question, as the provided context does not mention the authors' affiliations.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, the topics of the paper appear to be:\n",
      "\n",
      "1. Reconstruction of 3-D structure of a membrane using a single 2-D micrograph.\n",
      "2. Development of a novel approach for characterizing the internal pore structure of membranes.\n",
      "3. Comparison between original and reconstructed structures in terms of properties and performance.\n",
      "4. Application of focused ion beam (FIB) scanning electron microscopy (SEM) technique for 3-D membrane reconstruction.\n",
      "\n",
      "Please note that this is based on my understanding of the provided context, and I may have missed some important topics or not fully grasped the scope of the paper.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "No, the article does not mention any new material discovery. It discusses a novel approach to reconstructing the 3D structure of polymeric membranes using only a single 2D cross-sectional micrograph, and presents results comparing the properties of original and reconstructed structures.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, it seems that the topic of the paper is related to Separation and Purification Technology, which involves the use of FIB-SEM tomography and reconstruction for soft, porous, and poorly conducting materials. The main focus appears to be on image processing and feature extraction from 2-D images.\n",
      "\n",
      "As for your question about screening algorithms used in the paper, I couldn't find any specific information or references to screening algorithms in the provided context. The text primarily discusses feature extraction from 2-D images and image processing techniques, but it doesn't explicitly mention screening algorithms. If you're looking for information on screening algorithms in general, I can try to provide some helpful resources or explanations!\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, it does not appear that this article discusses AI algorithms or their applications in solving specific tasks. The text primarily focuses on the development of an algorithm for image segmentation and 3D reconstruction of a microporous membrane using FIB-SEM data.\n",
      "\n",
      "However, I can provide some general information about AI algorithms:\n",
      "\n",
      "AI algorithms are computer programs designed to simulate human intelligence by analyzing data, making decisions, and performing tasks that typically require human cognition. These algorithms can learn from data, improve over time through machine learning, and perform complex tasks such as image recognition, natural language processing, and decision-making.\n",
      "\n",
      "Some common AI algorithms include:\n",
      "\n",
      "1. Deep Learning (DL) algorithms: used for image and speech recognition, natural language processing, and other applications that require pattern recognition.\n",
      "2. Rule-Based Systems: use pre-defined rules to make decisions based on input data.\n",
      "3. Machine Learning (ML) algorithms: learn from data and improve over time through iterative processing of new information.\n",
      "4. Evolutionary Computation: uses principles of evolution, such as natural selection and genetic drift, to optimize solutions.\n",
      "\n",
      "If you have any specific questions about AI algorithms or their applications, I'd be happy to help!\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, I can summarize the workflow as follows:\n",
      "\n",
      "1. Calculate a normalized histogram of the distance map for both solid and void phases (Fig. 1).\n",
      "2. Combine the two histograms to generate a combined distance function (CDF) (Fig. 1).\n",
      "\n",
      "For generating 3-D structures:\n",
      "\n",
      "1. Generate a 3-D matrix with random values between 0 and 1, representing the membrane.\n",
      "2. Filter the 3-D matrix using a Gaussian smoothing kernel to introduce spatial correlation (Fig. 2).\n",
      "3. Generate another matrix with larger pore sizes towards the bottom of the membrane.\n",
      "4. Combine the two matrices based on their distance from the top of the structure.\n",
      "\n",
      "5. Threshold the combined matrix to differentiate between void and solid phases.\n",
      "6. Perform morphological closing using the imclose function in MATLAB to smooth sharp corners and make pores more circular.\n",
      "\n",
      "Please note that this summary is based on the provided context, and it might not be exhaustive or entirely accurate if there are missing details.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, I can attempt to provide a methods description.\n",
      "\n",
      "The method involves feature extraction from 2-D images using distance maps. Here's a breakdown of the steps:\n",
      "\n",
      "1. Image segmentation: The algorithm starts by selecting a random slice and performing image segmentation, where each pixel is assigned a label based on its visual characteristics.\n",
      "2. Distance map calculation: After image segmentation, 0s are assigned to void (pore) spaces and 1s are assigned to solid spaces. Then, the Euclidean distance between each pixel and the nearest non-zero pixel is calculated.\n",
      "\n",
      "The feature extraction method used in this study is a novel distance map that captures the statistical distribution of pore and solid through a membrane. This allows for the calculation of tortuosity of the membrane using a single 2-D micrograph with high accuracy and low standard deviation.\n",
      "\n",
      "Please note that I've tried to summarize the methods based on the provided context, but if there's any information missing or unclear, please let me know!\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "According to the text, the following models were used:\n",
      "\n",
      "1. Reconstruction algorithm ( Bayesian optimization)\n",
      "2. Pore network model\n",
      "\n",
      "These models were used to analyze and quantify the properties of porous materials and their membrane properties.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know. The provided text doesn't mention funding or any information about that.\n",
      "-----\n",
      "{'affiliation': \"I don't know the answer to that question, as the provided context does not mention the authors' affiliations.\", 'topic': 'Based on the provided context, the topics of the paper appear to be:\\n\\n1. Reconstruction of 3-D structure of a membrane using a single 2-D micrograph.\\n2. Development of a novel approach for characterizing the internal pore structure of membranes.\\n3. Comparison between original and reconstructed structures in terms of properties and performance.\\n4. Application of focused ion beam (FIB) scanning electron microscopy (SEM) technique for 3-D membrane reconstruction.\\n\\nPlease note that this is based on my understanding of the provided context, and I may have missed some important topics or not fully grasped the scope of the paper.', 'new materials': 'No, the article does not mention any new material discovery. It discusses a novel approach to reconstructing the 3D structure of polymeric membranes using only a single 2D cross-sectional micrograph, and presents results comparing the properties of original and reconstructed structures.', 'screening algorithms': \"Based on the provided context, it seems that the topic of the paper is related to Separation and Purification Technology, which involves the use of FIB-SEM tomography and reconstruction for soft, porous, and poorly conducting materials. The main focus appears to be on image processing and feature extraction from 2-D images.\\n\\nAs for your question about screening algorithms used in the paper, I couldn't find any specific information or references to screening algorithms in the provided context. The text primarily discusses feature extraction from 2-D images and image processing techniques, but it doesn't explicitly mention screening algorithms. If you're looking for information on screening algorithms in general, I can try to provide some helpful resources or explanations!\", 'AI algorithms': \"Based on the provided context, it does not appear that this article discusses AI algorithms or their applications in solving specific tasks. The text primarily focuses on the development of an algorithm for image segmentation and 3D reconstruction of a microporous membrane using FIB-SEM data.\\n\\nHowever, I can provide some general information about AI algorithms:\\n\\nAI algorithms are computer programs designed to simulate human intelligence by analyzing data, making decisions, and performing tasks that typically require human cognition. These algorithms can learn from data, improve over time through machine learning, and perform complex tasks such as image recognition, natural language processing, and decision-making.\\n\\nSome common AI algorithms include:\\n\\n1. Deep Learning (DL) algorithms: used for image and speech recognition, natural language processing, and other applications that require pattern recognition.\\n2. Rule-Based Systems: use pre-defined rules to make decisions based on input data.\\n3. Machine Learning (ML) algorithms: learn from data and improve over time through iterative processing of new information.\\n4. Evolutionary Computation: uses principles of evolution, such as natural selection and genetic drift, to optimize solutions.\\n\\nIf you have any specific questions about AI algorithms or their applications, I'd be happy to help!\", 'workflow': 'Based on the provided context, I can summarize the workflow as follows:\\n\\n1. Calculate a normalized histogram of the distance map for both solid and void phases (Fig. 1).\\n2. Combine the two histograms to generate a combined distance function (CDF) (Fig. 1).\\n\\nFor generating 3-D structures:\\n\\n1. Generate a 3-D matrix with random values between 0 and 1, representing the membrane.\\n2. Filter the 3-D matrix using a Gaussian smoothing kernel to introduce spatial correlation (Fig. 2).\\n3. Generate another matrix with larger pore sizes towards the bottom of the membrane.\\n4. Combine the two matrices based on their distance from the top of the structure.\\n\\n5. Threshold the combined matrix to differentiate between void and solid phases.\\n6. Perform morphological closing using the imclose function in MATLAB to smooth sharp corners and make pores more circular.\\n\\nPlease note that this summary is based on the provided context, and it might not be exhaustive or entirely accurate if there are missing details.', 'methods': \"Based on the provided context, I can attempt to provide a methods description.\\n\\nThe method involves feature extraction from 2-D images using distance maps. Here's a breakdown of the steps:\\n\\n1. Image segmentation: The algorithm starts by selecting a random slice and performing image segmentation, where each pixel is assigned a label based on its visual characteristics.\\n2. Distance map calculation: After image segmentation, 0s are assigned to void (pore) spaces and 1s are assigned to solid spaces. Then, the Euclidean distance between each pixel and the nearest non-zero pixel is calculated.\\n\\nThe feature extraction method used in this study is a novel distance map that captures the statistical distribution of pore and solid through a membrane. This allows for the calculation of tortuosity of the membrane using a single 2-D micrograph with high accuracy and low standard deviation.\\n\\nPlease note that I've tried to summarize the methods based on the provided context, but if there's any information missing or unclear, please let me know!\", 'models': 'According to the text, the following models were used:\\n\\n1. Reconstruction algorithm ( Bayesian optimization)\\n2. Pore network model\\n\\nThese models were used to analyze and quantify the properties of porous materials and their membrane properties.', 'funding': \"I don't know. The provided text doesn't mention funding or any information about that.\"}\n",
      "pdfs/Data-science-based_reconstruction_of_3-D_membrane_pore_structure_using_a_single_2-D_micrograph.pdf done\n",
      "15 pdfs/photoresponsive_organic_cages_computationally_driven_discovery_of_azobenzene_derived_organic_cages.pdf\n",
      "file already done\n",
      "pdfs/photoresponsive_organic_cages_computationally_driven_discovery_of_azobenzene_derived_organic_cages.pdf done\n",
      "16 pdfs/Assessment_of_chemistry_knowledge_in_large_language_models_that_generate_code.pdf\n",
      "file already done\n",
      "pdfs/Assessment_of_chemistry_knowledge_in_large_language_models_that_generate_code.pdf done\n",
      "17 pdfs/cell_penetration_of_oxadiazole_containing_macrocycles.pdf\n",
      "file already done\n",
      "pdfs/cell_penetration_of_oxadiazole_containing_macrocycles.pdf done\n",
      "18 pdfs/Exploiting_redundancy_in_large_materials_datasets_for_efficient_machine_learning_with_less_data.pdf\n",
      "file already done\n",
      "pdfs/Exploiting_redundancy_in_large_materials_datasets_for_efficient_machine_learning_with_less_data.pdf done\n",
      "19 pdfs/AGENT-BASED_LEARNING_OF_MATERIALS_DATASETS_FROM_SCIENTIFIC_LITERATURE.pdf\n",
      "42 pdfs/AGENT-BASED_LEARNING_OF_MATERIALS_DATASETS_FROM_SCIENTIFIC_LITERATURE.pdf\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I can help you with that!\n",
      "\n",
      "According to the text, the authors' affiliations are:\n",
      "\n",
      "* Mehrad Ansari:\n",
      "\t+ 1. Acceleration Consortium, University of Toronto, Toronto, Ontario M5S3E5, Canada\n",
      "\t+ 2. Department of Chemical Engineering & Applied Chemistry, University of Toronto, Toronto, Ontario M5S3E5, Canada\n",
      "\n",
      "* Seyed Mohamad Moosavi:\n",
      "\t+ 1. Acceleration Consortium, University of Toronto, Toronto, Ontario M5S3E5, Canada\n",
      "\t+ 2. Department of Chemical Engineering & Applied Chemistry, University of Toronto, Toronto, Ontario M5S3E5, Canada\n",
      "\n",
      "Here's the output in a dictionary format:\n",
      "\n",
      "```\n",
      "{\n",
      "\"Mehrad Ansari\": {\n",
      "\"Affiliation 1\": \"Acceleration Consortium, University of Toronto, Toronto, Ontario M5S3E5, Canada\",\n",
      "\"Affiliation 2\": \"Department of Chemical Engineering & Applied Chemistry, University of Toronto, Toronto, Ontario M5S3E5, Canada\"\n",
      "},\n",
      "\"Seyed Mohamad Moosavi\": {\n",
      "\"Affiliation 1\": \"Acceleration Consortium, University of Toronto, Toronto, Ontario M5S3E5, Canada\",\n",
      "\"Affiliation 2\": \"Department of Chemical Engineering & Applied Chemistry, University of Toronto, Toronto, Ontario M5S3E5, Canada\"\n",
      "}\n",
      "```\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, the topics of the paper appear to be:\n",
      "\n",
      "* Agent-based learning\n",
      "* Materials datasets from scientific literature\n",
      "* Named Entity Recognition and Relation Extraction\n",
      "* Chemical Engineering\n",
      "* Natural Language Processing (NLP)\n",
      "* Machine Learning\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know. The provided context doesn't mention anything about new material discovery. It appears to be discussing natural language processing and machine learning applications in the field of chemistry, specifically in the area of named entity recognition and relation extraction.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "There is no specific mention of a screening algorithm in this paper. The paper primarily focuses on developing an AI agent-based method for extracting domain-specific information from complex, unstructured text and evaluating its performance using various metrics such as precision, recall, F1 score, and binary accuracy.\n",
      "\n",
      "However, the \"DocSearch\" tool described in Section 5.1.1 of the paper could be considered a type of screening algorithm, as it aims to quickly identify relevant knowledge materials properties from text documents (papers) that are related to a given input query. The tool uses OpenAI's text-ada-002 embeddings and Maximum Marginal Relevance (MMR) based on cosine similarity to rank the retrieved documents.\n",
      "\n",
      "Please note that this interpretation is not explicitly mentioned in the paper, but it can be inferred from the context of the \"DocSearch\" tool description.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, here's an answer to the question:\n",
      "\n",
      "AI agents with appropriate tools and instructions are capable of being zero-shot learners, which is different from large language models (LLMs) that are few-shot learners. This allows for a great opportunity to boost the performance of standalone LLMs across various domain-specific tasks without having to go through labor-intensive fine-tuning processes.\n",
      "\n",
      "In other words, AI agents with the right tools and instructions can learn and perform tasks without being trained on specific examples or datasets (zero-shot learning), whereas large language models typically require some training data or examples to perform well.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, here is a description of the workflow used by the authors:\n",
      "\n",
      "The AI agent, Eunomia, is tasked with autonomously extracting information from scientific literature. The workflow involves:\n",
      "\n",
      "1. Online dataset search and document search using various tools.\n",
      "2. Identifying all MOFs (metal-organic frameworks) from a given research article.\n",
      "3. Predicting the property of each MOF (e.g., water stability) by providing reasoning for its decision.\n",
      "4. Ensuring the actual logical connection to the water stability property and reducing the likelihood of hallucinations using the chain-of-verification tool.\n",
      "5. Outputting a customized dataset that can be used to develop supervised or unsupervised machine learning methods.\n",
      "\n",
      "Note that this workflow is specific to the authors' approach and may not represent the general workflow for agent-based learning in materials datasets from scientific literature.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I can help with that!\n",
      "\n",
      "According to the provided context, the method used by Eunomia for extracting material science-relevant information from unstructured text and converting it into structured databases is an autonomous AI agent augmented with chemistry-informed tools.\n",
      "\n",
      "The approach involves:\n",
      "\n",
      "1. Using a pre-trained Large Language Model (LLM) such as GPT-4.\n",
      "2. Tapping into a wealth of knowledge from academic publications, domain-specific experts, the Internet, and other user-defined resources.\n",
      "3. Accommodating various information extraction tasks through natural language text prompts for new output schemas.\n",
      "4. Reducing the risk of hallucination through a chain-of-verification process.\n",
      "\n",
      "This method streamlines data extraction, achieving remarkable accuracy and performance solely with a pre-trained LLM, eliminating the need for fine-tuning. It also offers adaptability, simplifies the development of tailored datasets from literature for domain experts, and reduces the need for extensive programming, NLP, or machine learning expertise.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, the models mentioned in the article are:\n",
      "\n",
      "1. GPT-4\n",
      "2. LLM-NERRE\n",
      "3. Eunomia\n",
      "4. CoV (likely referring to a chemistry tool or dataset)\n",
      "5. PaperQA (a retrieval-augmented generative agent for scientific research)\n",
      "\n",
      "These models are used in various studies and experiments discussed in the article, including tasks such as relation extraction, reasoning, and question answering.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know. The article does not explicitly mention funding information.\n",
      "-----\n",
      "{'affiliation': 'I can help you with that!\\n\\nAccording to the text, the authors\\' affiliations are:\\n\\n* Mehrad Ansari:\\n\\t+ 1. Acceleration Consortium, University of Toronto, Toronto, Ontario M5S3E5, Canada\\n\\t+ 2. Department of Chemical Engineering & Applied Chemistry, University of Toronto, Toronto, Ontario M5S3E5, Canada\\n\\n* Seyed Mohamad Moosavi:\\n\\t+ 1. Acceleration Consortium, University of Toronto, Toronto, Ontario M5S3E5, Canada\\n\\t+ 2. Department of Chemical Engineering & Applied Chemistry, University of Toronto, Toronto, Ontario M5S3E5, Canada\\n\\nHere\\'s the output in a dictionary format:\\n\\n```\\n{\\n\"Mehrad Ansari\": {\\n\"Affiliation 1\": \"Acceleration Consortium, University of Toronto, Toronto, Ontario M5S3E5, Canada\",\\n\"Affiliation 2\": \"Department of Chemical Engineering & Applied Chemistry, University of Toronto, Toronto, Ontario M5S3E5, Canada\"\\n},\\n\"Seyed Mohamad Moosavi\": {\\n\"Affiliation 1\": \"Acceleration Consortium, University of Toronto, Toronto, Ontario M5S3E5, Canada\",\\n\"Affiliation 2\": \"Department of Chemical Engineering & Applied Chemistry, University of Toronto, Toronto, Ontario M5S3E5, Canada\"\\n}\\n```', 'topic': 'Based on the provided context, the topics of the paper appear to be:\\n\\n* Agent-based learning\\n* Materials datasets from scientific literature\\n* Named Entity Recognition and Relation Extraction\\n* Chemical Engineering\\n* Natural Language Processing (NLP)\\n* Machine Learning', 'new materials': \"I don't know. The provided context doesn't mention anything about new material discovery. It appears to be discussing natural language processing and machine learning applications in the field of chemistry, specifically in the area of named entity recognition and relation extraction.\", 'screening algorithms': 'There is no specific mention of a screening algorithm in this paper. The paper primarily focuses on developing an AI agent-based method for extracting domain-specific information from complex, unstructured text and evaluating its performance using various metrics such as precision, recall, F1 score, and binary accuracy.\\n\\nHowever, the \"DocSearch\" tool described in Section 5.1.1 of the paper could be considered a type of screening algorithm, as it aims to quickly identify relevant knowledge materials properties from text documents (papers) that are related to a given input query. The tool uses OpenAI\\'s text-ada-002 embeddings and Maximum Marginal Relevance (MMR) based on cosine similarity to rank the retrieved documents.\\n\\nPlease note that this interpretation is not explicitly mentioned in the paper, but it can be inferred from the context of the \"DocSearch\" tool description.', 'AI algorithms': \"Based on the provided context, here's an answer to the question:\\n\\nAI agents with appropriate tools and instructions are capable of being zero-shot learners, which is different from large language models (LLMs) that are few-shot learners. This allows for a great opportunity to boost the performance of standalone LLMs across various domain-specific tasks without having to go through labor-intensive fine-tuning processes.\\n\\nIn other words, AI agents with the right tools and instructions can learn and perform tasks without being trained on specific examples or datasets (zero-shot learning), whereas large language models typically require some training data or examples to perform well.\", 'workflow': \"Based on the provided context, here is a description of the workflow used by the authors:\\n\\nThe AI agent, Eunomia, is tasked with autonomously extracting information from scientific literature. The workflow involves:\\n\\n1. Online dataset search and document search using various tools.\\n2. Identifying all MOFs (metal-organic frameworks) from a given research article.\\n3. Predicting the property of each MOF (e.g., water stability) by providing reasoning for its decision.\\n4. Ensuring the actual logical connection to the water stability property and reducing the likelihood of hallucinations using the chain-of-verification tool.\\n5. Outputting a customized dataset that can be used to develop supervised or unsupervised machine learning methods.\\n\\nNote that this workflow is specific to the authors' approach and may not represent the general workflow for agent-based learning in materials datasets from scientific literature.\", 'methods': 'I can help with that!\\n\\nAccording to the provided context, the method used by Eunomia for extracting material science-relevant information from unstructured text and converting it into structured databases is an autonomous AI agent augmented with chemistry-informed tools.\\n\\nThe approach involves:\\n\\n1. Using a pre-trained Large Language Model (LLM) such as GPT-4.\\n2. Tapping into a wealth of knowledge from academic publications, domain-specific experts, the Internet, and other user-defined resources.\\n3. Accommodating various information extraction tasks through natural language text prompts for new output schemas.\\n4. Reducing the risk of hallucination through a chain-of-verification process.\\n\\nThis method streamlines data extraction, achieving remarkable accuracy and performance solely with a pre-trained LLM, eliminating the need for fine-tuning. It also offers adaptability, simplifies the development of tailored datasets from literature for domain experts, and reduces the need for extensive programming, NLP, or machine learning expertise.', 'models': 'Based on the provided context, the models mentioned in the article are:\\n\\n1. GPT-4\\n2. LLM-NERRE\\n3. Eunomia\\n4. CoV (likely referring to a chemistry tool or dataset)\\n5. PaperQA (a retrieval-augmented generative agent for scientific research)\\n\\nThese models are used in various studies and experiments discussed in the article, including tasks such as relation extraction, reasoning, and question answering.', 'funding': \"I don't know. The article does not explicitly mention funding information.\"}\n",
      "pdfs/AGENT-BASED_LEARNING_OF_MATERIALS_DATASETS_FROM_SCIENTIFIC_LITERATURE.pdf done\n",
      "20 pdfs/delocalized_asynchronous_closed_loop_discovery_of_organic_laser_emitters.pdf\n",
      "file already done\n",
      "pdfs/delocalized_asynchronous_closed_loop_discovery_of_organic_laser_emitters.pdf done\n",
      "21 pdfs/accelerated_chemical_science_with_ai.pdf\n",
      "file already done\n",
      "pdfs/accelerated_chemical_science_with_ai.pdf done\n",
      "22 pdfs/Probing_out-of-distribution_generalization_in_machine_learning_for_materials.pdf\n",
      "file already done\n",
      "pdfs/Probing_out-of-distribution_generalization_in_machine_learning_for_materials.pdf done\n",
      "23 pdfs/towards_comprehensive_coverage_of_chemical_space_quantum_mechanical_properties_of_836k_constitutional_and_conformational_closed_shell_neutral_isomers_consisting_of_hcnofsipsclbr.pdf\n",
      "file already done\n",
      "pdfs/towards_comprehensive_coverage_of_chemical_space_quantum_mechanical_properties_of_836k_constitutional_and_conformational_closed_shell_neutral_isomers_consisting_of_hcnofsipsclbr.pdf done\n",
      "24 pdfs/JARVIS-Leaderboard_a_large_scale_benchmark_of_materials_design_methods.pdf\n",
      "file already done\n",
      "pdfs/JARVIS-Leaderboard_a_large_scale_benchmark_of_materials_design_methods.pdf done\n",
      "25 pdfs/Big_data_and_benchmarking_initiatives_to_bridge_the_gap_from_AlphaFold_to_drug_design.pdf\n",
      "file already done\n",
      "pdfs/Big_data_and_benchmarking_initiatives_to_bridge_the_gap_from_AlphaFold_to_drug_design.pdf done\n",
      "26 pdfs/evaluating_the_feasibility_of_medium_chain_oleochemical_synthesis_using_microbial_chain_elongation.pdf\n",
      "file already done\n",
      "pdfs/evaluating_the_feasibility_of_medium_chain_oleochemical_synthesis_using_microbial_chain_elongation.pdf done\n",
      "27 pdfs/term_sheets_in_venture_capital.pdf\n",
      "file already done\n",
      "pdfs/term_sheets_in_venture_capital.pdf done\n",
      "28 pdfs/Learning_peptide_properties_with_positive_examples_only.pdf\n",
      "43 pdfs/Learning_peptide_properties_with_positive_examples_only.pdf\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, the author's affiliation is:\n",
      "\n",
      "`Department of Chemical Engineering, University of Rochester, Rochester, NY, 14627, USA. E-mail: andrew.white@rochester.edu`\n",
      "\n",
      "Here is the output in dictionary format:\n",
      "\n",
      "```\n",
      "{'Affiliation': 'Department of Chemical Engineering, University of Rochester, Rochester, NY, 14627, USA.',\n",
      " 'Email': 'andrew.white@rochester.edu'}\n",
      "```\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The topic of this paper is peptide sequence-function relationship prediction, specifically using a semi-supervised learning framework to predict properties such as hemolysis, non-specific interactions, and solubility based on peptide sequences. The authors demonstrate their approach on several datasets and evaluate its performance using classical classiﬁers.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, it appears that this article is related to Positive-Unlabeled (PU) learning strategies in machine learning. The topic seems to be focused on developing a probabilistic binary classifier for predicting the likelihood of a protein being active or inactive based on its amino acid sequence.\n",
      "\n",
      "The article discusses two PU learning strategies: Adapting Base Classifier and Reliable Negative Identification. It also mentions the use of an RNN (Recurrent Neural Network) as the base classifier, which is trained with positive and unlabeled examples.\n",
      "\n",
      "However, there is no explicit mention of any new material discovery in this article. The focus seems to be on developing a machine learning model for protein classification rather than discovering new materials or compounds.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, the following screening algorithms are mentioned:\n",
      "\n",
      "1. Support vector machines (SVM)\n",
      "2. Random forest\n",
      "3. Logistic regression\n",
      "4. Naive Bayesian classifier\n",
      "5. Sequence-based learning strategies\n",
      "\n",
      "These algorithms are used for classification purposes in bioinformatics and biomaterials design, such as predicting peptide properties or identifying protein–protein interactions.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I can help you with that! The provided context is an article about digital discovery and positive-unlabeled learning (PU) in the field of chemistry. It appears to discuss the application of AI algorithms for solving specific tasks, such as extracting reliable negatives from unlabeled data.\n",
      "\n",
      "To answer your question directly: Yes, AI algorithms are computational methods and processes used to solve specific tasks by mimicking human intelligence.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, here's a summary of the workflow used by the author:\n",
      "\n",
      "1. **Data Preparation**: The authors compile their Keras model using Adam optimizer and binary cross-entropy loss function.\n",
      "2. **Positive-Unlabeled (PU) Learning**: They focus on two PU learning strategies:\n",
      "\t* Adapting Base Classifier: This approach adjusts the probability of being positive based on a Bayesian inference technique.\n",
      "\t* Spy-based Method: They use a small randomly selected group of positive examples (S) as \"spies\" to find reliable negatives. These spies are added to the unlabeled data (U) and used to train a new classifier.\n",
      "3. **Spy-based Approach**:\n",
      "\t* Remove a small portion of positive samples (S) from the training data and add them to the unlabeled set (U).\n",
      "\t* Calculate the spy-rate, which determines what percentage of spies can remain in the unlabeled set.\n",
      "\t* Determine the decision boundary threshold (t) based on the spy-tolerance value.\n",
      "\t* Classify all added spies during training as positives if their posterior likelihood is greater than or equal to t. Samples in U with a posterior likelihood smaller than t are considered reliable negatives (RN).\n",
      "4. **Training**: The authors train a new classifier f given original positive samples (P) and the found RN.\n",
      "\n",
      "Please note that this summary focuses on the PU learning and spy-based approach, which is a specific part of the workflow described in the provided context.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, here is a possible methods description:\n",
      "\n",
      "**Methods**\n",
      "\n",
      "In this study, we employed a semi-supervised learning approach called Positive Unlabeled (PU) learning to predict peptide properties from their sequences. We solely used positive examples in our training data and adapted a base classifier using layer normalization and dropout regularization to improve the learning of bidirectional dependencies between distant residues within a sequence.\n",
      "\n",
      "The fully connected layers were downsized in three consecutive steps via layer normalization and dropout regularization. The final layer outputted the probability of being active for the desired training task using a sigmoid activation function. This approach allowed us to overcome the limitation of having only positive examples available, which is common in many biological applications.\n",
      "\n",
      "No negative examples were used in our training process, as they are often difficult to obtain and may not accurately represent the true negative cases. Instead, we relied on the PU learning strategy to identify reliable patterns in the positive data that can be used to make accurate predictions about peptide properties.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "According to the provided context, the following models were used in the article:\n",
      "\n",
      "1. **Adapting Base Classier**: This is a model that adapts its base classifier using unlabeled data.\n",
      "2. **Reliable Negative Identication PU method**: This is a Positive Unlabeled (PU) learning method that identifies reliable negative examples and uses them to improve the performance of the base classifier.\n",
      "3. **PN classier**: This is a classical Positive-Negative (PN) learning model that uses both positive and negative labeled data for training.\n",
      "\n",
      "These models were compared in terms of their accuracy (ACC) and area under the receiver operating characteristic curve (AUROC).\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The article does not explicitly mention any funding information. It appears to be a research paper published in a journal, and as such, it may have been supported by various sources, including grants, institutions, or authors' own funding. However, without further information, I cannot determine the specific funding sources for this study.\n",
      "-----\n",
      "{'affiliation': \"Based on the provided context, the author's affiliation is:\\n\\n`Department of Chemical Engineering, University of Rochester, Rochester, NY, 14627, USA. E-mail: andrew.white@rochester.edu`\\n\\nHere is the output in dictionary format:\\n\\n```\\n{'Affiliation': 'Department of Chemical Engineering, University of Rochester, Rochester, NY, 14627, USA.',\\n 'Email': 'andrew.white@rochester.edu'}\\n```\", 'topic': 'The topic of this paper is peptide sequence-function relationship prediction, specifically using a semi-supervised learning framework to predict properties such as hemolysis, non-specific interactions, and solubility based on peptide sequences. The authors demonstrate their approach on several datasets and evaluate its performance using classical classiﬁers.', 'new materials': 'Based on the provided context, it appears that this article is related to Positive-Unlabeled (PU) learning strategies in machine learning. The topic seems to be focused on developing a probabilistic binary classifier for predicting the likelihood of a protein being active or inactive based on its amino acid sequence.\\n\\nThe article discusses two PU learning strategies: Adapting Base Classifier and Reliable Negative Identification. It also mentions the use of an RNN (Recurrent Neural Network) as the base classifier, which is trained with positive and unlabeled examples.\\n\\nHowever, there is no explicit mention of any new material discovery in this article. The focus seems to be on developing a machine learning model for protein classification rather than discovering new materials or compounds.', 'screening algorithms': 'Based on the provided context, the following screening algorithms are mentioned:\\n\\n1. Support vector machines (SVM)\\n2. Random forest\\n3. Logistic regression\\n4. Naive Bayesian classifier\\n5. Sequence-based learning strategies\\n\\nThese algorithms are used for classification purposes in bioinformatics and biomaterials design, such as predicting peptide properties or identifying protein–protein interactions.', 'AI algorithms': 'I can help you with that! The provided context is an article about digital discovery and positive-unlabeled learning (PU) in the field of chemistry. It appears to discuss the application of AI algorithms for solving specific tasks, such as extracting reliable negatives from unlabeled data.\\n\\nTo answer your question directly: Yes, AI algorithms are computational methods and processes used to solve specific tasks by mimicking human intelligence.', 'workflow': 'Based on the provided context, here\\'s a summary of the workflow used by the author:\\n\\n1. **Data Preparation**: The authors compile their Keras model using Adam optimizer and binary cross-entropy loss function.\\n2. **Positive-Unlabeled (PU) Learning**: They focus on two PU learning strategies:\\n\\t* Adapting Base Classifier: This approach adjusts the probability of being positive based on a Bayesian inference technique.\\n\\t* Spy-based Method: They use a small randomly selected group of positive examples (S) as \"spies\" to find reliable negatives. These spies are added to the unlabeled data (U) and used to train a new classifier.\\n3. **Spy-based Approach**:\\n\\t* Remove a small portion of positive samples (S) from the training data and add them to the unlabeled set (U).\\n\\t* Calculate the spy-rate, which determines what percentage of spies can remain in the unlabeled set.\\n\\t* Determine the decision boundary threshold (t) based on the spy-tolerance value.\\n\\t* Classify all added spies during training as positives if their posterior likelihood is greater than or equal to t. Samples in U with a posterior likelihood smaller than t are considered reliable negatives (RN).\\n4. **Training**: The authors train a new classifier f given original positive samples (P) and the found RN.\\n\\nPlease note that this summary focuses on the PU learning and spy-based approach, which is a specific part of the workflow described in the provided context.', 'methods': 'Based on the provided context, here is a possible methods description:\\n\\n**Methods**\\n\\nIn this study, we employed a semi-supervised learning approach called Positive Unlabeled (PU) learning to predict peptide properties from their sequences. We solely used positive examples in our training data and adapted a base classifier using layer normalization and dropout regularization to improve the learning of bidirectional dependencies between distant residues within a sequence.\\n\\nThe fully connected layers were downsized in three consecutive steps via layer normalization and dropout regularization. The final layer outputted the probability of being active for the desired training task using a sigmoid activation function. This approach allowed us to overcome the limitation of having only positive examples available, which is common in many biological applications.\\n\\nNo negative examples were used in our training process, as they are often difficult to obtain and may not accurately represent the true negative cases. Instead, we relied on the PU learning strategy to identify reliable patterns in the positive data that can be used to make accurate predictions about peptide properties.', 'models': 'According to the provided context, the following models were used in the article:\\n\\n1. **Adapting Base Classi\\ue103er**: This is a model that adapts its base classifier using unlabeled data.\\n2. **Reliable Negative Identi\\ue103cation PU method**: This is a Positive Unlabeled (PU) learning method that identifies reliable negative examples and uses them to improve the performance of the base classifier.\\n3. **PN classi\\ue103er**: This is a classical Positive-Negative (PN) learning model that uses both positive and negative labeled data for training.\\n\\nThese models were compared in terms of their accuracy (ACC) and area under the receiver operating characteristic curve (AUROC).', 'funding': \"The article does not explicitly mention any funding information. It appears to be a research paper published in a journal, and as such, it may have been supported by various sources, including grants, institutions, or authors' own funding. However, without further information, I cannot determine the specific funding sources for this study.\"}\n",
      "pdfs/Learning_peptide_properties_with_positive_examples_only.pdf done\n",
      "29 pdfs/A_ligand_discovery_toolbox_for_the_WWE_domain_family_of_human_E3_ligases.pdf\n",
      "file already done\n",
      "pdfs/A_ligand_discovery_toolbox_for_the_WWE_domain_family_of_human_E3_ligases.pdf done\n",
      "30 pdfs/Accurate_predictions_of_keyhole_depths_using_machine_learning-aided_simulations.pdf\n",
      "file already done\n",
      "pdfs/Accurate_predictions_of_keyhole_depths_using_machine_learning-aided_simulations.pdf done\n",
      "31 pdfs/Correction_to_Up-regulation_of_autophagy_is_a_mechanism_of_resistance_to_chemotherapy_and_can_be_inhibited_by_pantoprazole_to_increase_drug_sensitivity.pdf\n",
      "file already done\n",
      "pdfs/Correction_to_Up-regulation_of_autophagy_is_a_mechanism_of_resistance_to_chemotherapy_and_can_be_inhibited_by_pantoprazole_to_increase_drug_sensitivity.pdf done\n",
      "32 pdfs/reactive_carbon_capture_enables_co2_electrolysis_with_liquid_feedstocks.pdf\n",
      "file already done\n",
      "pdfs/reactive_carbon_capture_enables_co2_electrolysis_with_liquid_feedstocks.pdf done\n",
      "33 pdfs/history-agnostic-battery-degradation-inference (1).pdf\n",
      "file already done\n",
      "pdfs/history-agnostic-battery-degradation-inference (1).pdf done\n",
      "34 pdfs/quest_self_supervised_skill_abstractions_for_learning_continuous_control.pdf\n",
      "file already done\n",
      "pdfs/quest_self_supervised_skill_abstractions_for_learning_continuous_control.pdf done\n",
      "35 pdfs/language_models_can_identify_enzymatic_binding_sites_in_protein_sequences.pdf\n",
      "file already done\n",
      "pdfs/language_models_can_identify_enzymatic_binding_sites_in_protein_sequences.pdf done\n",
      "36 pdfs/ansari-white-2023-serverless-prediction-of-peptide-properties-with-recurrent-neural-networks.pdf\n",
      "file already done\n",
      "pdfs/ansari-white-2023-serverless-prediction-of-peptide-properties-with-recurrent-neural-networks.pdf done\n",
      "37 pdfs/enhancing_protein_ligand_binding_affinity_predictions_using_neural_network_potentials.pdf\n",
      "file already done\n",
      "pdfs/enhancing_protein_ligand_binding_affinity_predictions_using_neural_network_potentials.pdf done\n",
      "38 pdfs/inverse_design_of_porous_materials_a_diffusion_model_approach.pdf\n",
      "file already done\n",
      "pdfs/inverse_design_of_porous_materials_a_diffusion_model_approach.pdf done\n",
      "39 pdfs/Flipping_the_script_Understanding_riboswitches_from_an_alternative_perspective.pdf\n",
      "file already done\n",
      "pdfs/Flipping_the_script_Understanding_riboswitches_from_an_alternative_perspective.pdf done\n",
      "40 pdfs/High-Tc_superconductor_candidates_proposed_by_machine_learning.pdf\n",
      "44 pdfs/High-Tc_superconductor_candidates_proposed_by_machine_learning.pdf\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't have enough information to answer this question as there is no mention of the author's affiliation in the given context.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the given context, the topics of the paper appear to be:\n",
      "\n",
      "1. Machine learning (ML) models for predicting superconducting transition temperatures (T's) and properties.\n",
      "2. Materials science and materials prediction, specifically in the area of superconductors.\n",
      "3. Computational methods for estimating physical properties, such as stability and band gap, using density functional theory (DFT).\n",
      "\n",
      "These topics are inferred from the use of terms like \"SuperCon data set\", \"Materials Project\", \"atomic weights\", \"covalent radius\", \"band gaps\", and \"superconductivity\" throughout the text.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Yes, the article mentions discovering new materials with potentially interesting properties, such as superconductors that may be able to exist at room temperature under unknown pressure. The authors use their similarity-based ML approach to rank the entire Materials Project database of ~153,000 materials and identify the top 100 high-T materials. These materials are then filtered for band gaps lower than 1 eV, which leads to the discovery of new materials that may be valuable targets for future research.\n",
      "-----\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, it appears that the paper does not explicitly discuss specific screening algorithms for identifying novel superconductors with high-T' s. However, two machine learning methods are described:\n",
      "\n",
      "1. **Similarity-Based Machine Learning**: This method uses similarity-based ML to estimate the T of a given test sample by finding its n-nearest neighbors in the training data set.\n",
      "2. **Ridge Regression Model**: The paper trains a ridge regression model on the cleaned implicit/ambient pressure SuperCon data set to predict high-T' s for novel superconductors.\n",
      "\n",
      "These machine learning methods can be seen as screening algorithms that help identify potential novel superconductors with high-T' s from a large population of materials.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def benchmarking(file, index):    \n",
    "    llm = Bot_LLM(folder_path='db/db'+str(index))\n",
    "    llm.chunk_indexing(file)    \n",
    "    #llm.folder_path = txt\n",
    "    dico = {}\n",
    "    ## Querying the RAG\n",
    "    dico['affiliation'] = llm.query_rag(\"What are the authors affiliation. Output a dictionary ?\")\n",
    "    print(\"-----\")\n",
    "    dico['topic']  = llm.query_rag(\"What are topics of the paper ?\")\n",
    "    \n",
    "    print(\"-----\")\n",
    "    dico['new materials'] = llm.query_rag(\"does the article mention any new material discovery ?\")\n",
    "    print(\"-----\")\n",
    "    dico['screening algorithms'] = llm.query_rag(\"A screening algorithm is a systematic procedure or method used to identify individuals who may have or be at risk for a particular condition or trait within a large population. These algorithms are designed to quickly and efficiently screen out those who are unlikely to have the condition, while identifying those who may require further diagnostic evaluation or intervention. If there are any, What are the screening algorithms used in the paper ?\")\n",
    "    print(\"-----\")\n",
    "    dico['AI algorithms'] = llm.query_rag(\"AI algorithms are computational methods and processes used to solve specific tasks by mimicking human intelligence. These algorithms enable machines to learn from data, make decisions, and perform tasks that typically require human intelligence.\")\n",
    "    print(\"-----\")\n",
    "    \n",
    "    dico['workflow'] = llm.query_rag(\"Can you describe to me the workflow used by the author ?\")\n",
    "    print(\"-----\")\n",
    "    dico['methods'] = llm.query_rag(\"can you do a methods description ?\")\n",
    "    print(\"-----\")\n",
    "    dico['models'] = llm.query_rag(\"what are the models used in the article ?\")\n",
    "    print(\"-----\")\n",
    "    dico['funding'] = llm.query_rag(\"does the article mention who funded it\")\n",
    "    print(\"-----\")\n",
    "    print(dico)\n",
    "    return dico\n",
    "\n",
    "\n",
    "import glob\n",
    "files_in_dir = glob.glob('pdfs'+ \"/*\")\n",
    "dico_response = {}\n",
    "\n",
    "\n",
    "def load_json_data(json_file_path):\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "dico_response = load_json_data('benchmarking.json')\n",
    "i = 1\n",
    "true_index = 40\n",
    "for index,file in enumerate(files_in_dir):\n",
    "    print(index,file)\n",
    "    try:\n",
    "        if file.split('/')[1] not in dico_response:\n",
    "            print(true_index,file)\n",
    "            true_index += 1\n",
    "            dico_response[file.split('/')[1]] = benchmarking(file,true_index)\n",
    "        else:\n",
    "            print(\"file already done\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        dico_response[file.split('/')[1]] = None\n",
    "    print(file+\" done\")\n",
    "        \n",
    "with open('benchmarking.json', 'w') as json_file:\n",
    "    json.dump(dico_response, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf635b-9783-4970-8cc2-fdd9b9a5d38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21052a-7892-4ea2-a7bd-e2e1b7a4d113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
