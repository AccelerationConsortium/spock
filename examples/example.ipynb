{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfa978e",
   "metadata": {},
   "source": [
    "## The LLM bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bc478dc-d890-4b55-ac5f-fbb144d2585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader, TextLoader\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "import json\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "class Bot_LLM:\n",
    "    def __init__(self,model='llama3',embed_model='mxbai-embed-large', folder_path='db2'):\n",
    "        self.llm = Ollama(model=model)\n",
    "        self.oembed = OllamaEmbeddings(model=embed_model)\n",
    "        self.folder_path = folder_path\n",
    "        self.vectorestore = None\n",
    "\n",
    "    \n",
    "    def get_topic_publication_abstract(self, abstract:str, input_file:str):\n",
    "        with open(input_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        parser = JsonOutputParser()\n",
    "        \n",
    "        new_text = \"\"\"The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "\n",
    "        As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
    "        the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
    "\n",
    "        Here is the output schema:\n",
    "        ```\n",
    "        {\"topic\": {'Machine Learning: [Keyword1, keyword2, keyword3], 'Batteries: [keyword1, keyword2, keyword3]}\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"Here is a text: {abstract} Please identify the topics from the following list: {liste}. Note: A single text can belong to multiple topics, so please list all relevant topics.  \\n{format_instructions}\"\n",
    "        ,\n",
    "            input_variables=[\"abstract\",\"liste\",\"topics\"],\n",
    "            partial_variables={\"format_instructions\": new_text}\n",
    "        )\n",
    "\n",
    "\n",
    "        chain = prompt | self.llm | parser\n",
    "        topics = chain.invoke({\"abstract\": abstract, \"liste\": data.keys()})\n",
    "        print('Topics: ', topics['topic'])\n",
    "        return topics['topic']\n",
    "\n",
    "    \n",
    "    def rag(self, document:str):\n",
    "        try:\n",
    "            # The document is a pdf file\n",
    "            loader = PDFPlumberLoader(document)\n",
    "            data = loader.load()\n",
    "            chunk_size = 500\n",
    "            chunk_overlap = 20\n",
    "\n",
    "        except:\n",
    "            data = [TextLoader(text).load() for text in document]\n",
    "            data = [item for sublist in data for item in sublist]\n",
    "\n",
    "            \n",
    "        text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        all_splits = text_splitter.split_documents(data)\n",
    "        self.vectorstore = Chroma.from_documents(documents=all_splits, embedding=self.oembed, persist_directory=self.folder_path)\n",
    "        \n",
    "    def query_rag(self, question:str) -> None:\n",
    "        if self.vectorstore:\n",
    "            docs = self.vectorstore.similarity_search(question)\n",
    "            from langchain.chains import RetrievalQA\n",
    "            qachain=RetrievalQA.from_chain_type(self.llm, retriever=self.vectorstore.as_retriever(), verbose=True)\n",
    "            res = qachain.invoke({\"query\": question})\n",
    "            print(res['result'])\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"No documents loaded\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf8061-7979-48aa-8429-b6995127aceb",
   "metadata": {},
   "source": [
    "# The Publication Class: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d60eae0-6833-408f-a2e3-07964944b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Publication:\n",
    "    def __init__(self,publication_filled, llm_use:bool=True) -> None:\n",
    "        self.publication_filled = publication_filled\n",
    "        self.title = self.get_publication_title()\n",
    "        self.abstract = self.get_publication_abstract().lower()\n",
    "        self.author = self.get_author_name()\n",
    "        self.year = self.get_year()\n",
    "        self.url = self.get_publication_url()\n",
    "        self.citation = self.get_citation()\n",
    "        self.pdf = self.get_pdf()\n",
    "        self.topic = None\n",
    "    \n",
    "      \n",
    "      \n",
    "    '''  \n",
    "    def get_topic(self,output_file=\"json/ouput.json\", # à voir cette histoire avec get_topic et __get_topic\n",
    "                  input_file=\"json/response.json\") -> list[str]:\n",
    "        try:\n",
    "            with open(output_file,'r') as file:\n",
    "                data = json.load(file)\n",
    "            return data[self.author]['topic']\n",
    "        except Exception as e:\n",
    "            return self.__get_topic(input_file)\n",
    "     '''   \n",
    "    def get_publication_url(self) -> str:\n",
    "        return self.publication_filled['pub_url']\n",
    "    \n",
    "    def get_publication_title(self) -> str:\n",
    "        return self.publication_filled['bib']['title'] \n",
    "\n",
    "    def get_publication_abstract(self) -> str:\n",
    "        return self.publication_filled['bib']['abstract']\n",
    "\n",
    "    def get_author_name(self) -> str:\n",
    "        return self.publication_filled['bib']['author']\n",
    "\n",
    "    def get_year(self) -> str:\n",
    "        return self.publication_filled['bib']['pub_year']\n",
    "    \n",
    "    def get_citation(self) -> str:\n",
    "        return self.publication_filled['bib']['citation']\n",
    "    \n",
    "    def get_topic(self,llm,input_file=\"json/response.json\") -> None:\n",
    "        self.topic: dict = llm.get_topic_publication_abstract(abstract=self.abstract,input_file=input_file)\n",
    "        return self.topic\n",
    "    \n",
    "    def get_pdf(self):\n",
    "        url = f\"https://scholar.google.com/scholar?q={self.title}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            html_content = response.text\n",
    "            try:\n",
    "                return self.__parse_google_scholar(html_content)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        self.available_attributes = {'title': self.title if self.title is not None else 'N/A',\n",
    "                                     'abstract' : self.abstract if self.abstract is not None else 'N/A',\n",
    "                                        'author': self.author if self.author is not None else 'N/A',\n",
    "                                        'year': self.year if self.year is not None else 'N/A',\n",
    "                                        'url': self.url if self.url is not None else 'N/A',\n",
    "                                        'citation': self.citation if self.citation is not None else 'N/A',\n",
    "                                        'pdf': self.pdf if self.pdf is not None else 'N/A',\n",
    "                                        'topic': self.topic if self.topic is not None else 'N/A'}\n",
    "        return str(self.available_attributes)\n",
    "\n",
    "\n",
    "        \n",
    "    def __parse_google_scholar(self,html_content):\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        a_tags = soup.find_all('a')\n",
    "        try:\n",
    "            pdf_link = [a['href'] for a in a_tags if 'href' in a.attrs and '.pdf' in a['href']][0]\n",
    "            print(f\"PDF link found: {pdf_link}\")\n",
    "            return pdf_link\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "    \n",
    "    def download_pdf(self,path):\n",
    "        \n",
    "        import requests\n",
    "        path = path + self.title + \".pdf\"\n",
    "        \n",
    "        if self.pdf is not None:\n",
    "            try:\n",
    "                response = requests.get(self.pdf)\n",
    "                if response.status_code == 200:\n",
    "                    with open(path, 'wb') as file:\n",
    "                        file.write(response.content)\n",
    "                    print(f\"PDF successfully downloaded and saved to {path}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download the PDF. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"An error occurred while downloading the PDF: {e}\")\n",
    "        else:\n",
    "            from scidownl import scihub_download\n",
    "            #  scidownl by title\n",
    "            try:\n",
    "                scihub_download(self.title, paper_type=\"title\", out=path)\n",
    "            except:\n",
    "                try:\n",
    "                    # By URL\n",
    "                    scihub_download(self.pdf, out=path)\n",
    "                except:\n",
    "                    print(\"Couldn't download the PDF\")\n",
    "           \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "     \n",
    "                    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42468463-5008-440a-a710-bd4a73d66915",
   "metadata": {},
   "source": [
    "# The Author\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48476c3c-3a8c-4503-bbf3-68347e88a9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nauthor = Author('Mehrad Ansari')\\npub = Publication(author.get_last_publication())\\n\\npub.download_pdf('pdfs/')\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "import json\n",
    "\n",
    "\n",
    "class Author:\n",
    "    def __init__(self, author):\n",
    "        \"\"\"\n",
    "        Initialize an Author object.\n",
    "\n",
    "        Args:\n",
    "            author (str): The name of the author.\n",
    "        \"\"\"\n",
    "        self.author_name = author\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return a string representation of the Author object.\n",
    "\n",
    "        Returns:\n",
    "            str: The name of the author.\n",
    "        \"\"\"\n",
    "        return self.author_name\n",
    "\n",
    "    def get_last_publication(self):\n",
    "        \"\"\"\n",
    "        Get the last publication of the author.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dict containing information about the last publication.\n",
    "        \"\"\"\n",
    "        search_query = scholarly.search_author(self.author_name)\n",
    "        first_author_result = next(search_query)\n",
    "        author = scholarly.fill(first_author_result)\n",
    "        first_publication = sorted(author['publications'], \n",
    "                                   key=lambda x: int(x['bib']['pub_year'])\n",
    "                                   if 'pub_year' in x['bib'] else 0, \n",
    "                                   reverse=True)[0]\n",
    "        first_publication_filled = scholarly.fill(first_publication)\n",
    "        return first_publication_filled\n",
    "\n",
    "    def setup_author(self, output_file, llm):\n",
    "        \"\"\"\n",
    "        Setup the author by adding their last publication to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            output_file (str): The path to the JSON file.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        with open(output_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        author_last_publication = Publication(self.get_last_publication())\n",
    "        \n",
    "\n",
    "        \n",
    "        data[self.author_name] = {\n",
    "            \"title\": author_last_publication.title,\n",
    "            \"abstract\": author_last_publication.abstract,\n",
    "            \"topic\": author_last_publication.get_topic(llm=llm), \n",
    "            \"author\": author_last_publication.author, \n",
    "            \"year\": author_last_publication.year,\n",
    "            \"url\": author_last_publication.url,\n",
    "            \"pdf\": author_last_publication.pdf,\n",
    "        }\n",
    "        \n",
    "        \n",
    "        with open(output_file, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "\n",
    "'''\n",
    "author = Author('Mehrad Ansari')\n",
    "pub = Publication(author.get_last_publication())\n",
    "\n",
    "pub.download_pdf('pdfs/')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc875ae1-baaf-468b-99ed-bd2dd9bde049",
   "metadata": {},
   "source": [
    "## Main module to test function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa3679f7-7f70-4168-a95e-5a07f34f01dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author object created successfully\n",
      "PDF link found: https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/640a28a76642bf8c8f413f32/original/history-agnostic-battery-degradation-inference.pdf\n",
      "Publication object created successfully - Having fetched the last publication\n",
      "current available attributes: \n",
      "{'title': 'History-agnostic battery degradation inference', 'abstract': 'lithium-ion batteries (libs) have attracted widespread attention as an efficient energy storage device on electric vehicles (ev) to achieve emission-free mobility. however, the performance of libs deteriorates with time and usage, and the state of health of used batteries are difficult to quantify. having accurate estimations of a battery’s remaining life across different life stages would benefit maintenance, safety, and serve as a means of qualifying used batteries for second-life applications. since the full history of a battery may not always be available in downstream applications, in this study, we demonstrate a deep learning framework that enables dynamic degradation rate prediction, including both short-term and long-term forecasting, while requiring only the most recent battery usage information. specifically, our model takes a rolling window of current and voltage time-series inputs, and predicts the near-term and …', 'author': 'Mehrad Ansari and Steven B Torrisi and Amalie Trewartha and Shijing Sun', 'year': 2024, 'url': 'https://www.sciencedirect.com/science/article/pii/S2352152X23036782', 'citation': 'Journal of Energy Storage 81, 110279, 2024', 'pdf': 'https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/640a28a76642bf8c8f413f32/original/history-agnostic-battery-degradation-inference.pdf', 'topic': 'N/A'}\n",
      "Running LLM on the abstract of the publication\n",
      "Topics:  {'Machine Learning': ['deep learning framework', 'dynamic degradation rate prediction', 'rolling window'], 'Batteries': ['lithium-ion batteries', 'electric vehicles', 'state of health', 'remaining life', 'second-life applications']}\n",
      "current available attributes: \n",
      "{'title': 'History-agnostic battery degradation inference', 'abstract': 'lithium-ion batteries (libs) have attracted widespread attention as an efficient energy storage device on electric vehicles (ev) to achieve emission-free mobility. however, the performance of libs deteriorates with time and usage, and the state of health of used batteries are difficult to quantify. having accurate estimations of a battery’s remaining life across different life stages would benefit maintenance, safety, and serve as a means of qualifying used batteries for second-life applications. since the full history of a battery may not always be available in downstream applications, in this study, we demonstrate a deep learning framework that enables dynamic degradation rate prediction, including both short-term and long-term forecasting, while requiring only the most recent battery usage information. specifically, our model takes a rolling window of current and voltage time-series inputs, and predicts the near-term and …', 'author': 'Mehrad Ansari and Steven B Torrisi and Amalie Trewartha and Shijing Sun', 'year': 2024, 'url': 'https://www.sciencedirect.com/science/article/pii/S2352152X23036782', 'citation': 'Journal of Energy Storage 81, 110279, 2024', 'pdf': 'https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/640a28a76642bf8c8f413f32/original/history-agnostic-battery-degradation-inference.pdf', 'topic': {'Machine Learning': ['deep learning framework', 'dynamic degradation rate prediction', 'rolling window'], 'Batteries': ['lithium-ion batteries', 'electric vehicles', 'state of health', 'remaining life', 'second-life applications']}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "author = Author('Mehrad Ansari')\n",
    "\n",
    "print(\"Author object created successfully\")\n",
    "\n",
    "pub = Publication(author.get_last_publication())\n",
    "\n",
    "print(\"Publication object created successfully - Having fetched the last publication\")\n",
    "\n",
    "print('current available attributes: ')\n",
    "print(pub)\n",
    "\n",
    "print('Running LLM on the abstract of the publication')\n",
    "pub.get_topic(llm=Bot_LLM())\n",
    "print('current available attributes: ')\n",
    "print(pub)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
