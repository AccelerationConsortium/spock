{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Abstracts Using a Large Language Model (LLM)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this Jupyter Notebook, we will perform an in-depth analysis of abstracts extracted from a CSV file using a Large Language Model (LLM). The goal of this analysis is to leverage the capabilities of LLMs to extract meaningful insights, identify key themes, and perform various natural language processing (NLP) tasks on the abstracts.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- **Data Loading**: Import and preprocess abstracts from a CSV file.\n",
    "- **Text Analysis**: Utilize LLMs to analyze the content of the abstracts.\n",
    "\n",
    "### Tools and Libraries\n",
    "\n",
    "- **LangChain**: To interface with the LLM.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. **Data Import**: Load the CSV file containing the abstracts.\n",
    "3. **LLM Integration**: Use the LLM to perform various NLP tasks.\n",
    "\n",
    "By the end of this notebook, you will have a comprehensive understanding of how to use LLMs for analyzing textual data and extracting valuable insights from scientific abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Import**: Load the CSV file containing the abstracts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from downloader import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10.1038/s41586-020-2746-2': {'abstract': 'The genetic circuits that allow cancer cells to evade destruction by the host immune system remain poorly understood1\\x963. Here, to identify a phenotypically robust core set of genes and pathways that enable cancer cells to evade killing mediated by cytotoxic T\\xa0lymphocytes (CTLs), we performed genome-wide CRISPR screens across a panel of genetically diverse mouse cancer cell lines that were cultured in the presence of CTLs. We identify a core set of 182\\xa0genes across these mouse cancer models, the individual perturbation of which increases either the sensitivity or the resistance of cancer cells to CTL-mediated toxicity. Systematic exploration of our dataset using genetic co-similarity reveals the hierarchical and coordinated manner in which genes and pathways act in cancer cells to orchestrate their evasion of CTLs, and shows that discrete functional modules that control the interferon response and tumour necrosis factor (TNF)-induced cytotoxicity are dominant sub-phenotypes. Our data establish a central role for genes that were previously identified as negative regulators of the type-II interferon response (for example, Ptpn2, Socs1 and Adar1) in mediating CTL evasion, and show that the lipid-droplet-related gene Fitm2 is required for maintaining cell fitness after exposure to interferon-? (IFN?). In addition, we identify the autophagy pathway as a conserved mediator of the evasion of CTLs by cancer cells, and show that this pathway is required to resist cytotoxicity induced by the cytokines IFN? and\\xa0TNF. Through the mapping of cytokine- and CTL-based genetic interactions, together with in vivo CRISPR screens, we show how the pleiotropic effects of autophagy control cancer-cell-intrinsic evasion of killing by CTLs and we highlight the importance of these effects within the tumour microenvironment. Collectively, these\\xa0data expand our knowledge of the genetic circuits that are involved in the evasion of the immune system by cancer cells, and highlight genetic interactions that contribute to phenotypes associated with escape from killing by CTLs.'}, '10.1016/j.surfcoat.2020.126648': {'abstract': 'The formation of \\x93mud-cracking\\x94 networks in coatings and thin films is a widely observed phenomenon, particularly in Cr-containing electrodeposited materials. Here we have documented and described aspects of mud-cracking specific to Cr-containing electrodeposited alloys made from trivalent Cr electrolytes. In addition to describing the different crack patterns observed in these materials and their relationship to the electroplating parameters, thickness, and composition of the coatings, we also investigated a never-before-reported phenomenon of post-deposition time-dependent cracking. Our analysis shows that cracks continue to form after the electroplating procedure has been completed, generally saturating 2 h after synthesis, but can continue for days afterwards. We theorize that the behaviour is related to the state of oxygen in these samples, which can sometimes reach beyond 30 at.%. It is believed that the breakdown of Cr-hydroxides and Cr-hydrides drives the increase in internal stresses that result in continued cracking. As a result, this time-dependent cracking phenomenon may impact the suitability of certain Cr-alloy electrodeposits in some applications.'}, '': {'abstract': 'Optimization of particle size distribution in coal fly ash is an important production parameter to improve fly ash quality for use in concrete. The particle size distributions of fly ash from nine Canadian and Indian sources were measured using a Laser Diffraction Analyzer (LDA). Use of a LDA provided a meaningful characterization of particle size variability between the studied fly ashes. Scanning electron microscopy (SEM) was employed to visualize the distribution and shapes of the fly ash particles. Each LDA measurement was verified by a corresponding scanning electron micrograph. Both LDA and SEM were found to be quick and efficient techniques in terms of sample preparation, data acquisition, and analysis. A bimodal size distribution was associated with all fly ashes as analyzed by LDA. The SEM micrographs showed two forms of particles: large agglomerates and smaller spherical primary particles.'}, '10.1021/acsaem.9b02371': {'abstract': 'We present electrospinning as a versatile technique to design and fabricate tailored polymer electrolyte membrane (PEM) fuel cell gas diffusion layers (GDLs) with a pore-size gradient (increasing from catalyst layer to flow field) to enhance the high current density performance and water management behavior of a PEM fuel cell. The novel graded electrospun GDL exhibits highly robust performance over a range of inlet gas relative humidities (RH). At relatively dry (50% RH) inlet conditions that exacerbate ohmic losses, the graded GDL lowers ohmic resistance and improves high current density performance compared to a uniform GDL with larger pores and fiber diameters. Specifically, the graded GDL facilitates a beneficial degree of liquid water retention at the catalyst layer/GDL interface due to the high capillary pressure inherent in its microstructure, thereby improving membrane hydration. Additionally, enhanced graphitization and connectivity of the graded electrospun fibers improves heat dissipation from the catalyst layer interface compared to the GDL with larger fiber diameters, thereby reducing membrane dehydration. When the inlet RH is raised to fully humid (100% RH) conditions, the graded GDL mitigates liquid water accumulation and lowers mass transport resistance. Specifically, the pore size gradient directs the removal of liquid water from the GDL, resulting in superior performance at high current densities.'}}\n"
     ]
    }
   ],
   "source": [
    "def read_csv(file_path, num_lines):\n",
    "    with open(file_path, 'r', encoding=\"utf8\", errors='ignore') as file:\n",
    "        reader = csv.reader(file)\n",
    "        dico = {}\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= num_lines:\n",
    "                break\n",
    "            if i == 0:\n",
    "                continue\n",
    "            dico[row[17]] = {\"abstract\": row[4]} \n",
    "            \"\"\"\n",
    "            try:\n",
    "               \n",
    "                downloader = Downloader(row[17], 'doi', f\"pdfs/{row[17]}.pdf\")\n",
    "                downloader.download()\n",
    "                dico[row[17]].update({\"pdf\": True})\n",
    "            except Exception as e:\n",
    "                dico[row[17]].update({\"pdf\": False})\n",
    "                print(e)\n",
    "          \"\"\"      \n",
    "        print(dico)\n",
    "        return dico\n",
    "\n",
    "file_path = \"filtered_AC_publications.csv\"\n",
    "num_lines = 5\n",
    "dico = read_csv(file_path, num_lines)\n",
    "\n",
    "## Writing the pdfs into a txt file:\n",
    "\n",
    "for key in dico:\n",
    "    with open(\"pdfs.txt\", \"a\") as f:\n",
    "        f.write(key + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# --------------------- Section 4: Download articles based on dictionary ---------------------\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Assuming 'dico' is your dictionary where DOIs are the keys\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doi \u001b[38;5;129;01min\u001b[39;00m dico\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 90\u001b[0m     \u001b[43marticle_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# --------------------- Section 5: Retry for missing articles ---------------------\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Check for articles that were not downloaded and try again\u001b[39;00m\n\u001b[1;32m     94\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/youssef/clone/spock/scholarly_creative_work/pdfs/\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Your preferred download directory\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 84\u001b[0m, in \u001b[0;36marticle_get\u001b[0;34m(doi)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marticle_get\u001b[39m(doi):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# If the article is successfully downloaded, rename the file\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mscihub_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoi\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     85\u001b[0m         rename_file(doi)\n",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m, in \u001b[0;36mscihub_get\u001b[0;34m(doi)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Search for the article using its DOI\u001b[39;00m\n\u001b[1;32m     26\u001b[0m wd\u001b[38;5;241m.\u001b[39mget(root \u001b[38;5;241m+\u001b[39m doi)  \u001b[38;5;66;03m# Open the Sci-Hub page for the DOI\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pause to let the page load\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Attempt to click the download button\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     b \u001b[38;5;241m=\u001b[39m wd\u001b[38;5;241m.\u001b[39mfind_element(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxpath\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//*[@id=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuttons\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]/button\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service  # Import Service\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "# --------------------- Section 1: Sci-Hub article downloader ---------------------\n",
    "# Function to automate downloading of articles from Sci-Hub using DOIs\n",
    "def scihub_get(doi):\n",
    "    chromeOptions = webdriver.ChromeOptions()  # Set Chrome options for Selenium\n",
    "    prefs = {\"download.default_directory\" : \"/home/youssef/clone/spock/scholarly_creative_work/pdfs\"}  # Your preferred download directory\n",
    "    chromeOptions.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    # Specify the path to the Chrome driver\n",
    "    chrome_driver_path = \"/usr/bin/chromedriver\"  # Update this with your actual path\n",
    "    service = Service(chrome_driver_path)  # Create a Service object for ChromeDriver\n",
    "    \n",
    "    # Initialize Chrome WebDriver using Service\n",
    "    wd = webdriver.Chrome(service=service, options=chromeOptions)\n",
    "    \n",
    "    # List of Sci-Hub domains to randomly select from\n",
    "    scihub = ['https://sci-hub.ru/', 'https://sci-hub.st/', 'https://sci-hub.se/']\n",
    "    root = scihub[random.randint(0, 2)]  # Select a random Sci-Hub domain\n",
    "    \n",
    "    # Search for the article using its DOI\n",
    "    wd.get(root + doi)  # Open the Sci-Hub page for the DOI\n",
    "    time.sleep(1)  # Pause to let the page load\n",
    "    \n",
    "    try:\n",
    "        # Attempt to click the download button\n",
    "        b = wd.find_element('xpath','//*[@id=\"buttons\"]/button')\n",
    "        b.click()\n",
    "        flag = True\n",
    "        elements = len( [f for f in os.listdir(\"/home/youssef/clone/spock/scholarly_creative_work/pdfs\") if f.endswith('.pdf')])\n",
    "        while elements == len([f for f in os.listdir(\"/home/youssef/clone/spock/scholarly_creative_work/pdfs\") if f.endswith('.pdf')]):\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        # Print error message if the article access fails\n",
    "        print('access failed.    doi = ' + doi)\n",
    "        flag = False\n",
    "        print(e)\n",
    "        time.sleep(5)  # Shorter wait before next attempt\n",
    "    \n",
    "    wd.quit()  # Close the WebDriver\n",
    "    return flag  # Return whether the download was successful\n",
    "\n",
    "# --------------------- Section 2: Rename downloaded files (using DOI only) ---------------------\n",
    "# Function to rename downloaded files based only on DOI\n",
    "def rename_file(doi):\n",
    "    time.sleep(1)  # Wait to ensure the file is downloaded\n",
    "    path = '/home/youssef/clone/spock/scholarly_creative_work/pdfs/'  # Your preferred download directory\n",
    "    # List all files in the download directory\n",
    "    dir_list = os.listdir(path)\n",
    "    \n",
    "    # Check if any files were downloaded\n",
    "    if len(dir_list) > 0:\n",
    "        found = 0\n",
    "        for file in dir_list:\n",
    "            if file[0:3] != 'No_':  # Skip already renamed files\n",
    "                found = 1  # File found\n",
    "                break\n",
    "\n",
    "        if found == 0:  # If no new file is found\n",
    "            print('download failed.    doi = ' + doi)\n",
    "        else:  # If a new file is found\n",
    "            l = file.split('.')\n",
    "            if l[len(l) - 1] != \"pdf\":  # Check if the file was fully downloaded\n",
    "                print('download incomplete.    doi = ' + doi)\n",
    "            else:\n",
    "                # Rename the file based only on DOI\n",
    "                old = path + '/' + file\n",
    "                doi_cleaned = doi.replace(\"/\", \"_\")  # Replace \"/\" in DOI with \"_\" to avoid issues in file name\n",
    "                new = path + doi_cleaned + '.pdf'\n",
    "                os.rename(old, new)  # Rename the file\n",
    "\n",
    "# --------------------- Section 3: Article download manager ---------------------\n",
    "# Function to handle the entire process for downloading and renaming articles\n",
    "def article_get(doi):\n",
    "    # If the article is successfully downloaded, rename the file\n",
    "    if scihub_get(doi):\n",
    "        rename_file(doi)\n",
    "\n",
    "# --------------------- Section 4: Download articles based on dictionary ---------------------\n",
    "# Assuming 'dico' is your dictionary where DOIs are the keys\n",
    "for doi in dico.keys():\n",
    "    article_get(doi)\n",
    "\n",
    "# --------------------- Section 5: Retry for missing articles ---------------------\n",
    "# Check for articles that were not downloaded and try again\n",
    "path = '/home/youssef/clone/spock/scholarly_creative_work/pdfs/'  # Your preferred download directory\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "for doi in dico.keys():\n",
    "    doi_cleaned = doi.replace(\"/\", \"_\")  # Replace \"/\" in DOI for file name\n",
    "    filename = 'No_' + doi_cleaned + '.pdf'\n",
    "    \n",
    "    if filename in dir_list:\n",
    "        continue  # Skip if the file already exists\n",
    "    else:\n",
    "        article_get(doi)\n",
    "\n",
    "# --------------------- Section 6: Output missing articles ---------------------\n",
    "# Output information about articles that are still missing after retry\n",
    "count = 0\n",
    "\n",
    "for doi in dico.keys():\n",
    "    doi_cleaned = doi.replace(\"/\", \"_\")  # Replace \"/\" in DOI for file name\n",
    "    filename = 'No_' + doi_cleaned + '.pdf'\n",
    "    \n",
    "    if filename in dir_list:\n",
    "        continue  # Skip if the file already exists\n",
    "    else:\n",
    "        # Print details of missing articles\n",
    "        print(filename + \"    doi: \" + doi)\n",
    "        count += 1\n",
    "\n",
    "# Print the total number of missing articles\n",
    "print(str(count) + \" articles missing in total.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(abstract:str):\n",
    "    \n",
    "    Llm = Ollama(model='llama3', temperature=0.2)\n",
    "    \n",
    "    \n",
    "    if abstract is None:\n",
    "        raise ValueError(\"Abstract is required\")\n",
    "    \n",
    "    parser = JsonOutputParser()\n",
    "    \n",
    "    #AI / accelerated materials discovery / SDLs / autonomous labs / high-throughput experimentation / high-throughput DFT\n",
    "    \n",
    "    topics = [\"Machine Learning\", \"Batteries\", \"AI\", \"accelerated materials discovery\", \"Self Driving Labs\", \"autonomous labs\", \"high-throughput experimentation\", \"high-throughput DFT\"]\n",
    "    \n",
    "    \n",
    "    new_text = \"\"\"\n",
    "    \n",
    "    The output needs to be formated as the following: \n",
    "    \n",
    "    {\n",
    "    \"topic\": {\n",
    "    \"topic1\": [\"Keyword1\", \"Keyword2\", \"Keyword3\"],\n",
    "    \"topic2\": [\"Keyword1\", \"Keyword2\", \"Keyword3\"]\n",
    "    }\n",
    "    }\n",
    "    \n",
    "    Only output the dictionary above, nothing else with it.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "    template=\" So you are a text assistant and I need you to help me identify the topics from the following list the text given to you {topics}. \\n Here's the text: {abstract}. \\n\\n Note: A single text can belong to multiple topics, so please list all relevant topics. {format_instructions}\",\n",
    "    input_variables=[\"format_instructions\", \"abstract\", \"topics\"]\n",
    "    )\n",
    "\n",
    "    chain = prompt | Llm | parser\n",
    "    topics = chain.invoke({\"format_instructions\": new_text, \"abstract\": abstract, \"topics\": topics})\n",
    "    return list(topics.values())[0]\n",
    "\n",
    "\n",
    "print(get_topic(\"The development of high-performance batteries is crucial for the future of electric vehicles. The current generation of batteries are not able to provide the range and power required for long-distance travel. This project aims to develop new materials for batteries that can provide higher energy density and faster charging times.\"))\n",
    "\n",
    "def get_info(abstract:str = None, **kwargs):\n",
    "    Llm = Ollama(model='llama3', temperature=0.5)\n",
    "    if abstract is None:\n",
    "        raise ValueError(\"Abstract is required\")\n",
    "    \n",
    "    dico = {}\n",
    "    for key, question in kwargs.items():\n",
    "        print(key, question)\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"So you are a text assistant and I want you to assist me by providing the following information: {question}. \\n\\n Here's the text: {abstract}. \\n\\n If the text doesn't contain any information about the topic given, output: 'N/A'\",\n",
    "            input_variables=[\"abstract\", \"question\"]\n",
    "        )\n",
    "        chain = prompt | Llm \n",
    "        info = chain.invoke({\"abstract\": abstract, \"question\": question})\n",
    "        dico[key] = info\n",
    "    print(dico)\n",
    "    return dico\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dico:\n",
    "    dico[key].update({\"topic\":get_topic(dico[key][\"abstract\"])})\n",
    "    dico[key].update(get_info(dico[key][\"abstract\"],affiliation=\"What affiliations do the authors or characters in the text have?\",\n",
    "                        new_materials=\"Does the text mention any new materials or discoveries?\",\n",
    "                        screening_algorithms=\"Are there any screening algorithms or systematic procedures discussed in the text?\",\n",
    "                        ai_algorithms=\"Does the text reference any AI algorithms or methods related to artificial intelligence?\",\n",
    "                        workflow=\"Can you describe the workflow or process followed in the text?\",\n",
    "                        methods=\"Can you summarize the methods or approaches mentioned in the text?\",\n",
    "                        models=\"What models or frameworks are discussed or used in the text?\",\n",
    "                        funding=\"Does the text mention any funding sources or sponsors?\"))\n",
    "\n",
    "print(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"output.json\", 'w') as file:\n",
    "    json.dump(dico, file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
