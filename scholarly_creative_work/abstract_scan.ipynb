{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Abstracts Using a Large Language Model (LLM)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this Jupyter Notebook, we will perform an in-depth analysis of abstracts extracted from a CSV file using a Large Language Model (LLM). The goal of this analysis is to leverage the capabilities of LLMs to extract meaningful insights, identify key themes, and perform various natural language processing (NLP) tasks on the abstracts.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- **Data Loading**: Import and preprocess abstracts from a CSV file.\n",
    "- **Text Analysis**: Utilize LLMs to analyze the content of the abstracts.\n",
    "\n",
    "### Tools and Libraries\n",
    "\n",
    "- **LangChain**: To interface with the LLM.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. **Data Import**: Load the CSV file containing the abstracts.\n",
    "3. **LLM Integration**: Use the LLM to perform various NLP tasks.\n",
    "\n",
    "By the end of this notebook, you will have a comprehensive understanding of how to use LLMs for analyzing textual data and extracting valuable insights from scientific abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/gpfs/fs1/home/m/mehrad/brikiyou/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (4.45.0.dev0)\n",
      "Requirement already satisfied: filelock in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install  pymupdf python-Levenshtein nltk\n",
    "#!pip install  git+https://github.com/huggingface/transformers.git\n",
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Import**: Load the CSV file containing the abstracts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from downloader import *\n",
    "#!pip install git+https://github.com/facebookresearch/nougat\n",
    "#%pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/home/m/mehrad/brikiyou/.cache/huggingface'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/facebook/nougat-base/resolve/main/processor_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/huggingface_hub/file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:315\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    314\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-66e4b6bd-25d6d2c42e9603a523e05e13;d9b2b578-4950-413d-9752-c7c5ea8a7f29)\n\nEntry Not Found for url: https://huggingface.co/facebook/nougat-base/resolve/main/processor_config.json.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/scinet/balam/rocky9/software/2023a/opt/base/python/3.11.5/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/m/mehrad/brikiyou/.cache/huggingface/hub/models--facebook--nougat-base/.no_exist/abfecedbb34367c820e233f710fdc7f54e6ab249'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/scinet/balam/rocky9/software/2023a/opt/base/python/3.11.5/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/m/mehrad/brikiyou/.cache/huggingface/hub/models--facebook--nougat-base/.no_exist'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/scinet/balam/rocky9/software/2023a/opt/base/python/3.11.5/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/m/mehrad/brikiyou/.cache/huggingface/hub/models--facebook--nougat-base'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/scinet/balam/rocky9/software/2023a/opt/base/python/3.11.5/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/m/mehrad/brikiyou/.cache/huggingface/hub'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m LLM()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf_to_md\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscholarly_creative_work/papers/10.2533_chimia.2019.1028.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/spock/scholarly_creative_work/LLM.py:28\u001b[0m, in \u001b[0;36mLLM.pdf_to_md\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     25\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRANSFORMERS_CACHE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/m/mehrad/brikiyou/scratch/transformers_cache\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 28\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mAutoProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/nougat-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m VisionEncoderDecoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/nougat-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/transformers/models/auto/processing_auto.py:250\u001b[0m, in \u001b[0;36mAutoProcessor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m get_file_from_repo_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    246\u001b[0m     key: kwargs[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(get_file_from_repo)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[1;32m    247\u001b[0m }\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Let's start by checking whether the processor class is saved in a processor config\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m processor_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mget_file_from_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROCESSOR_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mget_file_from_repo_kwargs\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     config_dict, _ \u001b[38;5;241m=\u001b[39m ProcessorMixin\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/transformers/utils/hub.py:557\u001b[0m, in \u001b[0;36mget_file_from_repo\u001b[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token` and `use_auth_token` are both specified. Please set only the argument `token`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    555\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/huggingface_hub/file_download.py:1282\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/fs0/scratch/m/mehrad/brikiyou/spock_package/lib/python3.11/site-packages/huggingface_hub/file_download.py:1729\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     no_exist_file_path \u001b[38;5;241m=\u001b[39m Path(storage_folder) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.no_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m commit_hash \u001b[38;5;241m/\u001b[39m relative_filename\n\u001b[0;32m-> 1729\u001b[0m     \u001b[43mno_exist_file_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m     no_exist_file_path\u001b[38;5;241m.\u001b[39mtouch()\n\u001b[1;32m   1731\u001b[0m     _cache_commit_hash_for_specific_revision(storage_folder, revision, commit_hash)\n",
      "File \u001b[0;32m/scinet/balam/rocky9/software/2023a/opt/base/python/3.11.5/lib/python3.11/pathlib.py:1120\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
      "File \u001b[0;32m/scinet/balam/rocky9/software/2023a/opt/base/python/3.11.5/lib/python3.11/pathlib.py:1120\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Path.mkdir at line 1120 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/scinet/balam/rocky9/software/2023a/opt/base/python/3.11.5/lib/python3.11/pathlib.py:1120\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
      "File \u001b[0;32m/scinet/balam/rocky9/software/2023a/opt/base/python/3.11.5/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/home/m/mehrad/brikiyou/.cache/huggingface'"
     ]
    }
   ],
   "source": [
    "from LLM import LLM\n",
    "from pathlib import Path\n",
    "\n",
    "llm = LLM()\n",
    "llm.pdf_to_md(Path(\"scholarly_creative_work/papers/10.2533_chimia.2019.1028.pdf\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10.1038/s41586-020-2746-2': {'abstract': 'The genetic circuits that allow cancer cells to evade destruction by the host immune system remain poorly understood1\\x963. Here, to identify a phenotypically robust core set of genes and pathways that enable cancer cells to evade killing mediated by cytotoxic T\\xa0lymphocytes (CTLs), we performed genome-wide CRISPR screens across a panel of genetically diverse mouse cancer cell lines that were cultured in the presence of CTLs. We identify a core set of 182\\xa0genes across these mouse cancer models, the individual perturbation of which increases either the sensitivity or the resistance of cancer cells to CTL-mediated toxicity. Systematic exploration of our dataset using genetic co-similarity reveals the hierarchical and coordinated manner in which genes and pathways act in cancer cells to orchestrate their evasion of CTLs, and shows that discrete functional modules that control the interferon response and tumour necrosis factor (TNF)-induced cytotoxicity are dominant sub-phenotypes. Our data establish a central role for genes that were previously identified as negative regulators of the type-II interferon response (for example, Ptpn2, Socs1 and Adar1) in mediating CTL evasion, and show that the lipid-droplet-related gene Fitm2 is required for maintaining cell fitness after exposure to interferon-? (IFN?). In addition, we identify the autophagy pathway as a conserved mediator of the evasion of CTLs by cancer cells, and show that this pathway is required to resist cytotoxicity induced by the cytokines IFN? and\\xa0TNF. Through the mapping of cytokine- and CTL-based genetic interactions, together with in vivo CRISPR screens, we show how the pleiotropic effects of autophagy control cancer-cell-intrinsic evasion of killing by CTLs and we highlight the importance of these effects within the tumour microenvironment. Collectively, these\\xa0data expand our knowledge of the genetic circuits that are involved in the evasion of the immune system by cancer cells, and highlight genetic interactions that contribute to phenotypes associated with escape from killing by CTLs.'}, '10.1016/j.surfcoat.2020.126648': {'abstract': 'The formation of \\x93mud-cracking\\x94 networks in coatings and thin films is a widely observed phenomenon, particularly in Cr-containing electrodeposited materials. Here we have documented and described aspects of mud-cracking specific to Cr-containing electrodeposited alloys made from trivalent Cr electrolytes. In addition to describing the different crack patterns observed in these materials and their relationship to the electroplating parameters, thickness, and composition of the coatings, we also investigated a never-before-reported phenomenon of post-deposition time-dependent cracking. Our analysis shows that cracks continue to form after the electroplating procedure has been completed, generally saturating 2 h after synthesis, but can continue for days afterwards. We theorize that the behaviour is related to the state of oxygen in these samples, which can sometimes reach beyond 30 at.%. It is believed that the breakdown of Cr-hydroxides and Cr-hydrides drives the increase in internal stresses that result in continued cracking. As a result, this time-dependent cracking phenomenon may impact the suitability of certain Cr-alloy electrodeposits in some applications.'}, '': {'abstract': 'Optimization of particle size distribution in coal fly ash is an important production parameter to improve fly ash quality for use in concrete. The particle size distributions of fly ash from nine Canadian and Indian sources were measured using a Laser Diffraction Analyzer (LDA). Use of a LDA provided a meaningful characterization of particle size variability between the studied fly ashes. Scanning electron microscopy (SEM) was employed to visualize the distribution and shapes of the fly ash particles. Each LDA measurement was verified by a corresponding scanning electron micrograph. Both LDA and SEM were found to be quick and efficient techniques in terms of sample preparation, data acquisition, and analysis. A bimodal size distribution was associated with all fly ashes as analyzed by LDA. The SEM micrographs showed two forms of particles: large agglomerates and smaller spherical primary particles.'}, '10.1021/acsaem.9b02371': {'abstract': 'We present electrospinning as a versatile technique to design and fabricate tailored polymer electrolyte membrane (PEM) fuel cell gas diffusion layers (GDLs) with a pore-size gradient (increasing from catalyst layer to flow field) to enhance the high current density performance and water management behavior of a PEM fuel cell. The novel graded electrospun GDL exhibits highly robust performance over a range of inlet gas relative humidities (RH). At relatively dry (50% RH) inlet conditions that exacerbate ohmic losses, the graded GDL lowers ohmic resistance and improves high current density performance compared to a uniform GDL with larger pores and fiber diameters. Specifically, the graded GDL facilitates a beneficial degree of liquid water retention at the catalyst layer/GDL interface due to the high capillary pressure inherent in its microstructure, thereby improving membrane hydration. Additionally, enhanced graphitization and connectivity of the graded electrospun fibers improves heat dissipation from the catalyst layer interface compared to the GDL with larger fiber diameters, thereby reducing membrane dehydration. When the inlet RH is raised to fully humid (100% RH) conditions, the graded GDL mitigates liquid water accumulation and lowers mass transport resistance. Specifically, the pore size gradient directs the removal of liquid water from the GDL, resulting in superior performance at high current densities.'}}\n"
     ]
    }
   ],
   "source": [
    "def read_csv(file_path, num_lines):\n",
    "    with open(file_path, 'r', encoding=\"utf8\", errors='ignore') as file:\n",
    "        reader = csv.reader(file)\n",
    "        dico = {}\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= num_lines:\n",
    "                break\n",
    "            if i == 0:\n",
    "                continue\n",
    "            dico[row[17]] = {\"abstract\": row[4]} \n",
    "            \"\"\"\n",
    "            try:\n",
    "               \n",
    "                downloader = Downloader(row[17], 'doi', f\"pdfs/{row[17]}.pdf\")\n",
    "                downloader.download()\n",
    "                dico[row[17]].update({\"pdf\": True})\n",
    "            except Exception as e:\n",
    "                dico[row[17]].update({\"pdf\": False})\n",
    "                print(e)\n",
    "          \"\"\"      \n",
    "        print(dico)\n",
    "        return dico\n",
    "\n",
    "file_path = \"filtered_AC_publications.csv\"\n",
    "num_lines = 5\n",
    "dico = read_csv(file_path, num_lines)\n",
    "\n",
    "## Writing the pdfs into a txt file:\n",
    "\n",
    "for key in dico:\n",
    "    with open(\"pdfs.txt\", \"a\") as f:\n",
    "        f.write(key + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_topic(abstract:str):\n",
    "    \n",
    "    Llm = Ollama(model='llama3', temperature=0.2)\n",
    "    \n",
    "    \n",
    "    if abstract is None:\n",
    "        raise ValueError(\"Abstract is required\")\n",
    "    \n",
    "    parser = JsonOutputParser()\n",
    "    \n",
    "    #AI / accelerated materials discovery / SDLs / autonomous labs / high-throughput experimentation / high-throughput DFT\n",
    "    \n",
    "    topics = [\"Machine Learning\", \"Batteries\", \"AI\", \"accelerated materials discovery\", \"Self Driving Labs\", \"autonomous labs\", \"high-throughput experimentation\", \"high-throughput DFT\"]\n",
    "    \n",
    "    \n",
    "    new_text = \"\"\"\n",
    "    \n",
    "    The output needs to be formated as the following: \n",
    "    \n",
    "    {\n",
    "    \"topic\": {\n",
    "    \"topic1\": [\"Keyword1\", \"Keyword2\", \"Keyword3\"],\n",
    "    \"topic2\": [\"Keyword1\", \"Keyword2\", \"Keyword3\"]\n",
    "    }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    Only output the dictionary above, nothing else with it.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "    template=\" So you are a text assistant and I need you to help me identify the topics from the following list the text given to you {topics}. \\n Here's the text: {abstract}. \\n\\n Note: A single text can belong to multiple topics, so please list all relevant topics. {format_instructions}\",\n",
    "    input_variables=[\"format_instructions\", \"abstract\", \"topics\"]\n",
    "    )\n",
    "\n",
    "    chain = prompt | Llm | parser\n",
    "    topics = chain.invoke({\"format_instructions\": new_text, \"abstract\": abstract, \"topics\": topics})\n",
    "    return list(topics.values())[0]\n",
    "\n",
    "\n",
    "print(get_topic(\"The development of high-performance batteries is crucial for the future of electric vehicles. The current generation of batteries are not able to provide the range and power required for long-distance travel. This project aims to develop new materials for batteries that can provide higher energy density and faster charging times.\"))\n",
    "\n",
    "def get_info(abstract:str = None, **kwargs):\n",
    "    Llm = Ollama(model='llama3', temperature=0.5)\n",
    "    if abstract is None:\n",
    "        raise ValueError(\"Abstract is required\")\n",
    "    \n",
    "    dico = {}\n",
    "    for key, question in kwargs.items():\n",
    "        print(key, question)\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"So you are a text assistant and I want you to assist me by providing the following information: {question}. \\n\\n Here's the text: {abstract}. \\n\\n If the text doesn't contain any information about the topic given, output: 'N/A'\",\n",
    "            input_variables=[\"abstract\", \"question\"]\n",
    "        )\n",
    "        chain = prompt | Llm \n",
    "        info = chain.invoke({\"abstract\": abstract, \"question\": question})\n",
    "        dico[key] = info\n",
    "    print(dico)\n",
    "    return dico\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dico:\n",
    "    dico[key].update({\"topic\":get_topic(dico[key][\"abstract\"])})\n",
    "    dico[key].update(get_info(dico[key][\"abstract\"],affiliation=\"What affiliations do the authors or characters in the text have?\",\n",
    "                        new_materials=\"Does the text mention any new materials or discoveries?\",\n",
    "                        screening_algorithms=\"Are there any screening algorithms or systematic procedures discussed in the text?\",\n",
    "                        ai_algorithms=\"Does the text reference any AI algorithms or methods related to artificial intelligence?\",\n",
    "                        workflow=\"Can you describe the workflow or process followed in the text?\",\n",
    "                        methods=\"Can you summarize the methods or approaches mentioned in the text?\",\n",
    "                        models=\"What models or frameworks are discussed or used in the text?\",\n",
    "                        funding=\"Does the text mention any funding sources or sponsors?\"))\n",
    "\n",
    "print(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"output.json\", 'w') as file:\n",
    "    json.dump(dico, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387585"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def go_get_pdf_link(doi):\n",
    "    url = f'https://dacemirror.sci-hub.se/journal-article/459ea6cdde8059dec98aaac7493f90f7/wood2010.pdf#nav'\n",
    "    \n",
    "    \n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "filename = Path('pdfs/metadata.pdf')\n",
    "url = 'https://dacemirror.sci-hub.se/journal-article/459ea6cdde8059dec98aaac7493f90f7/wood2010.pdf#nav'\n",
    "response = requests.get(url)\n",
    "filename.write_bytes(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created Software/ dataset\n",
    "\n",
    "Binary: yes/No\n",
    "\n",
    "Output: numbers/DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2733 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paper ID 617509 doesn't have the DOI.\n",
      "10.1016/j.tifs.2022.05.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2024/09/12 18:30:30 | Run scihub tasks. Tasks information: \n",
      "[INFO] | 2024/09/12 18:30:30 |          DOI(s): ['10.1016/j.tifs.2022.05.003']\n",
      "[INFO] | 2024/09/12 18:30:30 |          Output: papers/10.1016_j.tifs.2022.05.003.pdf\n",
      "[INFO] | 2024/09/12 18:30:30 |      SciHub Url: <auto.availability_first>\n",
      "[INFO] | 2024/09/12 18:30:30 | Choose scihub url [0]: https://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:30 | <- Request: scihub_url=https://sci-hub.st, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:30 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:30 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:30 | Choose scihub url [1]: https://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:30 | <- Request: scihub_url=https://sci-hub.ru, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:31 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:31 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:31 | Choose scihub url [2]: http://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:31 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:31 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:31 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:31 | Choose scihub url [3]: http://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:31 | <- Request: scihub_url=http://sci-hub.ru, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:32 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:32 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:32 | Choose scihub url [4]: http://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:32 | <- Request: scihub_url=http://sci-hub.mobi, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:32 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[INFO] | 2024/09/12 18:30:32 | Choose scihub url [5]: https://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:32 | <- Request: scihub_url=https://sci-hub.se, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:32 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:32 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:32 | Choose scihub url [6]: http://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:32 | <- Request: scihub_url=http://sci-hub.st, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:33 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:33 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:33 | Choose scihub url [7]: https://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:33 | <- Request: scihub_url=https://sci-hub.mobi, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:33 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[ERROR] | 2024/09/12 18:30:33 | Failed to download the paper: 10.1016/j.tifs.2022.05.003. Please try again.\n",
      "[INFO] | 2024/09/12 18:30:33 | Choose scihub url [0]: https://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:33 | <- Request: scihub_url=https://sci-hub.st, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:33 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:33 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:33 | Choose scihub url [1]: https://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:33 | <- Request: scihub_url=https://sci-hub.ru, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:34 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:34 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:34 | Choose scihub url [2]: http://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:34 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:34 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:34 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:34 | Choose scihub url [3]: http://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:34 | <- Request: scihub_url=http://sci-hub.ru, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:35 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:35 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:35 | Choose scihub url [4]: http://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:35 | <- Request: scihub_url=http://sci-hub.mobi, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:35 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[INFO] | 2024/09/12 18:30:35 | Choose scihub url [5]: https://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:35 | <- Request: scihub_url=https://sci-hub.se, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:36 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:36 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:36 | Choose scihub url [6]: http://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:36 | <- Request: scihub_url=http://sci-hub.st, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:36 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:36 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:36 | Choose scihub url [7]: https://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:36 | <- Request: scihub_url=https://sci-hub.mobi, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:36 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[ERROR] | 2024/09/12 18:30:36 | Failed to download the paper: 10.1016/j.tifs.2022.05.003. Please try again.\n",
      " 59%|█████▊    | 1603/2733 [00:07<00:05, 209.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download failed for PaperID 618099 with DOI: 10.1016/j.tifs.2022.05.003.\n",
      "10.1002/smm2.1117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2024/09/12 18:30:37 | Run scihub tasks. Tasks information: \n",
      "[INFO] | 2024/09/12 18:30:37 |          DOI(s): ['10.1002/smm2.1117']\n",
      "[INFO] | 2024/09/12 18:30:37 |          Output: papers/10.1002_smm2.1117.pdf\n",
      "[INFO] | 2024/09/12 18:30:37 |      SciHub Url: <auto.availability_first>\n",
      "[INFO] | 2024/09/12 18:30:37 | Choose scihub url [0]: https://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:37 | <- Request: scihub_url=https://sci-hub.st, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:38 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:38 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:38 | Choose scihub url [1]: https://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:38 | <- Request: scihub_url=https://sci-hub.ru, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:38 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:38 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:38 | Choose scihub url [2]: http://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:38 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      " 59%|█████▊    | 1603/2733 [00:09<00:06, 166.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDOI\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/spock_package/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/anaconda3/envs/spock_package/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/spock_package/lib/python3.11/subprocess.py:2141\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2138\u001b[0m                     key\u001b[38;5;241m.\u001b[39mfileobj\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   2139\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj2output[key\u001b[38;5;241m.\u001b[39mfileobj]\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m-> 2141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remaining_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[38;5;66;03m# All data exchanged.  Translate lists into strings.\u001b[39;00m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/spock_package/lib/python3.11/subprocess.py:1264\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/spock_package/lib/python3.11/subprocess.py:2047\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2045\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, timeout)\n\u001b[1;32m   2046\u001b[0m         delay \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(delay \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, remaining, \u001b[38;5;241m.05\u001b[39m)\n\u001b[0;32m-> 2047\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(delay)\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2024/09/12 18:30:39 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:39 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:39 | Choose scihub url [3]: http://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:39 | <- Request: scihub_url=http://sci-hub.ru, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:39 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:39 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:39 | Choose scihub url [4]: http://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:39 | <- Request: scihub_url=http://sci-hub.mobi, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:40 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[INFO] | 2024/09/12 18:30:40 | Choose scihub url [5]: https://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:40 | <- Request: scihub_url=https://sci-hub.se, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:40 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:40 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:40 | Choose scihub url [6]: http://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:40 | <- Request: scihub_url=http://sci-hub.st, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:40 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:40 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:40 | Choose scihub url [7]: https://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:40 | <- Request: scihub_url=https://sci-hub.mobi, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:41 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[ERROR] | 2024/09/12 18:30:41 | Failed to download the paper: 10.1002/smm2.1117. Please try again.\n",
      "[INFO] | 2024/09/12 18:30:41 | Choose scihub url [0]: https://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:41 | <- Request: scihub_url=https://sci-hub.st, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:41 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:41 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:41 | Choose scihub url [1]: https://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:41 | <- Request: scihub_url=https://sci-hub.ru, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:42 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:42 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:42 | Choose scihub url [2]: http://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:42 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:42 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:42 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:42 | Choose scihub url [3]: http://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:42 | <- Request: scihub_url=http://sci-hub.ru, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:43 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:43 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:43 | Choose scihub url [4]: http://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:43 | <- Request: scihub_url=http://sci-hub.mobi, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:43 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[INFO] | 2024/09/12 18:30:43 | Choose scihub url [5]: https://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:43 | <- Request: scihub_url=https://sci-hub.se, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:44 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:44 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:44 | Choose scihub url [6]: http://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:44 | <- Request: scihub_url=http://sci-hub.st, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:44 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:44 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:44 | Choose scihub url [7]: https://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:44 | <- Request: scihub_url=https://sci-hub.mobi, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:44 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[ERROR] | 2024/09/12 18:30:44 | Failed to download the paper: 10.1002/smm2.1117. Please try again.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "not_downloadable = []\n",
    "Invalid_DOIs = []\n",
    "downloaded_papers = []\n",
    "\n",
    "mirrors = [\n",
    "    \"https://sci-hub.ee\",\n",
    "    \"https://sci-hub.ren\",\n",
    "    \"https://sci-hub.wf\"\n",
    "]\n",
    "\n",
    "\n",
    "df = pd.read_csv('filtered_AC_publications.csv', encoding='ISO-8859-1', low_memory=False, delimiter=',')\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if i > 1600: # 1100 - 1500 \n",
    "        doi_url = f\"https://doi.org/{row['DOI']}\"\n",
    "        if not pd.isna(row['DOI']) and \"(\" not in doi_url and  \")\" not in doi_url:\n",
    "            output_path = f\"papers/{row['DOI'].replace('/', '_')}.pdf\"  \n",
    "            if not os.path.exists(output_path):\n",
    "                command = f'scidownl download --doi {row[\"DOI\"]} --out {output_path}'\n",
    "                # command = f'python -m PyPaperBot --doi {doi_url} --dwn-dir papers/'\n",
    "                try:\n",
    "                    print(row[\"DOI\"])\n",
    "                    subprocess.run(command, shell=True, check=True, timeout=20)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    pass\n",
    "                # !scidownl download --doi {row['DOI']} --out doi_url\n",
    "\n",
    "                if not os.path.exists(output_path):\n",
    "                    print(f\"\\nDownload failed for PaperID {row['ID']} with DOI: {row['DOI']}.\")\n",
    "                    not_downloadable.append((row['ID'], row['DOI']))\n",
    "                else: \n",
    "                    downloaded_papers.append((row['ID'], row['DOI']))\n",
    "            output_path = f\"papers/{row['DOI'].replace('/', '_')}.pdf\"\n",
    "        else:\n",
    "            print(f\"\\nPaper ID {row['ID']} doesn't have the DOI.\")\n",
    "            Invalid_DOIs.append(row['ID'])\n",
    "\n",
    "    # if i == 50:\n",
    "    #     break\n",
    "\n",
    "stats = {'Downloaded': downloaded_papers,\n",
    "        'Non-downloaded': not_downloadable,\n",
    "        'Invalid_DOIs': Invalid_DOIs}\n",
    "\n",
    "with open('stats.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
