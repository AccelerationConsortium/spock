{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Abstracts Using a Large Language Model (LLM)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this Jupyter Notebook, we will perform an in-depth analysis of abstracts extracted from a CSV file using a Large Language Model (LLM). The goal of this analysis is to leverage the capabilities of LLMs to extract meaningful insights, identify key themes, and perform various natural language processing (NLP) tasks on the abstracts.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- **Data Loading**: Import and preprocess abstracts from a CSV file.\n",
    "- **Text Analysis**: Utilize LLMs to analyze the content of the abstracts.\n",
    "\n",
    "### Tools and Libraries\n",
    "\n",
    "- **LangChain**: To interface with the LLM.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. **Data Import**: Load the CSV file containing the abstracts.\n",
    "3. **LLM Integration**: Use the LLM to perform various NLP tasks.\n",
    "\n",
    "By the end of this notebook, you will have a comprehensive understanding of how to use LLMs for analyzing textual data and extracting valuable insights from scientific abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Import**: Load the CSV file containing the abstracts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/nougat\n",
      "  Cloning https://github.com/facebookresearch/nougat to /tmp/pip-req-build-fm_hskrv\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/nougat /tmp/pip-req-build-fm_hskrv\n",
      "  Resolved https://github.com/facebookresearch/nougat to commit 47c77d70727558b4a2025005491ecb26ee97f523\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers>=4.25.1 (from nougat-ocr==0.1.17)\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m607.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting timm==0.5.4 (from nougat-ocr==0.1.17)\n",
      "  Downloading timm-0.5.4-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: orjson in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from nougat-ocr==0.1.17) (3.10.4)\n",
      "Collecting opencv-python-headless (from nougat-ocr==0.1.17)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting datasets[vision] (from nougat-ocr==0.1.17)\n",
      "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting lightning<2022,>=2.0.0 (from nougat-ocr==0.1.17)\n",
      "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting nltk (from nougat-ocr==0.1.17)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting python-Levenshtein (from nougat-ocr==0.1.17)\n",
      "  Downloading python_Levenshtein-0.25.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting sentencepiece (from nougat-ocr==0.1.17)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting sconf>=0.2.3 (from nougat-ocr==0.1.17)\n",
      "  Downloading sconf-0.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting albumentations>=1.0.0 (from nougat-ocr==0.1.17)\n",
      "  Downloading albumentations-1.4.14-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting pypdf>=3.1.0 (from nougat-ocr==0.1.17)\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pypdfium2 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from nougat-ocr==0.1.17) (4.30.0)\n",
      "Collecting torch>=1.4 (from timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision (from timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (1.26.4)\n",
      "Collecting scipy>=1.10.0 (from albumentations>=1.0.0->nougat-ocr==0.1.17)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-image>=0.21.0 (from albumentations>=1.0.0->nougat-ocr==0.1.17)\n",
      "  Downloading scikit_image-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: PyYAML in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (4.12.2)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (2.7.4)\n",
      "Collecting albucore>=0.0.13 (from albumentations>=1.0.0->nougat-ocr==0.1.17)\n",
      "  Downloading albucore-0.0.15-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting eval-type-backport (from albumentations>=1.0.0->nougat-ocr==0.1.17)\n",
      "  Downloading eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<2022,>=2.0.0->nougat-ocr==0.1.17) (2024.6.0)\n",
      "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17)\n",
      "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17) (23.2)\n",
      "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17)\n",
      "  Downloading torchmetrics-1.4.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17) (4.66.4)\n",
      "Collecting pytorch-lightning (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17)\n",
      "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting ruamel.yaml (from sconf>=0.2.3->nougat-ocr==0.1.17)\n",
      "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting munch (from sconf>=0.2.3->nougat-ocr==0.1.17)\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: filelock in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from transformers>=4.25.1->nougat-ocr==0.1.17) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from transformers>=4.25.1->nougat-ocr==0.1.17) (0.23.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from transformers>=4.25.1->nougat-ocr==0.1.17) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from transformers>=4.25.1->nougat-ocr==0.1.17) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.25.1->nougat-ocr==0.1.17)\n",
      "  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from transformers>=4.25.1->nougat-ocr==0.1.17) (0.19.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets[vision]->nougat-ocr==0.1.17)\n",
      "  Downloading pyarrow-17.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets[vision]->nougat-ocr==0.1.17)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from datasets[vision]->nougat-ocr==0.1.17) (2.2.2)\n",
      "Collecting xxhash (from datasets[vision]->nougat-ocr==0.1.17)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets[vision]->nougat-ocr==0.1.17)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: aiohttp in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from datasets[vision]->nougat-ocr==0.1.17) (3.9.5)\n",
      "Requirement already satisfied: Pillow>=9.4.0 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from datasets[vision]->nougat-ocr==0.1.17) (10.3.0)\n",
      "Requirement already satisfied: click in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from nltk->nougat-ocr==0.1.17) (8.1.7)\n",
      "Collecting joblib (from nltk->nougat-ocr==0.1.17)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting Levenshtein==0.25.1 (from python-Levenshtein->nougat-ocr==0.1.17)\n",
      "  Downloading Levenshtein-0.25.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein==0.25.1->python-Levenshtein->nougat-ocr==0.1.17)\n",
      "  Downloading rapidfuzz-3.9.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (1.9.4)\n",
      "Requirement already satisfied: setuptools in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2022,>=2.0.0->nougat-ocr==0.1.17) (69.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from pydantic>=2.7.0->albumentations>=1.0.0->nougat-ocr==0.1.17) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from pydantic>=2.7.0->albumentations>=1.0.0->nougat-ocr==0.1.17) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from requests->transformers>=4.25.1->nougat-ocr==0.1.17) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from requests->transformers>=4.25.1->nougat-ocr==0.1.17) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from requests->transformers>=4.25.1->nougat-ocr==0.1.17) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from requests->transformers>=4.25.1->nougat-ocr==0.1.17) (2024.6.2)\n",
      "Collecting networkx>=2.8 (from scikit-image>=0.21.0->albumentations>=1.0.0->nougat-ocr==0.1.17)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting imageio>=2.33 (from scikit-image>=0.21.0->albumentations>=1.0.0->nougat-ocr==0.1.17)\n",
      "  Downloading imageio-2.35.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image>=0.21.0->albumentations>=1.0.0->nougat-ocr==0.1.17)\n",
      "  Downloading tifffile-2024.8.30-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image>=0.21.0->albumentations>=1.0.0->nougat-ocr==0.1.17)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: sympy in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from pandas->datasets[vision]->nougat-ocr==0.1.17) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from pandas->datasets[vision]->nougat-ocr==0.1.17) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from pandas->datasets[vision]->nougat-ocr==0.1.17) (2024.1)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->sconf>=0.2.3->nougat-ocr==0.1.17)\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets[vision]->nougat-ocr==0.1.17) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from jinja2->torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/youssef/anaconda3/envs/spock_package/lib/python3.11/site-packages (from sympy->torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (1.3.0)\n",
      "Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading albumentations-1.4.14-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sconf-0.2.5-py3-none-any.whl (8.8 kB)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_Levenshtein-0.25.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading Levenshtein-0.25.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading albucore-0.0.15-py3-none-any.whl (9.0 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
      "Downloading pyarrow-17.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_image-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.6/797.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:04:01\u001b[0m^C\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.7/797.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:04:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from downloader import *\n",
    "!pip install git+https://github.com/facebookresearch/nougat\n",
    "%pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10.1038/s41586-020-2746-2': {'abstract': 'The genetic circuits that allow cancer cells to evade destruction by the host immune system remain poorly understood1\\x963. Here, to identify a phenotypically robust core set of genes and pathways that enable cancer cells to evade killing mediated by cytotoxic T\\xa0lymphocytes (CTLs), we performed genome-wide CRISPR screens across a panel of genetically diverse mouse cancer cell lines that were cultured in the presence of CTLs. We identify a core set of 182\\xa0genes across these mouse cancer models, the individual perturbation of which increases either the sensitivity or the resistance of cancer cells to CTL-mediated toxicity. Systematic exploration of our dataset using genetic co-similarity reveals the hierarchical and coordinated manner in which genes and pathways act in cancer cells to orchestrate their evasion of CTLs, and shows that discrete functional modules that control the interferon response and tumour necrosis factor (TNF)-induced cytotoxicity are dominant sub-phenotypes. Our data establish a central role for genes that were previously identified as negative regulators of the type-II interferon response (for example, Ptpn2, Socs1 and Adar1) in mediating CTL evasion, and show that the lipid-droplet-related gene Fitm2 is required for maintaining cell fitness after exposure to interferon-? (IFN?). In addition, we identify the autophagy pathway as a conserved mediator of the evasion of CTLs by cancer cells, and show that this pathway is required to resist cytotoxicity induced by the cytokines IFN? and\\xa0TNF. Through the mapping of cytokine- and CTL-based genetic interactions, together with in vivo CRISPR screens, we show how the pleiotropic effects of autophagy control cancer-cell-intrinsic evasion of killing by CTLs and we highlight the importance of these effects within the tumour microenvironment. Collectively, these\\xa0data expand our knowledge of the genetic circuits that are involved in the evasion of the immune system by cancer cells, and highlight genetic interactions that contribute to phenotypes associated with escape from killing by CTLs.'}, '10.1016/j.surfcoat.2020.126648': {'abstract': 'The formation of \\x93mud-cracking\\x94 networks in coatings and thin films is a widely observed phenomenon, particularly in Cr-containing electrodeposited materials. Here we have documented and described aspects of mud-cracking specific to Cr-containing electrodeposited alloys made from trivalent Cr electrolytes. In addition to describing the different crack patterns observed in these materials and their relationship to the electroplating parameters, thickness, and composition of the coatings, we also investigated a never-before-reported phenomenon of post-deposition time-dependent cracking. Our analysis shows that cracks continue to form after the electroplating procedure has been completed, generally saturating 2 h after synthesis, but can continue for days afterwards. We theorize that the behaviour is related to the state of oxygen in these samples, which can sometimes reach beyond 30 at.%. It is believed that the breakdown of Cr-hydroxides and Cr-hydrides drives the increase in internal stresses that result in continued cracking. As a result, this time-dependent cracking phenomenon may impact the suitability of certain Cr-alloy electrodeposits in some applications.'}, '': {'abstract': 'Optimization of particle size distribution in coal fly ash is an important production parameter to improve fly ash quality for use in concrete. The particle size distributions of fly ash from nine Canadian and Indian sources were measured using a Laser Diffraction Analyzer (LDA). Use of a LDA provided a meaningful characterization of particle size variability between the studied fly ashes. Scanning electron microscopy (SEM) was employed to visualize the distribution and shapes of the fly ash particles. Each LDA measurement was verified by a corresponding scanning electron micrograph. Both LDA and SEM were found to be quick and efficient techniques in terms of sample preparation, data acquisition, and analysis. A bimodal size distribution was associated with all fly ashes as analyzed by LDA. The SEM micrographs showed two forms of particles: large agglomerates and smaller spherical primary particles.'}, '10.1021/acsaem.9b02371': {'abstract': 'We present electrospinning as a versatile technique to design and fabricate tailored polymer electrolyte membrane (PEM) fuel cell gas diffusion layers (GDLs) with a pore-size gradient (increasing from catalyst layer to flow field) to enhance the high current density performance and water management behavior of a PEM fuel cell. The novel graded electrospun GDL exhibits highly robust performance over a range of inlet gas relative humidities (RH). At relatively dry (50% RH) inlet conditions that exacerbate ohmic losses, the graded GDL lowers ohmic resistance and improves high current density performance compared to a uniform GDL with larger pores and fiber diameters. Specifically, the graded GDL facilitates a beneficial degree of liquid water retention at the catalyst layer/GDL interface due to the high capillary pressure inherent in its microstructure, thereby improving membrane hydration. Additionally, enhanced graphitization and connectivity of the graded electrospun fibers improves heat dissipation from the catalyst layer interface compared to the GDL with larger fiber diameters, thereby reducing membrane dehydration. When the inlet RH is raised to fully humid (100% RH) conditions, the graded GDL mitigates liquid water accumulation and lowers mass transport resistance. Specifically, the pore size gradient directs the removal of liquid water from the GDL, resulting in superior performance at high current densities.'}}\n"
     ]
    }
   ],
   "source": [
    "def read_csv(file_path, num_lines):\n",
    "    with open(file_path, 'r', encoding=\"utf8\", errors='ignore') as file:\n",
    "        reader = csv.reader(file)\n",
    "        dico = {}\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= num_lines:\n",
    "                break\n",
    "            if i == 0:\n",
    "                continue\n",
    "            dico[row[17]] = {\"abstract\": row[4]} \n",
    "            \"\"\"\n",
    "            try:\n",
    "               \n",
    "                downloader = Downloader(row[17], 'doi', f\"pdfs/{row[17]}.pdf\")\n",
    "                downloader.download()\n",
    "                dico[row[17]].update({\"pdf\": True})\n",
    "            except Exception as e:\n",
    "                dico[row[17]].update({\"pdf\": False})\n",
    "                print(e)\n",
    "          \"\"\"      \n",
    "        print(dico)\n",
    "        return dico\n",
    "\n",
    "file_path = \"filtered_AC_publications.csv\"\n",
    "num_lines = 5\n",
    "dico = read_csv(file_path, num_lines)\n",
    "\n",
    "## Writing the pdfs into a txt file:\n",
    "\n",
    "for key in dico:\n",
    "    with open(\"pdfs.txt\", \"a\") as f:\n",
    "        f.write(key + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_topic(abstract:str):\n",
    "    \n",
    "    Llm = Ollama(model='llama3', temperature=0.2)\n",
    "    \n",
    "    \n",
    "    if abstract is None:\n",
    "        raise ValueError(\"Abstract is required\")\n",
    "    \n",
    "    parser = JsonOutputParser()\n",
    "    \n",
    "    #AI / accelerated materials discovery / SDLs / autonomous labs / high-throughput experimentation / high-throughput DFT\n",
    "    \n",
    "    topics = [\"Machine Learning\", \"Batteries\", \"AI\", \"accelerated materials discovery\", \"Self Driving Labs\", \"autonomous labs\", \"high-throughput experimentation\", \"high-throughput DFT\"]\n",
    "    \n",
    "    \n",
    "    new_text = \"\"\"\n",
    "    \n",
    "    The output needs to be formated as the following: \n",
    "    \n",
    "    {\n",
    "    \"topic\": {\n",
    "    \"topic1\": [\"Keyword1\", \"Keyword2\", \"Keyword3\"],\n",
    "    \"topic2\": [\"Keyword1\", \"Keyword2\", \"Keyword3\"]\n",
    "    }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    Only output the dictionary above, nothing else with it.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "    template=\" So you are a text assistant and I need you to help me identify the topics from the following list the text given to you {topics}. \\n Here's the text: {abstract}. \\n\\n Note: A single text can belong to multiple topics, so please list all relevant topics. {format_instructions}\",\n",
    "    input_variables=[\"format_instructions\", \"abstract\", \"topics\"]\n",
    "    )\n",
    "\n",
    "    chain = prompt | Llm | parser\n",
    "    topics = chain.invoke({\"format_instructions\": new_text, \"abstract\": abstract, \"topics\": topics})\n",
    "    return list(topics.values())[0]\n",
    "\n",
    "\n",
    "print(get_topic(\"The development of high-performance batteries is crucial for the future of electric vehicles. The current generation of batteries are not able to provide the range and power required for long-distance travel. This project aims to develop new materials for batteries that can provide higher energy density and faster charging times.\"))\n",
    "\n",
    "def get_info(abstract:str = None, **kwargs):\n",
    "    Llm = Ollama(model='llama3', temperature=0.5)\n",
    "    if abstract is None:\n",
    "        raise ValueError(\"Abstract is required\")\n",
    "    \n",
    "    dico = {}\n",
    "    for key, question in kwargs.items():\n",
    "        print(key, question)\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"So you are a text assistant and I want you to assist me by providing the following information: {question}. \\n\\n Here's the text: {abstract}. \\n\\n If the text doesn't contain any information about the topic given, output: 'N/A'\",\n",
    "            input_variables=[\"abstract\", \"question\"]\n",
    "        )\n",
    "        chain = prompt | Llm \n",
    "        info = chain.invoke({\"abstract\": abstract, \"question\": question})\n",
    "        dico[key] = info\n",
    "    print(dico)\n",
    "    return dico\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dico:\n",
    "    dico[key].update({\"topic\":get_topic(dico[key][\"abstract\"])})\n",
    "    dico[key].update(get_info(dico[key][\"abstract\"],affiliation=\"What affiliations do the authors or characters in the text have?\",\n",
    "                        new_materials=\"Does the text mention any new materials or discoveries?\",\n",
    "                        screening_algorithms=\"Are there any screening algorithms or systematic procedures discussed in the text?\",\n",
    "                        ai_algorithms=\"Does the text reference any AI algorithms or methods related to artificial intelligence?\",\n",
    "                        workflow=\"Can you describe the workflow or process followed in the text?\",\n",
    "                        methods=\"Can you summarize the methods or approaches mentioned in the text?\",\n",
    "                        models=\"What models or frameworks are discussed or used in the text?\",\n",
    "                        funding=\"Does the text mention any funding sources or sponsors?\"))\n",
    "\n",
    "print(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"output.json\", 'w') as file:\n",
    "    json.dump(dico, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387585"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def go_get_pdf_link(doi):\n",
    "    url = f'https://dacemirror.sci-hub.se/journal-article/459ea6cdde8059dec98aaac7493f90f7/wood2010.pdf#nav'\n",
    "    \n",
    "    \n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "filename = Path('pdfs/metadata.pdf')\n",
    "url = 'https://dacemirror.sci-hub.se/journal-article/459ea6cdde8059dec98aaac7493f90f7/wood2010.pdf#nav'\n",
    "response = requests.get(url)\n",
    "filename.write_bytes(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created Software/ dataset\n",
    "\n",
    "Binary: yes/No\n",
    "\n",
    "Output: numbers/DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2733 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paper ID 617509 doesn't have the DOI.\n",
      "10.1016/j.tifs.2022.05.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2024/09/12 18:30:30 | Run scihub tasks. Tasks information: \n",
      "[INFO] | 2024/09/12 18:30:30 |          DOI(s): ['10.1016/j.tifs.2022.05.003']\n",
      "[INFO] | 2024/09/12 18:30:30 |          Output: papers/10.1016_j.tifs.2022.05.003.pdf\n",
      "[INFO] | 2024/09/12 18:30:30 |      SciHub Url: <auto.availability_first>\n",
      "[INFO] | 2024/09/12 18:30:30 | Choose scihub url [0]: https://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:30 | <- Request: scihub_url=https://sci-hub.st, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:30 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:30 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:30 | Choose scihub url [1]: https://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:30 | <- Request: scihub_url=https://sci-hub.ru, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:31 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:31 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:31 | Choose scihub url [2]: http://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:31 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:31 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:31 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:31 | Choose scihub url [3]: http://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:31 | <- Request: scihub_url=http://sci-hub.ru, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:32 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:32 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:32 | Choose scihub url [4]: http://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:32 | <- Request: scihub_url=http://sci-hub.mobi, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:32 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[INFO] | 2024/09/12 18:30:32 | Choose scihub url [5]: https://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:32 | <- Request: scihub_url=https://sci-hub.se, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:32 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:32 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:32 | Choose scihub url [6]: http://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:32 | <- Request: scihub_url=http://sci-hub.st, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:33 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:33 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:33 | Choose scihub url [7]: https://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:33 | <- Request: scihub_url=https://sci-hub.mobi, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:33 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[ERROR] | 2024/09/12 18:30:33 | Failed to download the paper: 10.1016/j.tifs.2022.05.003. Please try again.\n",
      "[INFO] | 2024/09/12 18:30:33 | Choose scihub url [0]: https://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:33 | <- Request: scihub_url=https://sci-hub.st, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:33 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:33 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:33 | Choose scihub url [1]: https://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:33 | <- Request: scihub_url=https://sci-hub.ru, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:34 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:34 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:34 | Choose scihub url [2]: http://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:34 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:34 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:34 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:34 | Choose scihub url [3]: http://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:34 | <- Request: scihub_url=http://sci-hub.ru, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:35 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:35 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:35 | Choose scihub url [4]: http://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:35 | <- Request: scihub_url=http://sci-hub.mobi, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:35 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[INFO] | 2024/09/12 18:30:35 | Choose scihub url [5]: https://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:35 | <- Request: scihub_url=https://sci-hub.se, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:36 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:36 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:36 | Choose scihub url [6]: http://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:36 | <- Request: scihub_url=http://sci-hub.st, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:36 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:36 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:36 | Choose scihub url [7]: https://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:36 | <- Request: scihub_url=https://sci-hub.mobi, source=DoiSource[type=doi, id=10.1016/j.tifs.2022.05.003], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:36 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[ERROR] | 2024/09/12 18:30:36 | Failed to download the paper: 10.1016/j.tifs.2022.05.003. Please try again.\n",
      " 59%|█████▊    | 1603/2733 [00:07<00:05, 209.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download failed for PaperID 618099 with DOI: 10.1016/j.tifs.2022.05.003.\n",
      "10.1002/smm2.1117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2024/09/12 18:30:37 | Run scihub tasks. Tasks information: \n",
      "[INFO] | 2024/09/12 18:30:37 |          DOI(s): ['10.1002/smm2.1117']\n",
      "[INFO] | 2024/09/12 18:30:37 |          Output: papers/10.1002_smm2.1117.pdf\n",
      "[INFO] | 2024/09/12 18:30:37 |      SciHub Url: <auto.availability_first>\n",
      "[INFO] | 2024/09/12 18:30:37 | Choose scihub url [0]: https://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:37 | <- Request: scihub_url=https://sci-hub.st, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:38 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:38 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:38 | Choose scihub url [1]: https://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:38 | <- Request: scihub_url=https://sci-hub.ru, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:38 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:38 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:38 | Choose scihub url [2]: http://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:38 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      " 59%|█████▊    | 1603/2733 [00:09<00:06, 166.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDOI\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/spock_package/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/anaconda3/envs/spock_package/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/spock_package/lib/python3.11/subprocess.py:2141\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2138\u001b[0m                     key\u001b[38;5;241m.\u001b[39mfileobj\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   2139\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj2output[key\u001b[38;5;241m.\u001b[39mfileobj]\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m-> 2141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remaining_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[38;5;66;03m# All data exchanged.  Translate lists into strings.\u001b[39;00m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/spock_package/lib/python3.11/subprocess.py:1264\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/spock_package/lib/python3.11/subprocess.py:2047\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2045\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, timeout)\n\u001b[1;32m   2046\u001b[0m         delay \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(delay \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, remaining, \u001b[38;5;241m.05\u001b[39m)\n\u001b[0;32m-> 2047\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(delay)\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2024/09/12 18:30:39 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:39 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:39 | Choose scihub url [3]: http://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:39 | <- Request: scihub_url=http://sci-hub.ru, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:39 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:39 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:39 | Choose scihub url [4]: http://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:39 | <- Request: scihub_url=http://sci-hub.mobi, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:40 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[INFO] | 2024/09/12 18:30:40 | Choose scihub url [5]: https://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:40 | <- Request: scihub_url=https://sci-hub.se, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:40 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:40 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:40 | Choose scihub url [6]: http://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:40 | <- Request: scihub_url=http://sci-hub.st, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:40 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:40 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:40 | Choose scihub url [7]: https://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:40 | <- Request: scihub_url=https://sci-hub.mobi, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:41 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[ERROR] | 2024/09/12 18:30:41 | Failed to download the paper: 10.1002/smm2.1117. Please try again.\n",
      "[INFO] | 2024/09/12 18:30:41 | Choose scihub url [0]: https://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:41 | <- Request: scihub_url=https://sci-hub.st, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:41 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:41 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:41 | Choose scihub url [1]: https://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:41 | <- Request: scihub_url=https://sci-hub.ru, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:42 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:42 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:42 | Choose scihub url [2]: http://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:42 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:42 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:42 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:42 | Choose scihub url [3]: http://sci-hub.ru\n",
      "[INFO] | 2024/09/12 18:30:42 | <- Request: scihub_url=http://sci-hub.ru, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:43 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:43 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:43 | Choose scihub url [4]: http://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:43 | <- Request: scihub_url=http://sci-hub.mobi, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:43 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[INFO] | 2024/09/12 18:30:43 | Choose scihub url [5]: https://sci-hub.se\n",
      "[INFO] | 2024/09/12 18:30:43 | <- Request: scihub_url=https://sci-hub.se, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:44 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:44 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:44 | Choose scihub url [6]: http://sci-hub.st\n",
      "[INFO] | 2024/09/12 18:30:44 | <- Request: scihub_url=http://sci-hub.st, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[INFO] | 2024/09/12 18:30:44 | -> Response: status_code=200, content_length=0\n",
      "[WARNING] | 2024/09/12 18:30:44 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2024/09/12 18:30:44 | Choose scihub url [7]: https://sci-hub.mobi\n",
      "[INFO] | 2024/09/12 18:30:44 | <- Request: scihub_url=https://sci-hub.mobi, source=DoiSource[type=doi, id=10.1002/smm2.1117], proxies={}\n",
      "[WARNING] | 2024/09/12 18:30:44 | Error occurs, task status: crawling_failed, error: HTTPSConnectionPool(host='sci-hub.mobi', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n",
      "[ERROR] | 2024/09/12 18:30:44 | Failed to download the paper: 10.1002/smm2.1117. Please try again.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "not_downloadable = []\n",
    "Invalid_DOIs = []\n",
    "downloaded_papers = []\n",
    "\n",
    "mirrors = [\n",
    "    \"https://sci-hub.ee\",\n",
    "    \"https://sci-hub.ren\",\n",
    "    \"https://sci-hub.wf\"\n",
    "]\n",
    "\n",
    "\n",
    "df = pd.read_csv('filtered_AC_publications.csv', encoding='ISO-8859-1', low_memory=False, delimiter=',')\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if i > 1600: # 1100 - 1500 \n",
    "        doi_url = f\"https://doi.org/{row['DOI']}\"\n",
    "        if not pd.isna(row['DOI']) and \"(\" not in doi_url and  \")\" not in doi_url:\n",
    "            output_path = f\"papers/{row['DOI'].replace('/', '_')}.pdf\"  \n",
    "            if not os.path.exists(output_path):\n",
    "                command = f'scidownl download --doi {row[\"DOI\"]} --out {output_path}'\n",
    "                # command = f'python -m PyPaperBot --doi {doi_url} --dwn-dir papers/'\n",
    "                try:\n",
    "                    print(row[\"DOI\"])\n",
    "                    subprocess.run(command, shell=True, check=True, timeout=20)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    pass\n",
    "                # !scidownl download --doi {row['DOI']} --out doi_url\n",
    "\n",
    "                if not os.path.exists(output_path):\n",
    "                    print(f\"\\nDownload failed for PaperID {row['ID']} with DOI: {row['DOI']}.\")\n",
    "                    not_downloadable.append((row['ID'], row['DOI']))\n",
    "                else: \n",
    "                    downloaded_papers.append((row['ID'], row['DOI']))\n",
    "            output_path = f\"papers/{row['DOI'].replace('/', '_')}.pdf\"\n",
    "        else:\n",
    "            print(f\"\\nPaper ID {row['ID']} doesn't have the DOI.\")\n",
    "            Invalid_DOIs.append(row['ID'])\n",
    "\n",
    "    # if i == 50:\n",
    "    #     break\n",
    "\n",
    "stats = {'Downloaded': downloaded_papers,\n",
    "        'Non-downloaded': not_downloadable,\n",
    "        'Invalid_DOIs': Invalid_DOIs}\n",
    "\n",
    "with open('stats.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
